<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="https://www.treszkai.com/theme/css/style.less">
  <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="icon" type="image/vnd.microsoft.icon" href="https://www.treszkai.com/">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/normalize.css">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/style.css?md5=58cb4674">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:regular,i,b|Fira+Code">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/fontawesome-all.min.css">


  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Laszlo Treszkai">
  <meta name="description" content="Posts and writings by Laszlo Treszkai">

  <link href="https://www.treszkai.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Laszlo Treszkai Atom" />

<meta name="keywords" content="statistics, Bayes, sampling">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
  var macros_dict = { "\\RR": "\\mathbb{R}"  ,  "\\EE": "\\mathbb{E}"  ,  "\\parm": "\\textcolor{grey}{\\bullet}"  ,  "\\indep": "\\perp\\!\\!\\!\\perp"  ,  "\\emptyset": "\\varnothing"  ,  "\\proves": "\\vdash"  ,  "\\Union": "\\bigcup"  ,  "\\Intersect": "\\bigcap"  ,  "\\grad": "\\nabla"  ,  "\\given": "\\,\\vert\\,"  ,  "\\Godel": "\\ulcorner #1 \\urcorner"    ,  "\\D": "\\mathcal{D}"  ,  "\\N": "\\mathcal{N}"    };
  document.querySelectorAll("script[type='math/tex']").forEach(function(el) {
    el.outerHTML = katex.renderToString(el.innerHTML, { displayMode: false, macros: macros_dict });
  });
  document.querySelectorAll("script[type='math/tex; mode=display']").forEach(function(el) {
    el.outerHTML = katex.renderToString(el.innerHTML, { displayMode: true, macros: macros_dict });
  });
});
</script>
  <title>
    Laszlo Treszkai
&ndash; Sampling from the posterior with Markov-chain Monte&nbsp;Carlo  </title>

</head>

<body>
  <main>
    <header>
      <div class="site-name">
        <a href="https://www.treszkai.com">Laszlo Treszkai</a>
      </div>
      <p>
      <a href="https://www.treszkai.com/index.html">Posts</a><span class="separator"></span><a href="https://www.treszkai.com/pages/about-me.html" title="About&nbsp;me">About&nbsp;me</a>      </p>
    </header>

<article>
  <div class="article__title">
    <h1><a href="https://www.treszkai.com/2019/08/06/mcmc/">Sampling from the posterior with Markov-chain Monte&nbsp;Carlo</a></h1>
  </div>
  <div class="article__meta">
    <p class="article__meta__post-date">Posted on: 6 Aug. 2019</p>
 Tags:
      <a href="https://www.treszkai.com/tag/statistics.html">#statistics</a>,      <a href="https://www.treszkai.com/tag/bayes.html">#Bayes</a>,      <a href="https://www.treszkai.com/tag/sampling.html">#sampling</a>    </p>
  </div>
  <div class="article__text">
    <p>John K. Kruschke&#8217;s book, titled <em>Doing Bayesian Data Analysis: A Tutorial with R, <span class="caps">JAGS</span>, and Stan (2nd ed.)</em> (<a href="https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884">Amazon</a>, <a href="https://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">official site</a>), gives a very quick and practical introduction to Bayesian analysis. Compared to <span class="caps">BDA3</span>, it contains less proofs, but also less jargon; more explanations that are informal, and more introductions to the basics. As such, I would recommend it to someone who hasn&#8217;t had much of an exposure to statistics yet, or is not a mathematician nor a&nbsp;programmer.</p>
<p>The book includes thorough and nicely visualized descriptions of multiple Markov-chain Monte Carlo methods for sampling from a posterior distribution, of which I&#8217;ll try to summarize the most basic one in this&nbsp;post.</p>
<h2>Goal of&nbsp;sampling</h2>
<p>Given the prior <script type="math/tex">p(θ)</script> and the likelihood <script type="math/tex">p(\D\given θ)</script>, we want samples from the posterior <script type="math/tex">p(θ\given \D)</script>. In the following sections I&#8217;ll use the fact that the unnormalized posterior is equal to the prior multiplied with the likelihood: <script type="math/tex">p(θ, \D) = p(θ)\,p(\D \given θ)</script>. Here, I&#8217;ll talk only about continuous probability spaces; discrete spaces can be sampled&nbsp;similarly.</p>
<h2>Metropolis&nbsp;algorithm</h2>
<p>Just like the other <span class="caps">MC</span> methods, the Metropolis algorithm starts with a seed value for <script type="math/tex">θ</script> – let&#8217;s call it <script type="math/tex">θ_0</script>. (I assume in practice <script type="math/tex">θ_0</script> is sampled from the prior.) Then, once you have a seed value <script type="math/tex">θ_i</script>, repeat the following two steps for a prespecified number of iterations, or until an effective sample size is&nbsp;achieved.</p>
<ol>
<li>Sample <script type="math/tex">θ'_{i+1}</script> from a proposal distribution around <script type="math/tex">\theta_i</script>, which could be a Gaussian: <script type="math/tex">\theta'_{i+1} \sim \N (θ_i, Σ)</script>.</li>
<li>
<ul>
<li>If <script type="math/tex">p(θ_{i},\D) \le p(θ'_{i+1},\D)</script> – i.e. if <script type="math/tex">p(θ_{i} \given \D) \le p(θ'_{i+1} \given \D)</script> – then <em>accept</em> the proposed parameter value: <script type="math/tex">θ_{i+1} := θ'_{i+1}</script>.</li>
<li>Otherwise, the probability of accepting the proposed parameter is the ratio of the posterior at the proposed value and at the current value; otherwise, reject&nbsp;it:</li>
</ul>
</li>
</ol>
<script type="math/tex; mode=display">% <![CDATA[
\begin{gathered}
p = \frac{p(θ'_{i+1}, \D)}{p(θ_{i}, \D)} = \frac{p(θ'_{i+1} \given \D)}{p(θ_{i} \given \D)}, \\
b \sim Bernoulli(p), \\
θ_{i+1} =
\begin{cases}
    θ_{i+1}' <span class="amp">&amp;</span> \text{if } b=1,\\
    θ_i <span class="amp">&amp;</span> \text{if } b=0.
\end{cases}
\end{gathered} %]]></script>
<p>It can be proven that after a so-called “burn-in” period, the probability of any <script type="math/tex">θ_{n}</script> value will be the posterior probability: <script type="math/tex">θ_n \sim p(\theta_n\given \D)</script> if <script type="math/tex">n \gg 1</script>, therefore if you do the procedure long enough, you&#8217;ll end up with many samples from the posterior. Note that the <em>effective sample size</em> will be much lower than <script type="math/tex">N</script>, because neighboring samples are strongly correlated, so we have to drop most of the <script type="math/tex">θ_i</script> values so&nbsp;obtained.</p>
<p>The beauty of this algorithm is that during this whole procedure, we only need to be able to compute the <em>unnormalized posterior</em> – so the algorithm can be easily used for sampling using the prior and the likelihood, even when the model is specified up to a multiplicative constant (as in an undirected graphical&nbsp;model).</p>
<p>This algorithm doesn&#8217;t easily escape a “probability island” – i.e. a region that is surrounded with a wide region of probability 0. (Although if the proposal distribution is wide enough, then the algorithm is theoretically able to make that jump <em>eventually</em>, which maybe in practice “approximately&nbsp;never”.)</p>
<p>One downside of this basic algorithm is that the proposal distribution needs to be fine-tuned for the individual application: differences in effective sample size can be orders of magnitudes, even for a simple <script type="math/tex">\text{Beta}(14,20)</script> distribution (i.e. a 1-dimensional unimodal distribution with finite&nbsp;support).</p>
<p>Another downside is that in multiple dimensions this random walk is quite inefficient, and <em>even more</em> dependent on a correct choice of the covariance matrix <script type="math/tex">Σ</script> – but apart from the obvious reason that “high-dimensional spaces are big”, I couldn&#8217;t tell&nbsp;why.</p>
<p>The well-known Metropolis–Hastings algorithm, Gibbs sampling and Hamiltonian Monte Carlo are different twists on this core idea, and they are also described in the&nbsp;book.</p>
<p>Allegedly, credit for this method is due more to Marshall and Arianna Rosenbluth – if there is agreement on that, we could rename it to Rosenbluthsian Monte&nbsp;Carlo.</p>
<h2>For more&nbsp;information&#8230;</h2>
<p>If you want to learn about sampling, or Bayesian data analysis, consider reading <a href="https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884">the book</a>, it’s a great read from what I&#8217;ve read so&nbsp;far.</p>
<p>Stay tuned for more of Bayes, or Curry, or Euler, or&nbsp;McCarthy.</p>
  </div>

</article>


  </main>
    <footer>
      <section class="author">
        <div class="author__name">
          <a href="https://www.treszkai.com/pages/about.html">Laszlo Treszkai</a>
          <p></p>
        </div>
        <div class="author__link">
          <ul>
            <li>
              <a href="https://github.com/treszkai" target="_blank" title="github">
                <i class="fab fa-github-square"></i>
              </a>
            </li>
            <li>
              <a href="https://twitter.com/ltreszkai" target="_blank" title="twitter">
                <i class="fab fa-twitter-square"></i>
              </a>
            </li>
            <li>
              <a href="https://www.treszkai.com/feeds/all.atom.xml" target="_blank" title="Feed">
                <i class="fas fa-rss-square"></i>
              </a>
            </li>
          </ul>
        </div>
      </section>
      <div class="ending-message">
        <p>&copy; Laszlo Treszkai. Powered by <a href="http://getpelican.com" target="_blank">Pelican</a>, with theme based on <a href="https://github.com/laughk/pelican-hss" target="_blank">HSS</a> and <a href="https://tonsky.me">Tonsky’s</a>.</p>
      </div>
    </footer>
</body>
</html>