<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004)</title>
  <meta name="description" content="This post is a summary of the seminal paper on inverse reinforcement learning: Pieter Abbeel, Andrew Y. Ng: Apprenticeship Learning via Inverse Reinforcement...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://treszkai.github.io/2019/08/19/irl-summary">
  <link rel="alternate" type="application/rss+xml" title="Laszlo Treszkai" href="/feed.xml"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004) | Laszlo Treszkai</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summary of the seminal paper on inverse reinforcement learning." />
<meta property="og:description" content="Summary of the seminal paper on inverse reinforcement learning." />
<link rel="canonical" href="https://treszkai.github.io/2019/08/19/irl-summary" />
<meta property="og:url" content="https://treszkai.github.io/2019/08/19/irl-summary" />
<meta property="og:site_name" content="Laszlo Treszkai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-19T00:00:00+02:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://treszkai.github.io/2019/08/19/irl-summary"},"description":"Summary of the seminal paper on inverse reinforcement learning.","@type":"BlogPosting","headline":"Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004)","dateModified":"2019-08-19T00:00:00+02:00","datePublished":"2019-08-19T00:00:00+02:00","url":"https://treszkai.github.io/2019/08/19/irl-summary","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://treszkai.github.io/feed.xml" title="Laszlo Treszkai" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-114915944-1', 'auto');
  ga('send', 'pageview');
}
</script>
  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
        var macros_dict = { 
            "\\RR": "\\mathbb{R}"  ,  
            "\\EE": "\\mathbb{E}"  ,  
            "\\parm": "\\textcolor{grey}{\\bullet}"  ,  
            "\\indep": "\\perp\\!\\!\\!\\perp"  ,  
            "\\emptyset": "\\varnothing"  ,  
            "\\proves": "\\vdash"  ,  
            "\\Union": "\\bigcup"  ,  
            "\\Intersect": "\\bigcap"  ,  
            "\\grad": "\\nabla"  ,  
            "\\given": "\\,\\vert\\,"  ,  
            "\\Godel": "\\ulcorner #1 \\urcorner"  
            
        };

        document.querySelectorAll("script[type='math/tex']").forEach(function(el) {
            var text = el.textContent === "" ? el.innerHTML : el.textContent;
            el.outerHTML = katex.renderToString(text, { displayMode: false, macros: macros_dict });
        });

        document.querySelectorAll("script[type='math/tex; mode=display']").forEach(function(el) {
            var text = el.textContent === "" ? el.innerHTML : el.textContent;
            el.outerHTML = katex.renderToString(text, { displayMode: true, macros: macros_dict });
        });
    });
  </script>


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><span class="site-title"><a rel="author" href="/">Laszlo Treszkai</a><span class="pronunciation">read: [laslo treskai] </span></span><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About me</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004)</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-08-19T00:00:00+02:00" itemprop="datePublished">19 Aug. 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This post is a summary of the seminal paper on inverse reinforcement learning: Pieter Abbeel, Andrew Y. Ng: <em>Apprenticeship Learning via Inverse Reinforcement Learning</em> (2004) [<a href="http://ai.stanford.edu/~pabbeel/irl/">link</a>].</p>

<p>Traditional <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html">reinforcement learning</a> (RL) starts with specifying a reward function, and during training we search for policies that maximize this reward function<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. In contrast, inverse reinforcement learning (IRL) starts with expert demonstrations of the desired behavior, infers a reward function that the expert likely followed, and trains a policy to maximize that.</p>

<!-- more -->

<p>IRL is useful for learning complex tasks where it is hard to manually specify a reward function that makes desirable trade-offs between desiderata; such tasks include driving a car or teaching a robot to do a backflip, where we want the car to reach to the destination promptly but also safely, or the robot to flip with its arms straight and <a href="https://youtu.be/xet3KDUfS_U?t=50">sticking the landing</a>.</p>

<p>In contrast with previous attempts at apprenticeship learning (i.e. learning from an expert), which tried to mimic the expert demonstrations directly, IRL assumes that the expert follows a reward function that is a linear combination of the feature vectors (<script type="math/tex">R = w^T φ(s)</script>), and finds a reward function that maximizes the received reward under the set of demonstrations. The hand-specified function <script type="math/tex">φ: S→ℝ^k</script> maps a state of the Markov decision process (MDP) to a feature vector, which vector includes parameters for the different desiderata of the task, such as the distances to objects surrounding the car, the speed of the car, or the current lane.</p>

<p>IRL assumes knowledge of an expert policy <script type="math/tex">π_E</script>, or at least samples from it. Using these, we only care about the estimated “accumulated feature values”, <script type="math/tex">μ(π_E) ∈ ℝ^k</script>, which is the expected discounted sum of the feature vectors if sampled from the policy, because then the value of a policy (parametrised by <script type="math/tex">w</script>) can be calculated from it directly: <script type="math/tex">R = w^T μ(π_E)</script>.</p>

<p>The goal is then to find a policy whose performance is close to that of the expert’s on the unknown reward function <script type="math/tex">R_{\star} = w^T_{\star} φ</script>. This is done by finding a policy whose feature vector is close to the expert’s feature vector, which assures that the value of these policies is close too.</p>

<p>The algorithm for IRL is the following:</p>
<ol>
  <li>Pick a random initial policy, and calculate its <script type="math/tex">μ</script>.</li>
  <li>Find the vector of weights w that lies within the unit ball and <em>maximizes</em> the difference between the expert feature expectations and the feature expectations of our best policy thus far.</li>
  <li>If this maximum is small, then go to step 7.</li>
  <li>Otherwise <script type="math/tex">w</script> is our new weights for <script type="math/tex">R</script>.</li>
  <li>Calculate optimal policy for this <script type="math/tex">R</script>.</li>
  <li>Repeat from step 2.</li>
  <li>Let the agent designer pick a policy from any of those found in step 5 in the different iterations; or find the policy in the convex closure of these policies that is closest to the expert policy.</li>
</ol>

<p>The maximization in step 2 allows us to find a policy that is close to the expert’s, regardless of the choice of a reward function. After all, we are interested in the policy, not the reward function, and so the estimated <script type="math/tex">R</script> is not necessarily correct.</p>

<p>This algorithm is proved to terminate within <script type="math/tex">O(k \log(k))</script> steps, using at least <script type="math/tex">O(k \log(k))</script> number of samples from the expert policy.</p>

<p>Experiments are done in a gridworld environment, where IRL learns the expert policy in approximately 100 times less sample trajectories than simply mimicking the expert. Another experiment is a car driving simulator with 3 lanes viewed from the top, where IRL is capable of learning multiple driving styles, such as “prefer the right lane but avoid collisions”. Video demonstrations of the latter show that the sentiment of the expert policy is indeed followed, although sometimes with unnecessary lane switches (most modern RL algorithms also exhibit this undesired property).</p>

<h4 id="footnotes">Footnotes</h4>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Or, more accurately, a policy that maximizes the expected utility derived from this reward function and some method of temporal discounting. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://treszkai.github.io/2019/08/19/irl-summary';
      this.page.identifier = 'https://treszkai.github.io/2019/08/19/irl-summary';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://treszkai.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript><a class="u-url" href="/2019/08/19/irl-summary" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Laszlo Treszkai</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Laszlo Treszkai</li><li><a class="u-email" href="mailto:laszlo.treszkai@gmail.com">laszlo.treszkai@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/treszkai"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">treszkai</span></a></li><li><a href="https://www.twitter.com/ltreszkai"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">ltreszkai</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <ul class="right-footer-list">
          <li>Explorations in math, AI, ML, et al.
</li>
          <li class="rss-subscribe">Subscribe <a href="/feed.xml">via RSS</a>.</li>
        </ul>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
