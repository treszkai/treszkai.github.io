<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="https://www.treszkai.com/theme/css/style.less">
  <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="icon" type="image/vnd.microsoft.icon" href="https://www.treszkai.com/">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/normalize.css">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/style.css?md5=c06cfd86">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:regular,i,b|Fira+Code">
  <link rel="stylesheet" type="text/css" href="https://www.treszkai.com/theme/css/fontawesome-all.min.css">


  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Laszlo Treszkai">

  <link href="https://www.treszkai.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Laszlo Treszkai Atom" />

<meta property="og:title" content="On the overconfidence of modern neural networks">
<meta property="og:image" content="2019/09/26/overconfidence/">
<meta property="og:description" content="Evaluating various methods to improve the calibration of deep neural networks.">
<meta name="description" content="Evaluating various methods to improve the calibration of deep neural networks."><meta name="keywords" content="">

  <title>
    Laszlo Treszkai
&ndash; On the overconfidence of modern neural&nbsp;networks  </title>

</head>

<body>
  <main>
    <header>
      <div class="site-name">
        <a href="https://www.treszkai.com/">Laszlo Treszkai</a>
      </div>
      <p>
      <a href="https://www.treszkai.com/index.html">Posts</a><span class="separator"></span><a href="https://www.treszkai.com/pages/about-me.html" title="About&nbsp;me">About&nbsp;me</a>      </p>
    </header>

<article>
  <div class="article__title">
    <h1><a href="https://www.treszkai.com/2019/09/26/overconfidence/">On the overconfidence of modern neural&nbsp;networks</a></h1>
  </div>
  <div class="article__meta">
    <p class="article__meta__post-date">Posted on: 26 Sep. 2019</p>
    </p>
  </div>
  <div class="article__text">
    <p><em>On the overconfidence of modern neural networks</em>. This is the title of the coursework I did with a fellow student at the University of Edinburgh. (<span class="caps">PDF</span>: <a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw3.pdf">Part 1</a>, <a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw4.pdf">Part 2</a>.)</p>
<p>Our topic was influenced by a previous study, titled <em>On Calibration of Modern Neural Networks</em> <!-- {% cite Guo2017-calibration %} --> <a class="citation" href="#Guo2017-calibration">(Guo, Pleiss, Sun, <span class="amp">&amp;</span> Weinberger, 2017)</a>.</p>
<p>Applications of uncertainty estimation include threshold-based outlier detection, active learning, uncertainty-driven exploration of reinforcement learning, or certain safety-critical&nbsp;applications.</p>
<h2>What is&nbsp;uncertainty?</h2>
<p>No computer vision system is perfect, so an image classification algorithm sometimes identifies people as not-people, or not-people as people.
While we usually care about the class with the highest output (the “most likely” class), we can treat the softmax outputs of a classifier as uncertainty estimates.
(After all, that is how we trained a model when treating the softmax outputs of a classifier as a probability distribution, and minimizing the negative log likelihood of the model given the data.)
For example, out of 1000 classifications made with an output of 0.8, approximately 800 should be correct <em>if the system is well-calibrated</em>.</p>
<p><img alt="Example output of a YOLO object detection network" src="https://www.treszkai.com/2019/09/26/overconfidence/yolo.png"></p>
<p>(Example output of a <span class="caps">YOLO</span> object detection network, with the probability estimates. Image source: <a href="https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/">Analytics Vidhya</a>.)</p>
<p>Ideally, we want our system to be 100% correct, but we rarely have access to an all-knowing Oracle. In cases where it is hard to distinguish between two categories (like on the cat-dog below) we want the uncertainties to be well-calibrated, so that predictions are neither overly confident nor insufficiently&nbsp;confident.</p>
<p><img alt="Image of a cat that could be mistaken for a dog" src="https://www.treszkai.com/2019/09/26/overconfidence/catdog.jpeg"></p>
<p>(Image source: Google&nbsp;Brain)</p>
<h2>Our&nbsp;results</h2>
<h3>Interim&nbsp;report</h3>
<p><a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw3.pdf">Link to report (<span class="caps">PDF</span>)</a></p>
<p>Our initial experiments showed that our baseline model is already well-calibrated when trained on the <span class="caps">EMNIST</span> By-Class dataset.
Calibration worsened when we used only a subset of the training set.
We found that increasing regularization increases calibration, but too much regularization leads to a decrease in both accuracy and calibration. (See figure below.)
This contradicts the findings of <!-- {% cite Guo2017-calibration -L section -l 3 %} --> <a class="citation" href="#Guo2017-calibration">(Guo, Pleiss, Sun, <span class="amp">&amp;</span> Weinberger, 2017, sec. 3)</a>, who found that model calibration can improve by increasing the weight decay constant, well after the model achieves minimum classification accuracy.
One of our main findings is that cross-entropy error is not a good indicator of model&nbsp;calibration.</p>
<p><img alt="Figure 5 of our interim report." src="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw3-fig5.png"></p>
<p>(<span class="caps">ECE</span>: expected calibration error. The lower the&nbsp;better.)</p>
<h3>Final&nbsp;report</h3>
<p><a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw4.pdf">Link to report (<span class="caps">PDF</span>)</a></p>
<p>We replicate the findings of <!-- {% cite Guo2017-calibration %} --> <a class="citation" href="#Guo2017-calibration">(Guo, Pleiss, Sun, <span class="amp">&amp;</span> Weinberger, 2017)</a>£ that deep neural networks achieve higher accuracy but worse calibration than shallow nets, and compare different approaches for improving the calibration of neural networks (see figure below). As the baseline approach, we consider the calibration of the softmax outputs from a single network; this is compared to <em>deep ensembles</em>, <em><span class="caps">MC</span> dropout</em>, and <em>concrete dropout</em>. Through experiments on the <span class="caps">CIFAR</span>-100 data set, we find that a large neural network can be significantly over-confident about its predictions. We show on a classification problem that an ensemble of deep networks has better classification accuracy and calibration compared to a single network, and that <span class="caps">MC</span> dropout and concrete dropout significantly improve the calibration of a large&nbsp;network.</p>
<p><img alt="Confidence and calibration plots for BigNet. (Figure 2 of our report)" src="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw4-fig2.png"></p>
<p>(<em>Top row:</em> confidence plots for a deep neural net. The more skewed to the right, the better. <em>Bottom row:</em> corresponding calibration plots. The more close to the diagonal, the&nbsp;better.)</p>
<h2>Things I would do&nbsp;differently</h2>
<p>With a little more experience behind my back now, I would make the following changes in experiment design and writing the&nbsp;report:</p>
<ul>
<li><em>Use a validation set.</em> We only used a training set because we trained for minimum error, and we expected <em>calibration</em> to be independent from <em>accuracy</em>, but that is a strong assumption (and likely incorrect, seeing our results in the interim&nbsp;report).</li>
<li><em>Use better biblography sources.</em> Instead of Google Scholar, I would search <a href="https://dblp.uni-trier.de/"><span class="caps">DBLP</span></a>, where the information is more correct and&nbsp;consistent.</li>
<li><em>Use pastel colors.</em> I let my collaborator have it his way, but ever since this submission I’m having nightmares in purple and glowing green&nbsp;:D</li>
</ul>
<p>In future work, I would like to test the calibration of a Bayesian neural network, where the weights of the network have a probability distribution instead of a point&nbsp;estimate.</p>
<h2>References</h2>
<!&#8212; {% bibliography &#8212;cited %} &#8212;>
<ol class="bibliography"><li><span id="Guo2017-calibration">Guo, C., Pleiss, G., Sun, Y., <span class="amp">&amp;</span> Weinberger, <span class="caps">K. Q.</span>(2017). On Calibration of Modern Neural Networks. In D. Precup <span class="amp">&amp;</span> <span class="caps">Y. W.</span> Teh (Eds.), <i>Proceedings of the 34th International Conference on Machine Learning</i> (Vol. 70, pp. 1321–1330). International Convention Centre, Sydney, Australia: <span class="caps">PMLR</span>. Retrieved from&nbsp;http://proceedings.mlr.press/v70/guo17a.html</span></li></ol>
  </div>

</article>


  </main>
    <footer>
      <section class="author">
        <div class="author__name">
          <a href="https://www.treszkai.com/pages/about.html">Laszlo Treszkai</a>
          <p></p>
        </div>
        <div class="author__link">
          <ul>
            <li>
              <a href="https://github.com/treszkai" target="_blank" title="github">
                <i class="fab fa-github-square"></i>
              </a>
            </li>
            <li>
              <a href="https://twitter.com/ltreszkai" target="_blank" title="twitter">
                <i class="fab fa-twitter-square"></i>
              </a>
            </li>
            <li>
              <a href="https://www.treszkai.com/feeds/all.atom.xml" target="_blank" title="Feed">
                <i class="fas fa-rss-square"></i>
              </a>
            </li>
          </ul>
        </div>
      </section>
      <div class="ending-message">
        <p>&copy; Laszlo Treszkai. Powered by <a href="http://getpelican.com" target="_blank">Pelican</a>, with theme based on <a href="https://github.com/laughk/pelican-hss" target="_blank">HSS</a> and <a href="https://tonsky.me">Tonsky’s</a>.</p>
      </div>
    </footer>
</body>
</html>