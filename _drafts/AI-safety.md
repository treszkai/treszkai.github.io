None of us knows the precise timeline for AGI.

Incentives for:
 - uncertainty quantification
 - interpretability
 - robustness (e.g. against adversarial attacks, on new environments)

There are market incentives in place for not harming individual humans, but not for not extinguishing the human race.

If we could feel the consequences of all possible outcomes of an action, and would discount correctly the future rewards, then we'd be much more careful about many things (personal level: smoking; human level: negotiating with countries armed with nuclear weapons, and developing AGI)

# Assumptions made by different AI safety agendas

## MIRI's

https://intelligence.org/2017/02/28/using-machine-learning/

1. Future AI systems are likely to look like more powerful versions of present-day ML systems in many ways. We may get better deep learning algorithms, for example, but we’re likely to still be relying heavily on something like deep learning.
    - Alternatively, you may think that AGI won’t look like modern ML in most respects, but that the ML aspects are easier to productively study today and are unlikely to be made completely irrelevant by future developments.
2. Artificial general intelligence (AGI) is likely to be developed relatively soon (say, in the next couple of decades).
    - Alternatively, you may think timelines are long, but that we should focus on scenarios with shorter timelines because they’re more urgent.
3. Building task-directed AGI is a good idea, and we can make progress today studying how to do so.
    - TODO: describe task-directed AGI.
