---
layout: post
title:  "Explore what’s “trivial”"
categories: math
scriptarr:
 - "https://d3js.org/d3.v4.min.js"
stylearr:
 - "css/graphs.css"
---

I experienced a big mental shift when I realized that I don't have to believe anything when it comes to mathematics: every "true" statement is either an axiom, or could be derived from axioms. (At least, that is a more sensible definition of "trueness" than "whatever the professor said".) I don't mean that I only put high credence on propositions whose proof I have seen, because that's far from the case, and it's fine. I mean that a _small_ set of axioms can generate almost anything that mankind has proved, and that _in theory_ I could look up the proof of any of those propositions and could follow them step-by-step myself – this is fascinating to me.
<!-- Yes, even the proofs proved with the help of automatic theorem provers. -->

I often find hidden gems of knowledge behind mathematical statements that most of us take for granted. For example, every second-year math student knows that every linear transformation TODO(or mapping?)
over a complex vector space has an eigenvalueTODO decomposition: but the way this decomposition works with complex eigenvalues is quite interesting.
The first week of machine learning studies teach that we must not evaluate models on the same set of examples on which the model was trained – but why, what happens when we use the training set for model validation?
It's easy to prove that the mean and variance of the sum of two statistically independent random variables _X_ and _Y_ is the sum of their means and variances, and we're not at all surprised when the sum of two independent normal distributions will be normally distributed – but unless we have seen a proof of this theorem, we could just as well be surprised.

Or, one could just say that these things are all trivial, or obvious, or useless. To me, that's boring thinking. I find all of it interesting and almost always rewarding!

In this blog I am going to share some of my explorations into mathematics and artificial intelligence and machine learning. However, I will not restrain myself just because something _looks_ trivial.

As a teaser, here is what happens behind the scenes when you add two normally distributed (a.k.a. Gaussian) random variables together.

_Top chart:_ the probability density function (pdf) of a Gaussian r.v. _X_ with mean –1 and standard deviation 1 in solid blue, a Gaussian r.v. _Y_ with mean 3 and standard deviation 2 in solid red, and the r.v. of _z–Y_ in dashed red (which happens to be also Gaussian).

_Middle chart:_ the product of the pdfs of _X_ and _z-Y_.

_Bottom chart:_ for every value of _z_ the area under the product of pdfs _X_ and _z-Y_, i.e. the convolution of the pdfs of _X_ and _Y_, which is the pdf of the r.v. _X+Y_. (And, this also happens be a Gaussian.)

<p><svg id="chart"></svg></p>

<script src="scripts/gaussians.js"></script>

**Hint:** move the mouse the bottom chart to change the value of _z._
