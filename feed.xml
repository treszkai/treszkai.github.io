<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://www.treszkai.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.treszkai.com/" rel="alternate" type="text/html" /><updated>2020-05-02T23:53:50+02:00</updated><id>https://www.treszkai.com/feed.xml</id><title type="html">Laszlo Treszkai</title><subtitle>Explorations in math, AI, ML, et al.
</subtitle><entry><title type="html">The effects of daylight savings time adjustment on the incidence rate of acute myocardial infarction: a Bayesian meta-analysis (original research)</title><link href="https://www.treszkai.com/blog/dst-vs-ami" rel="alternate" type="text/html" title="The effects of daylight savings time adjustment on the incidence rate of acute myocardial infarction: a Bayesian meta-analysis (original research)" /><published>2019-11-11T00:00:00+01:00</published><updated>2019-11-11T00:00:00+01:00</updated><id>https://www.treszkai.com/blog/dst-vs-ami</id><content type="html" xml:base="https://www.treszkai.com/blog/dst-vs-ami">&lt;p&gt;&lt;em&gt;Laszlo Treszkai (firstname.lastname@gmail.com)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Version of 11 November, 2019.&lt;/p&gt;

&lt;p&gt;This document might be revised in the future; any potential updates will be linked from here.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;Multiple observational studies claim that the daylight savings time (DST) adjustment in spring causes an increase in acute myocardial infarction (AMI) count during the following days or weeks, attributing this increase to the reduction in sleep or the disturbance in the circadian rhythm. Previous studies used frequentist methods for interval estimation and often showed “statistically significant” differences, although the results were inconsistent and sometimes the effects in the same study were incoherent (such as a significant difference on Tuesday but not on Monday). A recent meta-analysis used frequentist methods and showed an increase in incidence rate after the spring adjustment and could not show a change after the autumn adjustment.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods&lt;/h3&gt;

&lt;p&gt;This study reanalyzes the data described in the relevant observational studies. We propose a Bayesian model that should capture the alleged phenomenon truthfully, apply this model consistently to every study, and combine the results using a fixed-effects model. Under our model, the risk ratio on Monday is the highest, it is slightly lower on Tuesday, and it decreases linearly to 1 until Saturday. We do the calculations using both analytic methods and Monte Carlo methods with the Stan software.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;In total, 7 observational studies were identified and analyzed, from which one was excluded. The remaining 6 studies included 14,024 AMI incidences on the week following spring DST adjustment, and 15,921 incidences on the week following autumn DST adjustment.
Together with related trend data obtained from the surrounding weeks, these figures show a risk ratio (RR) of 107.7% on the Monday following a spring DST change (95% credible interval: [104.8%, 110.7%]), and a mean RR of 97.7% (95% CrI: [95.1%, 100.3%]) after the autumn DST change. The results from analytic and Monte Carlo methods matched precisely. The credible intervals obtained from a non-informative prior yield practically the same results, and so does a slightly more complex model for the time decay of the effect.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Overall, the spring DST adjustment has a small but quasi-certain positive effect on AMI incidences, and the risk ratio in autumn is approximately 1 or slightly less than 1.
We note that the combined RR is less than half of what has been suggested by certain smaller but highly cited studies, but our analysis shows larger effects than the recent meta-analysis of the same data by Manfredini et al. (2019).
Our results give strong support to the hypothesis that the DST transitions – especially the spring transition when sleep is reduced – have a noticeable effect on our circadian rhythm.
Nonetheless, we cannot confidently claim that these results are of direct practical importance: there is no evidence that the additional AMI counts in the days after DST transition are not merely shifted earlier from the following weeks.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This study has a two-fold purpose. First, it compiles all the published data about the effects of DST on the risk of AMI, and presents a meta-analysis where the data from multiple countries and years is analyzed in a unified model. On the other hand, it demonstrates the use of Bayesian methods in an analysis or meta-analysis, explaining the thinking behind model specification and quantifying our prior beliefs about the parameters. The software required for reproducing this paper is freely available at &lt;a href=&quot;https://github.com/treszkai/BayesianScience&quot;&gt;https://github.com/treszkai/BayesianScience&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sipilä et al. (2016) explain the importance of sleep and its effects on the risk of heart disease:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sleep is essential for well-being and its disturbances
have been associated with disruption of numerous
physiological processes and changes in cardiovascular
risk factors (1,2). Sleep disordered breathing has been
associated with risk of coronary heart disease (3,4) and
sleep impairment with prognosis of myocardial infarction
(MI) (5).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Daylight saving time (DST) is used in many countries
including the United States and the members of
the European Union for prolonging of sun-light
proportion of day. Clock shifts however alter and disrupt
chronobiological rhythms and impair sleep (7,8) providing
a ‘‘natural experiment’’ for studying the effects of
rhythm and sleep disruptions on the incidence of
vascular events. Although chronobiological factors
have been shown to affect the incidence of MI (9,10),
studies on the association of DST and the incidence of MI
have been partly conflicting. With one exception (11), all
studies show changes in the temporal distribution of MI
in the week following DST transitions but the patterns of
change differ (12–15) and there is no agreement about
the impact of these changes on the overall incidence of
MI (11–16).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We will see that there is a simple reason for the disagreement between studies: most of the studies have been critically underpowered.&lt;/p&gt;

&lt;p&gt;Although the majority of medical research uses frequentist methods, this is not the first meta-analysis in medicine that uses Bayesian statistics. The following are some noteworthy examples:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Gelman et al. (2013) present an example for estimating mortality ratios after a myocardial infarction between the control group and a group that uses beta-blockers, using data from 22 independent studies.&lt;/li&gt;
  &lt;li&gt;Devin Incerti (2015) provides a Bayesian re-analysis of the effects of mammography on breast cancer-related mortality rates.&lt;/li&gt;
  &lt;li&gt;Yang et al. (2017) analyze 25 randomized controlled trials of prokinetics for the treatment of functional dyspepsia in a Bayesian network meta-analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methodology-shared-in-most-papers&quot;&gt;Methodology shared in most papers&lt;/h3&gt;

&lt;p&gt;Following the naming of (Čulić 2013), we refer to the week following the DST adjustment as “posttransitional week”.&lt;/p&gt;

&lt;p&gt;Every study that was included compares the observed AMI counts against a trend prediction. The trend prediction for AMI counts on given days – sometimes called “control group” – was usually defined as the average of the respective days on the two weeks before and after the posttransitional week. The analysis of Sandhu et al. (2014) was the only exception, as they used a regression model that included AMIs from all year except the two weeks following the spring and autumn DST adjustments.&lt;/p&gt;

&lt;p&gt;Years on which the DST adjustment coincided with Easter were usually excluded from the studies. If Easter fell on the 2 weeks following (or preceding) the DST adjustment, the control period was the two out of three weeks that did not include Easter.&lt;/p&gt;

&lt;p&gt;Every paper adjusted the AMI counts for the shorter (resp. longer) Sunday following a spring (resp. autumn) DST transition by multiplying the real counts with &lt;script type=&quot;math/tex&quot;&gt;24/23&lt;/script&gt; (resp. &lt;script type=&quot;math/tex&quot;&gt;24/25&lt;/script&gt;). This sometimes resulted in fractional AMI counts, which we rounded to the nearest integer when treated as an observation.&lt;/p&gt;

&lt;h2 id=&quot;materials-and-methods&quot;&gt;Materials and methods&lt;/h2&gt;

&lt;h3 id=&quot;study-selection&quot;&gt;Study selection&lt;/h3&gt;

&lt;p&gt;We analyzed data from every study that was included in the meta-analysis of Manfredini et al. (2019).&lt;/p&gt;

&lt;p&gt;Performing a PubMed search instead of using the list of publications from (Manfredini et al. 2019) would be a tedious process with little benefit: said meta-analysis retrieved 2633 papers dated up to 31 December 2018 (from which 7 were relevant).&lt;/p&gt;

&lt;h3 id=&quot;analyzed-data&quot;&gt;Analyzed data&lt;/h3&gt;

&lt;p&gt;From each paper, we extracted the trend predictions and the actual AMI counts on each day of the spring and autumn posttransitional weeks. When the trend prediction was not available, we divided the total number of AMI cases by the study length in days. We restricted our analysis to the number of incidences, and ignored all variables that describe incidences, such as age and gender of patient, STEMI (ST elevation MI) or non-STEMI, or various medications taken prior to the incident.&lt;/p&gt;

&lt;h3 id=&quot;problems-with-standard-statistical-tests&quot;&gt;Problems with standard statistical tests&lt;/h3&gt;

&lt;p&gt;The standard statistical practice for deciding whether there is a difference in a particular variable (such as AMI counts) between two groups is to use a &lt;em&gt;null hypothesis significance test&lt;/em&gt; (NHST).
Using this method, one defines a &lt;em&gt;null hypothesis&lt;/em&gt; as the variable of interest having some predetermined value, which in this case would correspond to zero increase in AMI counts after a DST change.
The NHST answers the question: assuming the null hypothesis is true, what is the probability that data which is generated according to the sampling and testing intentions has a more extreme test statistic than that of the actual observations (Kruschke, Liddell 2018). If this probability is less than some fixed threshold (typically 0.05), the effect is claimed to exist.
The NHST suffers from a multitude of problems, and has received its fair share of criticism from statisticians.
It encourages black-and-white thinking without allowing uncertainty (claiming that an effect either exists or not, depending on the p-value), it encourages binary classification of effects without quantifying the relationship (&lt;em&gt;statistically&lt;/em&gt; significant differences might be of no &lt;em&gt;practical&lt;/em&gt; relevance if they are small), and these tests are conducted &lt;em&gt;against&lt;/em&gt; a given null hypothesis without any way to gain evidence &lt;em&gt;for&lt;/em&gt; the null hypothesis (an inability to refute the null hypothesis is not equal to accepting it).
Recently, The American Statistician released a special issue titled &lt;em&gt;Moving to a World Beyond “p &amp;lt; 0.05”&lt;/em&gt; (Wasserstein 2019), together with commentaries from 94 authors.&lt;/p&gt;

&lt;p&gt;We can get a more accurate sense of the value of the parameter if instead of testing a hypothesis, we estimate the value of the parameter. The standard tool for this is stating the 95% confidence interval (CI) for a parameter, which is the set of parameter values that wouldn’t be rejected at the &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
p&lt;0.05 %]]&gt;&lt;/script&gt; level. This is the approach suggested by Cumming (2014) and Cumming and Calin-Jageman (2016), who call it the &lt;em&gt;New Statistics&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;While reporting intervals is better than a single value from it (i.e. the p-value), confidence intervals still suffer from deep-rooted flaws. It still encourages black-and-white thinking: parameter values inside the CI are compatible with the null hypothesis, those outside it are not. Confidence intervals do not give distributional information, i.e. a value close to the limits of the CI is not “less compatible” with the hypothesis then a value in the middle, nor is a study of large sample size “more confident” than a smaller study (although usually the CI of a large study is narrower). This binary nature makes it hard to aggregate the results of multiple studies and to perform a meta-analysis accurately. In addition, confidence intervals are also frequently misinterpreted: specifically, the true parameter value is &lt;em&gt;not&lt;/em&gt; 95% likely to be inside the CI, although they are often thought to be.&lt;/p&gt;

&lt;p&gt;Kruschke and Liddell (2018) compare approaches to statistical inference along two axes: whether the method uses a frequentist or Bayesian framework, and whether the method compares hypotheses or estimates parameter values. They make a detailed case that Bayesian parameter estimation is superior in most situations to the frequentist methods or Bayesian hypothesis testing, hence the title of the paper, &lt;em&gt;The Bayesian New Statistics&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;overview-of-our-model-and-statistical-methods&quot;&gt;Overview of our model and statistical methods&lt;/h3&gt;

&lt;p&gt;In this meta-analysis we define a (Bayesian) statistical model for the parameter of interest and our observations. For every paper, we have the following observations: the AMI counts on each day of the posttransitional week, and the AMI counts predicted by the trend. The unobserved parameter is the risk ratio (RR), i.e. the multiplier by which mean AMI counts increase in the posttransitional week, compared to the same day of an ordinary week. Our description of this parameter initially also include some reasonable uncertainty in our beliefs, quantified in the &lt;em&gt;prior distribution&lt;/em&gt;. The goal of the analysis is to derive the &lt;em&gt;posterior probability distribution&lt;/em&gt; of the RR (or &lt;em&gt;posterior&lt;/em&gt; for short), which is an adjustment of the prior probabilities based on the likelihood of each parameter value, i.e. the probability that a given parameter value would produce the observed data. Although the posterior is influenced by the prior and the statistical model, this influence can be insubstantial in the face of enough data, as will be the case in this analysis. Finally, the posterior is summarized in a 95% credible interval of parameter values, which is either a central credible interval or a highest density posterior interval.&lt;/p&gt;

&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;

&lt;p&gt;For a particular study &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;t_i^{(s)}&lt;/script&gt; denotes the AMI counts as predicted by the trend model on day &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of the posttransitional week (with &lt;script type=&quot;math/tex&quot;&gt;d = 1,\,\ldots,\,5&lt;/script&gt; for Monday, …, Friday after the DST change) and &lt;script type=&quot;math/tex&quot;&gt;y_d^{(s)}&lt;/script&gt; denotes the observed count on day &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. The (unobserved) mean of the distribution of &lt;script type=&quot;math/tex&quot;&gt;y_d^{(s)}&lt;/script&gt; is denoted by &lt;script type=&quot;math/tex&quot;&gt;x_d^{(s)}&lt;/script&gt; – the meaning of this variable will become clear in the next section.
The risk ratio for day &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is denoted by &lt;script type=&quot;math/tex&quot;&gt;r_d^{(s)} = x_d^{(s)} / t_d^{(s)}&lt;/script&gt;. Finally, &lt;script type=&quot;math/tex&quot;&gt;\mathcal D^{(s)}&lt;/script&gt; denotes the whole dataset, i.e. all of the observations &lt;script type=&quot;math/tex&quot;&gt;\{y_1^{(s)},\ldots,y_5^{(s)}\}&lt;/script&gt;. To avoid cluttered notation, sometimes the superscript is omitted, resulting in e.g. &lt;script type=&quot;math/tex&quot;&gt;y_1&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;poisson-distribution&quot;&gt;Poisson distribution&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_distribution&quot;&gt;Poisson distribution&lt;/a&gt; is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. In our case, the “event” is an AMI, and the fixed interval of time is a day. Although AMIs don’t happen at a constant rate throughout the day, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_distribution#Sums_of_Poisson-distributed_random_variables&quot;&gt;sum of Poisson-distributed random variables&lt;/a&gt; is also Poisson-distributed, so any day’s total will also be Poisson-distributed.&lt;/p&gt;

&lt;p&gt;The distribution has a single parameter, which is a positive real number, and is often denoted &lt;script type=&quot;math/tex&quot;&gt;λ&lt;/script&gt;. The mean (expected value) of &lt;script type=&quot;math/tex&quot;&gt;\text{Poisson}(λ)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;λ&lt;/script&gt;, and the standard deviation is &lt;script type=&quot;math/tex&quot;&gt;\sqrt{λ}&lt;/script&gt;. Its probability mass function is shown below for &lt;script type=&quot;math/tex&quot;&gt;λ=100&lt;/script&gt;, along with the 95% highest density interval (HDI) – the shortest interval that covers 95% of the probability mass.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/poisson-dist.svg&quot; alt=&quot;Distribution of Poisson plot with mean 100&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The analyzed studies reported the sum of AMIs on a given day over the period of the study (e.g. all posttransitional Tuesdays during the years 2010–2013), never the AMI counts for individual years. This sum is denoted with &lt;script type=&quot;math/tex&quot;&gt;y_d&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; signifies the day. We note again that the individual counts are each Poisson-distributed, so their sum is Poisson-distributed too. (However, their &lt;em&gt;average&lt;/em&gt; would not be Poisson-distributed.) This means that &lt;script type=&quot;math/tex&quot;&gt;y_d&lt;/script&gt; is sampled from a Poisson distribution whose parameter &lt;script type=&quot;math/tex&quot;&gt;x_d&lt;/script&gt; is the sum of the trend on day &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; over the period of the study (&lt;script type=&quot;math/tex&quot;&gt;t_d&lt;/script&gt;), multiplied with the RR for the given day (&lt;script type=&quot;math/tex&quot;&gt;r_d&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;In order for the Poisson assumption to &lt;em&gt;not hold&lt;/em&gt; in this analysis, two individuals experiencing an AMI on a given day need to be statistically dependent &lt;em&gt;conditional on the day’s average&lt;/em&gt;. This is not the case during a heat wave or a news broadcast about a major catastrophe, when the AMIs are dependent but not conditionally dependent. The rare scenarios for conditional dependence are when two people partake in a strenuous activity together (such as hiking), or when the AMI of a person causes an AMI in another.&lt;/p&gt;

&lt;h3 id=&quot;model-of-posttransitional-ami-counts&quot;&gt;Model of posttransitional AMI counts&lt;/h3&gt;

&lt;p&gt;We perform the analysis using a fixed-effects model, which assumes that the DST adjustment effects an identical increase in AMI counts in every country, every year. The independence of region is a strong assumption because the leading hypothesis attributes the increase in myocardial infarctions to the disruption of the circadian rhythm, and those beyond their working age do not necessarily experience sleep loss on a posttransitional Monday. Therefore, we hypothesize that the effect is likely to be lower in countries where the average age of retirement is lower – a random-effects model could account for these differences. The independence of year is a weak assumption.&lt;/p&gt;

&lt;p&gt;The model for the AMI count on a posttransitional Monday is described by the following graph – such a graph is called a Bayes network or a directed graphical model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/tikz_bayesnet_Mon.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Loosely speaking, the arrows denote causal or logical dependencies, where the exact formula for the dependency is shown next to the nodes (in a canonical Bayes network, the formulas are described only in the text). The model can be translated into the following sentences:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The observed posttransitional AMI count on Monday follows a Poisson distribution.&lt;/li&gt;
  &lt;li&gt;The mean of the posttransitional AMI count on Monday is equal to the trend count on Monday, multiplied by the RR on Monday.&lt;/li&gt;
  &lt;li&gt;Monday’s RR is a random variable, meaning it has an associated prior belief distribution (which we define below).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;moving-to-a-multi-day-model&quot;&gt;Moving to a multi-day model&lt;/h3&gt;

&lt;p&gt;The reviewed literature performed hypothesis tests for every day of the posttransitional week – including weekends, sometimes noting a significant difference for Tuesday, but not Monday (Sandhu 2014). Such day-by-day tests of “statistical significance” need not concern themselves of &lt;em&gt;consistency&lt;/em&gt; – in the everyday sense of the word –, i.e. that prior to observations we expect any effect to be highest on Monday and wear off as time progresses.&lt;/p&gt;

&lt;p&gt;When performing a Bayesian analysis, we &lt;em&gt;must&lt;/em&gt; have prior expectations on the expected parameter values – these prior beliefs are then changed according to the model and the observed data, resulting in the posterior distribution. In accordance with the literature, we assume that the effect is constrained to the posttransitional week, and that if there is an effect on Monday, there is some effect on Friday too. We expect no increase on Sunday, the day of the adjustment (after adjusting for the shorter day), because relatively few people wake up at the same time on Sundays (and sleep shorter as a consequence). On Tuesday, Wednesday, Thursday, Friday, we expect the relative increase to be 80%, 60%, 40%, 20% of the increase on Monday (see figure below) – this we call the “linear weekday model”. (This linear assumption will be weakened in a later analysis.) We denote the increase in RR on Monday with &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (the only parameter of the model), thus &lt;script type=&quot;math/tex&quot;&gt;r_\text{Mo} = 1 + \theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;r_\text{Tu} = 1 + 0.8 \cdot \theta&lt;/script&gt;, …, &lt;script type=&quot;math/tex&quot;&gt;r_\text{Fr} = 1 + 0.2 \cdot \theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The infarction counts on neighboring days are conditionally independent given &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; (apart from exceptional cases, such as a mass catastrophe), which means we can model the days separately and simply multiply their likelihoods. (Prior to observing the data, it feels &lt;em&gt;very&lt;/em&gt; unlikely to us that there would be any effect on Friday, but one paper attempted to measure effects on the 2 and 4 weeks following DST adjustment, meaning they didn’t think such a long-lasting effect is completely implausible, therefore we consider including Friday as part of the expert opinion.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/rr_example.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This model of all weekdays is described by the following graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/tikz_bayesnet.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here the rectangle means the nodes inside it should be repeated for &lt;script type=&quot;math/tex&quot;&gt;d = \text{Mo}..\text{Fr})&lt;/script&gt; – this rectangle is called a “plate”. A common parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; determines &lt;script type=&quot;math/tex&quot;&gt;r_d&lt;/script&gt; for a given day &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;, which, together with &lt;script type=&quot;math/tex&quot;&gt;t_d&lt;/script&gt;, determines the number of expected AMIs (&lt;script type=&quot;math/tex&quot;&gt;x_d&lt;/script&gt;) and actual AMIs (&lt;script type=&quot;math/tex&quot;&gt;y_d&lt;/script&gt;).&lt;/p&gt;

&lt;h3 id=&quot;prior-beliefs-about-rr-spring&quot;&gt;Prior beliefs about RR (spring)&lt;/h3&gt;

&lt;p&gt;We would like to estimate the value of a continuous parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, where the standard procedure is to conduct a one-sided t-test, with the null hypothesis defined as &lt;script type=&quot;math/tex&quot;&gt;\theta = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Gelman et al. (2013) suggest beginning Bayesian data analysis with a noninformative or &lt;em&gt;weakly informative prior&lt;/em&gt; – this avoid biasing the results to any particular value, and lets the posterior represent the data more closely.&lt;/p&gt;

&lt;p&gt;I believe &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is likely to be approximately &lt;script type=&quot;math/tex&quot;&gt;0.0&lt;/script&gt; (i.e., &lt;script type=&quot;math/tex&quot;&gt;\text{RR} \approx 1&lt;/script&gt;, no effect), but it wouldn’t be very surprising if &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; were positive. (I find it very unlikely, less than &lt;script type=&quot;math/tex&quot;&gt;\approx 0.1\%&lt;/script&gt;, that the RR decreases.) So I would like to place substantial probability mass close to 0.0, and spread the rest on values between &lt;script type=&quot;math/tex&quot;&gt;0.0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1.0&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;P(\theta &gt; 1.0) \lessapprox 0.1\%&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;We can formalize this description by placing 50-50% of the prior probability mass of either there being zero effect (a Gaussian distribution with standard deviation of 0.01), or there being an increase in AMI counts, where the increase in RR has an Exponential(&lt;script type=&quot;math/tex&quot;&gt;\lambda=0.2^{-1}&lt;/script&gt;) prior on it. (An Exponential(&lt;script type=&quot;math/tex&quot;&gt;\lambda=0.2^{-1}&lt;/script&gt;) distribution has a mean of &lt;script type=&quot;math/tex&quot;&gt;0.2&lt;/script&gt;.) This distribution is plotted on the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/prior_Monday.svg&quot; alt=&quot;Prior distribution of Monday RR&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;prior-beliefs-about-the-rr-autumn&quot;&gt;Prior beliefs about the RR (autumn)&lt;/h3&gt;

&lt;p&gt;The AMI counts on the autumn posttransitional week used the same model as the spring counts, but it assumed an (improper) uniform prior on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. (This prior is improper because no distribution exists that is uniformly distributed on the whole linear number line. In practice we would get the same posterior if we assumed a Uniform(−2,+2) prior.)&lt;/p&gt;

&lt;h3 id=&quot;summary-of-assumptions&quot;&gt;Summary of assumptions&lt;/h3&gt;

&lt;p&gt;Every statistical test makes assumptions about the data, but in most reports using null hypothesis tests significance tests, these assumptions are never mentioned, instead they are implicit in the performed tests.
Therefore, statistics is often sold as a sort of alchemy that transmutes randomness into certainty, an “uncertainty laundering” that begins with data and concludes with success as measured by statistical significance (Gelman 2016).
I view it as a &lt;em&gt;strength&lt;/em&gt; of Bayesian data analysis that these assumptions must be stated explicitly. To summarize this section, we make the following assumptions in this analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Every region that use DST has the same RR in every year.&lt;/li&gt;
  &lt;li&gt;Any effect is limited to the posttransitional weekdays, and the effect is highest on Monday, 20% less on Tuesday, and so on until 0% on Saturday.&lt;/li&gt;
  &lt;li&gt;Our prior belief on the spring RR is split half-half between &lt;script type=&quot;math/tex&quot;&gt;1.0&lt;/script&gt; and all values greater than &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, with the probability decaying exponentially at a rate of &lt;script type=&quot;math/tex&quot;&gt;0.2^{-1}&lt;/script&gt;. We make no prior assumptions about the autumn RR.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;posterior-calculations-analytically&quot;&gt;Posterior calculations analytically&lt;/h3&gt;

&lt;p&gt;We performed our calculations for the fixed-effects model in spring analytically, using custom software written in Python. The result of these calculations was a 95% central credible interval, which is an interval of parameter values containing 95% of the posterior probability, with 2.5% on the negative and positive ends. This is not equal to the HDI when the distribution is skewed, but is usually a good approximation.&lt;/p&gt;

&lt;h3 id=&quot;posterior-calculations-with-monte-carlo-methods&quot;&gt;Posterior calculations with Monte Carlo methods&lt;/h3&gt;

&lt;p&gt;We also performed our posterior calculations with Monte Carlo methods using the open source statistical modeling software &lt;a href=&quot;https://mc-stan.org/&quot;&gt;Stan&lt;/a&gt;. Models in Stan are written using its own description language (which comes with &lt;a href=&quot;https://mc-stan.org/users/documentation/&quot;&gt;extensive documentation&lt;/a&gt; and a &lt;a href=&quot;https://discourse.mc-stan.org/&quot;&gt;supportive community&lt;/a&gt;), and they need to be first compiled into binary form using an interface in R, Python, or other languages. Then, after providing the observable data to the model, Stan draws samples from the posterior distribution of the parameters, and calculates the 95% highest posterior density interval (HDI, a.k.a. HPD) – the interval that covers the most plausible parameter values. For most practical purposes, 1000 independent samples would be enough, but we drew 50,000 samples to accurately assess the equality to the analytic solution.&lt;/p&gt;

&lt;p&gt;The code for the fixed-effects linear weekday Stan model is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-stan&quot;&gt;data {
  int DAYS;            // Number of days
  int STUDIES;         // Number of studies
  real NORMAL_SIGMA;   // The standard deviation of the normal component of the prior
  real EXPON_BETA;     // The beta parameter of the exponential component of the prior

  // The observed AMI counts and the trend predictions, for each day of each study
  int&amp;lt;lower=0&amp;gt; ami_obs[STUDIES, DAYS];
  real&amp;lt;lower=0&amp;gt; ami_trend[STUDIES, DAYS];
}

parameters {
  // Monday RR - 1.
  // (We cannot model RR_Mon directly because cannot assign a
  //   common distribution for that.)
  // Its probabilistic value is assigned in the model block below.
  real rr_Mon_minus_1;
}

transformed parameters {
  // The RR for every day
  real rr_day[DAYS];
  // The posttransitional AMI counts for every day of every study.
  real ami_dst_mean[STUDIES, DAYS];

  // Specifying the RR for every day, using the linear weekday model.
  for (i in 1:DAYS) {
    rr_day[i] = (rr_Mon_minus_1 * (DAYS + 1 - i) / DAYS) + 1;
  }

  for (s in 1:STUDIES) {
    for (i in 1:DAYS) {
      ami_dst_mean[s][i] = ami_trend[s][i] * rr_day[i];
    }
  }
}

model {
  // Mixture models are specified using the construct below:
  // target += log_sum_exp(c1 * XXX_lpdf(x | p1), c2 * YYY_lpdf(x | p2));
  target += log_sum_exp(normal_lpdf(rr_Mon_minus_1 | 0, NORMAL_SIGMA),
                        exponential_lpdf(rr_Mon_minus_1 | EXPON_BETA));

  // Finally, the observations are drawn from a Poisson distribution.
  for (s in 1:STUDIES) {
    for (i in 1:DAYS) {
      ami_obs[s][i] ~ poisson(ami_dst_mean[s][i]);
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;parameters&lt;/code&gt; blocks declare the observed quantities and the unobserved parameters, without specifying their distribution.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;transformed parameters&lt;/code&gt; block contains all quantities that can be deterministically derived from the parameters.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;model&lt;/code&gt; block describes both the prior distributions for the parameters and the likelihood functions.&lt;/p&gt;

&lt;h4 id=&quot;sampling-using-the-python-interface&quot;&gt;Sampling using the Python interface&lt;/h4&gt;

&lt;p&gt;We can compile the Stan model and sample from it in Python using &lt;a href=&quot;https://pystan.readthedocs.io/&quot;&gt;PyStan&lt;/a&gt;. Once the software and its dependencies are installed, we can use the following code to draw 50,000 samples from the posterior and plot the results. On my computer, the model compilation takes about a minute, the sampling a few seconds.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pystan&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 6-long list of 5-long lists integers (weekday observations)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1735&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1644&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1555&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1522&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1467&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Janszky and Ljung 2008
&lt;/span&gt;           &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# Jiddou et al. 2013
&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 6-long list of 5-long lists floats
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_trend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;stan_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'STUDIES'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'DAYS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'NORMAL_SIGMA'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'EXPON_BETA'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'ami_obs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'ami_trend'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_trend&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pystan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StanModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dst_model.stan'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stan_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pystan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stansummary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rr_day[1]'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;sampling-using-the-r-interface&quot;&gt;Sampling using the R interface&lt;/h4&gt;

&lt;p&gt;The R interface of Stan is called &lt;a href=&quot;https://github.com/stan-dev/rstan/&quot;&gt;RStan&lt;/a&gt;, and can be used as follows:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rstan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# observe startup messages&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stan_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STUDIES&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DAYS&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NORMAL_SIGMA&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXPON_BETA&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ami_obs&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ami_trend&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_trend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'dst_model.stan'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stan_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rr_day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;effect-of-study-size-in-a-bayesian-framework&quot;&gt;Effect of study size in a Bayesian framework&lt;/h3&gt;

&lt;p&gt;For a small study, i.e. if the trend and observed AMI counts are low, we want to see a very slight change in the prior; for a large study, we want to see a bigger change.&lt;/p&gt;

&lt;p&gt;Two factors should play into this. First, if the trend predicts low counts, then we are likely to observe relatively big fluctuations: observing 12 heart attacks on a day when the long-term average is 10 represents a +20% increase, yet it occurs once every 3 days on average. Second, if the study was small and the trend is estimated from only a few weeks’ data, our &lt;em&gt;estimate&lt;/em&gt; of the trend itself has greater variance. This second factor is not yet modeled in our work, but in small studies like that of Čulić (2013), this too could play a role.&lt;/p&gt;

&lt;p&gt;To see the difference between a small and a large study, we visualize the prior and the posterior for the following scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Observation higher than trend, small sample size (top left);&lt;/li&gt;
  &lt;li&gt;Observation equals trend, large sample size (top right);&lt;/li&gt;
  &lt;li&gt;Observation lower than trend, large sample size (bottom left);&lt;/li&gt;
  &lt;li&gt;Observation higher than trend, large sample size (bottom right).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;figs/test-posteriors.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the sample size is small, there is only a slight change from prior to posterior. With a large sample size, the prior beliefs barely have an effect on the posterior. (In the lower right plot, the posterior peaks at more than 1.1 because with 1100 AMI every day, the linear weekday model fits better with a larger RR.)&lt;/p&gt;

&lt;h2 id=&quot;results-1&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;relevant-studies&quot;&gt;Relevant studies&lt;/h3&gt;

&lt;p&gt;The list of studies analyzed are identical to those analyzed in (Manfredini et al., 2019):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Janszky and Ljung (2008)&lt;/li&gt;
  &lt;li&gt;Janszky et al. (2012)&lt;/li&gt;
  &lt;li&gt;Čulić (2013)&lt;/li&gt;
  &lt;li&gt;Jiddou et al. (2013)&lt;/li&gt;
  &lt;li&gt;Sandhu et al. (2014)&lt;/li&gt;
  &lt;li&gt;Kirchberger et al. (2015)&lt;/li&gt;
  &lt;li&gt;Sipilä et al. (2016)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We excluded the study of Janszky et al. (2012), as the population is a strict subset of (Janszky and Ljung, 2008), with no additional information that is relevant for our analysis. The meta-analysis of Manfredini et al. (2019) did not exclude this study, which biased their results significantly, as the population size of this study is the second largest of all.&lt;/p&gt;

&lt;p&gt;Key characteristics of the above studies can be found in the table below, with more details in the appendix.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper&lt;/th&gt;
      &lt;th&gt;Sun&lt;/th&gt;
      &lt;th&gt;Mon&lt;/th&gt;
      &lt;th&gt;Tue&lt;/th&gt;
      &lt;th&gt;Wed&lt;/th&gt;
      &lt;th&gt;Thu&lt;/th&gt;
      &lt;th&gt;Fri&lt;/th&gt;
      &lt;th&gt;Sat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;(Janszky and Ljung, 2008)&lt;/td&gt;
      &lt;td&gt;(1374)&lt;/td&gt;
      &lt;td&gt;(1636)&lt;/td&gt;
      &lt;td&gt;(1494)&lt;/td&gt;
      &lt;td&gt;(1471)&lt;/td&gt;
      &lt;td&gt;(1484)&lt;/td&gt;
      &lt;td&gt;(1422)&lt;/td&gt;
      &lt;td&gt;(1370)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;1439&lt;/td&gt;
      &lt;td&gt;1735&lt;/td&gt;
      &lt;td&gt;1644&lt;/td&gt;
      &lt;td&gt;1555&lt;/td&gt;
      &lt;td&gt;1522&lt;/td&gt;
      &lt;td&gt;1467&lt;/td&gt;
      &lt;td&gt;1414&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Jiddou et al., 2013)&lt;/td&gt;
      &lt;td&gt;(13)&lt;/td&gt;
      &lt;td&gt;(29)&lt;/td&gt;
      &lt;td&gt;(20)&lt;/td&gt;
      &lt;td&gt;(23)&lt;/td&gt;
      &lt;td&gt;(17)&lt;/td&gt;
      &lt;td&gt;(25)&lt;/td&gt;
      &lt;td&gt;(16)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Čulić, 2013)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(7)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(7)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(5)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Kirchberger et al., 2015)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
      &lt;td&gt;(70)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Sandhu et al., 2014)&lt;/td&gt;
      &lt;td&gt;(111)&lt;/td&gt;
      &lt;td&gt;(138)&lt;/td&gt;
      &lt;td&gt;(127)&lt;/td&gt;
      &lt;td&gt;(125)&lt;/td&gt;
      &lt;td&gt;(120)&lt;/td&gt;
      &lt;td&gt;(120)&lt;/td&gt;
      &lt;td&gt;(110)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;108&lt;/td&gt;
      &lt;td&gt;170&lt;/td&gt;
      &lt;td&gt;125&lt;/td&gt;
      &lt;td&gt;122&lt;/td&gt;
      &lt;td&gt;117&lt;/td&gt;
      &lt;td&gt;117&lt;/td&gt;
      &lt;td&gt;114&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Sipilä et al., 2016)&lt;/td&gt;
      &lt;td&gt;(208)&lt;/td&gt;
      &lt;td&gt;(269)&lt;/td&gt;
      &lt;td&gt;(243)&lt;/td&gt;
      &lt;td&gt;(259)&lt;/td&gt;
      &lt;td&gt;(227)&lt;/td&gt;
      &lt;td&gt;(227)&lt;/td&gt;
      &lt;td&gt;(198)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;201&lt;/td&gt;
      &lt;td&gt;229&lt;/td&gt;
      &lt;td&gt;253&lt;/td&gt;
      &lt;td&gt;254&lt;/td&gt;
      &lt;td&gt;262&lt;/td&gt;
      &lt;td&gt;242&lt;/td&gt;
      &lt;td&gt;179&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;(Spring AMI counts. Trend predictions in parentheses, under them the number of incidences on the posttransitional week. Total count on the posttransitional week: 14,024.)&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper&lt;/th&gt;
      &lt;th&gt;Sun&lt;/th&gt;
      &lt;th&gt;Mon&lt;/th&gt;
      &lt;th&gt;Tue&lt;/th&gt;
      &lt;th&gt;Wed&lt;/th&gt;
      &lt;th&gt;Thu&lt;/th&gt;
      &lt;th&gt;Fri&lt;/th&gt;
      &lt;th&gt;Sat&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;(Janszky and Ljung, 2008)&lt;/td&gt;
      &lt;td&gt;(1780)&lt;/td&gt;
      &lt;td&gt;(2140)&lt;/td&gt;
      &lt;td&gt;(1991)&lt;/td&gt;
      &lt;td&gt;(1910)&lt;/td&gt;
      &lt;td&gt;(1941)&lt;/td&gt;
      &lt;td&gt;(1949)&lt;/td&gt;
      &lt;td&gt;(1781)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;1777&lt;/td&gt;
      &lt;td&gt;2038&lt;/td&gt;
      &lt;td&gt;1958&lt;/td&gt;
      &lt;td&gt;1895&lt;/td&gt;
      &lt;td&gt;1916&lt;/td&gt;
      &lt;td&gt;1977&lt;/td&gt;
      &lt;td&gt;1732&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Jiddou et al., 2013)&lt;/td&gt;
      &lt;td&gt;(18)&lt;/td&gt;
      &lt;td&gt;(24)&lt;/td&gt;
      &lt;td&gt;(21)&lt;/td&gt;
      &lt;td&gt;(27)&lt;/td&gt;
      &lt;td&gt;(22)&lt;/td&gt;
      &lt;td&gt;(24)&lt;/td&gt;
      &lt;td&gt;(20)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;34&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Kirchberger et al., 2015)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
      &lt;td&gt;(67)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Sandhu et al., 2014)&lt;/td&gt;
      &lt;td&gt;(86)&lt;/td&gt;
      &lt;td&gt;(107)&lt;/td&gt;
      &lt;td&gt;(99)&lt;/td&gt;
      &lt;td&gt;(97)&lt;/td&gt;
      &lt;td&gt;(93)&lt;/td&gt;
      &lt;td&gt;(93)&lt;/td&gt;
      &lt;td&gt;(85)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;89&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
      &lt;td&gt;93&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
      &lt;td&gt;86&lt;/td&gt;
      &lt;td&gt;99&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Sipilä et al., 2016)&lt;/td&gt;
      &lt;td&gt;(159)&lt;/td&gt;
      &lt;td&gt;(197)&lt;/td&gt;
      &lt;td&gt;(193)&lt;/td&gt;
      &lt;td&gt;(170)&lt;/td&gt;
      &lt;td&gt;(201)&lt;/td&gt;
      &lt;td&gt;(178)&lt;/td&gt;
      &lt;td&gt;(157)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;214&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;198&lt;/td&gt;
      &lt;td&gt;199&lt;/td&gt;
      &lt;td&gt;172&lt;/td&gt;
      &lt;td&gt;153&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;(Čulić, 2013)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(7)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(7)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(6)&lt;/td&gt;
      &lt;td&gt;(5)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;(Autumn AMI counts. Trend predictions in parentheses, under them the number of incidences on the posttransitional week. Total count on the posttransitional week: 15,921.)&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;ami-risk-after-spring-transition&quot;&gt;AMI risk after spring transition&lt;/h3&gt;

&lt;p&gt;The posteriors after the individual papers are shown below, along with their 95% central credible interval (CCrI).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/forest_plot.svg&quot; alt=&quot;Forest plot that shows the posterior after the individual papers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The width of the 95% CCrI is a measure of the precision of the estimate. The 95% CCrI after (Janszky and Ljung, 2008) and (Sipilä et al. 2016) are comparably narrow, but they are centered around 1.085 and 1.001, respectively. In fact, as we can see from the likelihood functions (not shown here), the study of Sipilä et al. (2016) presents a case for a slight &lt;em&gt;decrease&lt;/em&gt; in AMI risk under this model.&lt;/p&gt;

&lt;p&gt;In the fixed effects model the posterior is weighted heavily towards the study with the largest sample size (Janszky and Ljung 2008), and the other studies barely play a role.
Specifically, the posterior mean of the RR is 107.7% (95% central credible interval: &lt;script type=&quot;math/tex&quot;&gt;[104.7\%, 110.7\%]&lt;/script&gt;) – the posterior is shown below. We emphasize again that the relative weights of the studies is not arbitrary, but is fully determined by the model and the data through the rules of probability theory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/combined_posterior.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We arrive at the same posterior when drawing samples from it through a Monte Carlo method with Stan. Furthermore, as the tails of posterior are symmetric, the 95% highest density interval of [104.8%, 110.7%] closely aligns with the 95% central credible interval obtained earlier ([104.7%, 110.7%]). (This fact merely verifies that the two methods compute the model correctly, it does not provide additional evidence about the quality of the data.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/posterior_mc.svg&quot; alt=&quot;Posterior after spring data – with Stan&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The studies together provide so many data points that the choice of prior does not play an important role. Assuming a uniform prior on the risk ratio, i.e. assuming that we have no more &lt;em&gt;prior&lt;/em&gt; evidence for +2% than for +20% or −30% change in risk, we arrive at practically the same posterior, and a 95% HDI of [104.7%, 110.7%].&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/posterior_mc_uniform.svg&quot; alt=&quot;Posterior after spring data, uniform prior&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;exponential-weekday-model&quot;&gt;Exponential weekday model&lt;/h4&gt;

&lt;p&gt;The exponential weekday model relaxes the assumption of linear decrease in RR throughout the week, and instead models the daily RRs as exponentially decreasing. That is, for a parameter &lt;script type=&quot;math/tex&quot;&gt;\alpha \in [0,1]&lt;/script&gt;, the risk ratios are determined as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_\text{Mon} = 1 + \theta&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_\text{Tue} = 1 + \alpha \cdot \theta&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_\text{Wed} = 1 + \alpha^2 \cdot \theta&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Assuming a uniform prior on both &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, the posterior for this model looks as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/posterior_mc_exp.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I expected the posterior on &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; to be centered much closer to zero (meaning a rapid decrease in risk after Monday), but the posterior shows the opposite: most of the plausible values of &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; correspond to an &lt;script type=&quot;math/tex&quot;&gt;r_\text{Fri} / r_\text{Mon}&lt;/script&gt; ratio greater than the 0.2 ratio assumed previously (&lt;script type=&quot;math/tex&quot;&gt;0.7^4 \approx 0.24&lt;/script&gt;). The 95% HDI for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is [104.0%, 110.2%] (mean 107.1%), which is close to the linear weekday model, and the 95% HDI for &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is [0.66, 1.0] (mean 0.83). The figure below shows the risk ratios over the week for 20 of the sampled combinations of &lt;script type=&quot;math/tex&quot;&gt;(\alpha, \theta)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/risk_ratios_exp.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Over the five weekdays this posterior corresponds to an average risk ratio of 105.0% (95% HDI: [103.1%, 107.0%]). Assuming an affected population of 1.6 billion globally, with AMI rates standard across the USA &lt;a href=&quot;https://www.cdc.gov/heartdisease/heart_attack.htm&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;^\textsf{[source]}&lt;/script&gt;&lt;/a&gt;, this means that over the whole posttransitional week the an additional 2700 people experience AMI (95% HDI: [1600, 3700]), on top of the regular 53,000 per week.&lt;/p&gt;

&lt;h3 id=&quot;ami-risk-after-autumn-transition&quot;&gt;AMI risk after autumn transition&lt;/h3&gt;

&lt;p&gt;The posterior for the autumn data, using the linear weekday model with uniform prior on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is shown below. The 95% HDI of [95.1%, 100.3%] suggests a decrease in AMI risk, but the hypothesis of “no change in risk” (&lt;script type=&quot;math/tex&quot;&gt;\theta = 100.0\%&lt;/script&gt;) is also compatible with the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;figs/posterior_mc_autumn.svg&quot; alt=&quot;Posterior after autumn data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Globally, this translates to a change of AMI counts over the whole week of −700 (95% HDI [−1600, +100]), from the original 53,000.&lt;/p&gt;

&lt;h2 id=&quot;visualizing-the-observations-and-the-posterior-predictive-distribution&quot;&gt;Visualizing the observations and the posterior predictive distribution&lt;/h2&gt;

&lt;h3 id=&quot;posterior-predictive-distribution&quot;&gt;Posterior predictive distribution&lt;/h3&gt;

&lt;p&gt;In the figure below we visualize the posterior predictive distribution (for each day of each paper) on the spring posttransitional week, together with the actual observations.&lt;/p&gt;

&lt;p&gt;These predictive distributions on &lt;script type=&quot;math/tex&quot;&gt;\tilde y&lt;/script&gt; can be calculated by integrating the likelihoods &lt;script type=&quot;math/tex&quot;&gt;P(\tilde y \given \theta)&lt;/script&gt; over the parameter space, weighted by the posterior probability of the parameter values &lt;script type=&quot;math/tex&quot;&gt;p(\theta \given \mathcal D)&lt;/script&gt;, using the following formula:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\tilde y \given \mathcal D) =
\int P(\tilde y \given \theta, \mathcal D) \,d\theta =
\int P(\tilde y \given \theta) p(\theta \given \mathcal D) \,d\theta&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;figs/posterior_predictive_95.svg&quot; alt=&quot;Posterior predictive distribution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Posterior predictive distribution for spring.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Only the Monday observation of (Sipilä et al., 2016) falls out of the 95% central credible interval (CCrI), and in addition the Thursday observation of (Sipilä et al., 2016) and the Monday observation of (Čulić, 2013) falls out of the 90% CCrI (shown &lt;a href=&quot;figs/posterior_predictive_90.svg&quot;&gt;here&lt;/a&gt;), indicating a good fit of the model.&lt;/p&gt;

&lt;h2 id=&quot;further-research&quot;&gt;Further research&lt;/h2&gt;

&lt;p&gt;The importance of this issue depends on whether the increase in AMIs on the posttransitional week is merely a shift from the weeks afterwards. In other words, how many of these additional AMIs would have been asymptomatic, had it not been for the DST? We suspect that this number is quite low, because effectively the transition shifts the sleep schedule by an hour, which happens relatively often (e.g. when traveling), and single-day sleep deprivations are even more common. One way to approach this question is to collect the AMI counts in the few weeks following a DST transition, and compare the results obtained from regions with DST and regions without DST.&lt;/p&gt;

&lt;p&gt;The main deficiency of this meta-analysis is the assumption of equal effects regardless of country, while using the fixed effects model. This assumption could be relaxed in a random effects model, although that would introduce a subjective choice of inter-country variance, making the results harder to interpret correctly and simpler to misinterpret.&lt;sup&gt;&lt;a href=&quot;#fn-misinterpret&quot;&gt;[fn-1]&lt;/a&gt;&lt;a id=&quot;fn-src-misinterpret&quot;&gt;&lt;/a&gt; ↓&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;As the absolute effect of DST transitions on AMI incidences is not substantial (given the low base rate), even on a global scale, I suggest no further research on this specific topic.&lt;sup&gt;&lt;a href=&quot;#fn-further&quot;&gt;[fn-2]&lt;/a&gt;&lt;a id=&quot;fn-src-further&quot;&gt;&lt;/a&gt; ↓&lt;/sup&gt; There are many research areas around either sleep or cardiovascular health that are more important.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-1&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;A standard argument against Bayesian methods is that the subjective choice of prior influences the results arbitrarily. Although this is a philosophical question, we believe meaningful and consistent probabilistic inference cannot be done without describing our initial beliefs and defining how different parameter values would result in different observations. However, in our case the likelihood of the observed data dominated the prior, rendering the choice of prior almost irrelevant.&lt;/p&gt;

&lt;p&gt;Our analysis showed an increase in AMI risk during spring (relative risk increase 5–11% on Monday, less on later days), which translates to an additional 1600–3700 AMI incidences over the whole affected period. The data from the autumn transition showed either no change or a slight decrease in AMI risk (at most 5% relative risk decrease), translating to an estimated change in incidence counts somewhere between −1600 and +100.
These figures alone do not provide an argument against the institution of DST, especially without evidence that these changes are not merely the result of future AMI incidences advanced (in spring) or postponed (in autumn), which is the default position.
However, the analysis provides strong evidence for the hypothesis that our body can react negatively to a single hour shift in our sleep cycles, which should be a crucial factor in the evaluation of DST, and shows the importance of a consistent sleep schedule.&lt;/p&gt;

&lt;h2 id=&quot;license&quot;&gt;License&lt;/h2&gt;

&lt;p&gt;&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border-width:0&quot; src=&quot;https://i.creativecommons.org/l/by/4.0/88x31.png&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;i&gt;&lt;span xmlns:dct=&quot;http://purl.org/dc/terms/&quot; property=&quot;dct:title&quot;&gt;The effects of daylight savings time adjustment on the incidence rate of acute myocardial infarction: a Bayesian meta-analysis&lt;/span&gt;&lt;/i&gt; by &lt;a xmlns:cc=&quot;http://creativecommons.org/ns#&quot; href=&quot;https://treszkai.github.io/2019/11/11/dst-vs-ami&quot; property=&quot;cc:attributionName&quot; rel=&quot;cc:attributionURL&quot;&gt;Laszlo Treszkai&lt;/a&gt; is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt; (CC-BY-4.0).&lt;/p&gt;

&lt;p&gt;The data presented in the &lt;a href=&quot;#Relevant-studies&quot;&gt;Relevant studies&lt;/a&gt; section belong to the original authors and they do not fall under the above CC-BY-4.0 license.&lt;/p&gt;

&lt;p&gt;The software used for this analysis is distributed under the MIT license.&lt;/p&gt;

&lt;p&gt;Please cite this work as follows:&lt;/p&gt;

&lt;p&gt;Laszlo Treszkai. 2019. &lt;em&gt;The effects of daylight savings time adjustment on the incidence rate of acute myocardial infarction: a Bayesian meta-analysis&lt;/em&gt;. &lt;a href=&quot;http://treszkai.github.io/2019/11/11/dst-vs-ami&quot;&gt;http://treszkai.github.io/2019/11/11/dst-vs-ami&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;BibTeX:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{,
  title = {The effects of daylight savings time adjustment on the incidence rate of acute myocardial infarction: a {B}ayesian meta-analysis},
  author = {Laszlo Treszkai},
  howpublished = {\url{http://treszkai.github.io/2019/11/11/dst-vs-ami}},
%  note = {Accessed: yyyy-mm-dd}  % Optional. The document at this URL is not going to change.
  year = {2019}
  month = {oct}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;Cumming, G. (2014). &lt;em&gt;The new statistics why and how.&lt;/em&gt; Psychological Science, 25(1), 7–29.&lt;/p&gt;

&lt;p&gt;Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. &lt;em&gt;Bayesian Data Analysis.&lt;/em&gt; &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ronald L. Wasserstein, Nicole A. Lazar. 2016. &lt;em&gt;The ASA Statement on p-Values: Context, Process, and Purpose.&lt;/em&gt; The American Statistician. Volume 70, Issue 2, pp. 129-133. &lt;a href=&quot;https://doi.org/10.1080/00031305.2016.1154108&quot;&gt;link (OA)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ronald L. Wasserstein, Allen L. Schirm &amp;amp; Nicole A. Lazar. 2019. &lt;em&gt;Moving to a World Beyond “p &amp;lt; 0.05”.&lt;/em&gt; Volume 73, pp. 1–19. &lt;a href=&quot;https://doi.org/10.1080/00031305.2019.1583913&quot;&gt;link (OA)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;John K. Kruschke, Torrin M. Liddell, 2018. &lt;em&gt;The Bayesian New Statistics.&lt;/em&gt;  Psychonomic Bulletin &amp;amp; Review. Volume 25, Issue 1, pp 178–206. &lt;a href=&quot;https://link.springer.com/article/10.3758/s13423-016-1221-4&quot;&gt;link (OA)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Amneet Sandhu, Milan Seth, Hitinder S. Gurm. 2014. &lt;em&gt;Daylight savings time and myocardial infarction.&lt;/em&gt; Open Heart. &lt;a href=&quot;http://dx.doi.org/10.1136/openhrt-2013-000019&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Roberto Manfredini, Fabio Fabbian, Rosaria Cappadona, Alfredo De Giorgi, Francesca Bravi, Tiziano Carradori, Maria Elena Flacco, Lamberto Manzoli. 2019.
&lt;em&gt;Daylight Saving Time and Acute Myocardial Infarction: A Meta-Analysis&lt;/em&gt;. Journal of Clinical Medicine. 2019, &lt;em&gt;8&lt;/em&gt;, 404; &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6463000/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Kirchberger et al. 2015. &lt;em&gt;Are daylight saving time transitions associated with changes in myocardial infarction incidence? Results from the German MONICA/KORA Myocardial Infarction Registry&lt;/em&gt;. BMC Public Health. 2015; 15: 778. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4535383/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Janszky and Ljung. 2008. &lt;em&gt;Shifts to and from Daylight Saving Time and Incidence of Myocardial Infarction&lt;/em&gt;. The New England Journal of Medicine.  BMC Public Health. 359; 18. &lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc0807104&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Viktor Čulić. 2013. &lt;em&gt;Daylight saving time transitions and acute myocardial infarction&lt;/em&gt;. Chronobiology International. 2013; 30(5): 662–668. &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.3109/07420528.2013.775144&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Janszky, Ahnve, Ljung, Mukamal, Gautam, Wallentin, Stenestrand. 2012. &lt;em&gt;Daylight saving time shifts and incidence of acute myocardial infarction – Swedish Register of Information and Knowledge About Swedish Heart Intensive Care Admissions (RIKS-HIA)&lt;/em&gt;. Sleep Medicine 13 (2012) 237–242. &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S1389945711003832&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Monica R. Jiddou, MD, Mark Pica, BS, Judy Boura, MS, Lihua Qu, MS, and Barry A. Franklin, PhD. 2013. &lt;em&gt;Incidence of Myocardial Infarction With Shifts to and From Daylight Savings Time&lt;/em&gt;. The American Journal of Cardiology. Volume 111, Issue 5, Pages 631–635. &lt;a href=&quot;http://dx.doi.org/10.1016/j.amjcard.2012.11.010&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jussi O.T. Sipilä, Päivi Rautava &amp;amp; Ville Kytö. 2016. &lt;em&gt;Association of daylight saving time transitions with incidence and in-hospital mortality of myocardial infarction in Finland&lt;/em&gt;. Annals of Medicine, 48:1-2, 10-16. &lt;a href=&quot;http://dx.doi.org/10.3109/07853890.2015.1119302&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Young Joo Yang, Chang Seok Bang, Gwang Ho Baik, Tae Young Park, Suk Pyo Shin, Ki Tae Suk, Dong Joon Kim. 2017.
&lt;em&gt;Prokinetics for the treatment of functional dyspepsia: Bayesian network meta-analysis&lt;/em&gt;.
BMC Gastroenterology 17:83 DOI 10.1186/s12876-017-0639-0. &lt;a href=&quot;https://bmcgastroenterol.biomedcentral.com/track/pdf/10.1186/s12876-017-0639-0&quot;&gt;link (OA)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Xiaole Su, Xinfang Xie, Lijun Liu, Jicheng Lv, Fujian Song, Vlado Perkovic, Hong Zhang. 2017.
&lt;em&gt;Comparative Effectiveness of 12 Treatment Strategies for Preventing Contrast-Induced Acute Kidney Injury: A Systematic Review and Bayesian Network Meta-analysis&lt;/em&gt;
Volume 69, Issue 1, pp. 69–77.
DOI: 10.1053/j.ajkd.2016.07.033, &lt;a href=&quot;https://www.ajkd.org/article/S0272-6386(16)30421-8/fulltext&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Devin Incerti. 2015. &lt;em&gt;Bayesian Meta-Analysis with R and Stan&lt;/em&gt;. Self-published, online. https://devinincerti.com/2015/10/31/bayesian-meta-analysis.html. Retrieved 4 Oct 2019.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;appendix&quot;&gt;Appendix&lt;/h1&gt;

&lt;h2 id=&quot;characteristics-of-studies&quot;&gt;Characteristics of studies&lt;/h2&gt;

&lt;h3 id=&quot;janszky-and-ljung-2008&quot;&gt;Janszky and Ljung (2008)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;source: the Swedish registry of acute myocardial infarction (“which provides high-quality information on all acute myocardial infarctions in the country since 1987”)&lt;/li&gt;
  &lt;li&gt;years: 1987–2006&lt;/li&gt;
  &lt;li&gt;observations: the incidence of AMI during each of the first 7 days after the spring or autumn transition&lt;/li&gt;
  &lt;li&gt;trend: the mean of the incidences on the corresponding weekdays 2 weeks before and 2 weeks after the day of interest&lt;/li&gt;
  &lt;li&gt;total AMI cases on spring posttransitional week: 10,776&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Quotes&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The effects of transitions were consistently more pronounced for people under 65 years of age than for those 65 years of age or older.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The authors properly controlled for the Easter holiday.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Analyses of the data for the spring shift are based on the 15 years between 1987 and
2006 in which Easter Sunday was not the transition day.
[…]
For years in which Easter
Sunday was celebrated 2 weeks after the Sunday of the spring shift, we defined the control period for the Sunday of
the shift as the Sunday 3 weeks before and the Sunday 3 weeks after (thus skipping Easter Sunday).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Overanalysis&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The following observations do not have any plausible explanation, and are probably just noise. Question: did later studies confirm these findings?&lt;/p&gt;

&lt;p&gt;1.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;When we did not exclude Easter if it coincided with the exposure or control days, we observed an even higher effect size associated with the spring transition.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For the autumn shift, in contrast to the analyses of all acute myocardial infarctions, analyses restricted to fatal cases showed a smaller decrease in the incidence of acute myocardial infarction on Monday, and the risk of fatal acute myocardial infarction increased during the first week after the shift.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;3.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The effect of the spring transition to daylight saving time on the incidence of acute myocardial infarction was somewhat more pronounced in women than in men, and the autumn effect was more pronounced in men than in women.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Additional information&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The authors were employed by institutions in Stockholm, Sweden, meaning the use of the Swedish registry is &lt;em&gt;no evidence for selection bias&lt;/em&gt;. Furthermore, the end of the 30-year period of their study is only a year away from the date of the publication.&lt;/p&gt;

&lt;h3 id=&quot;janszky-et-al-2012&quot;&gt;Janszky et al. (2012)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;those AMI patients who were admitted to CCUs at participating hospitals&lt;/li&gt;
  &lt;li&gt;from 1995 to 2007&lt;/li&gt;
  &lt;li&gt;dataset: Register of Information and Knowledge about Swedish Heart Intensive Care Admissions (RIKS-HIA)&lt;/li&gt;
  &lt;li&gt;total AMI cases during spring posttransitional week: 3235.9&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This study didn’t publish per-day AMI counts, only the total during the whole posttransitional week.&lt;/p&gt;

&lt;p&gt;The time period matches exactly that of Janszky and Ljung (2008), and every case included in this study was also included in Janszky and Ljung (2008). As such, this study doesn’t add new information to the previous work with regards to the variables we consider, and it is &lt;strong&gt;excluded from our meta-analysis&lt;/strong&gt; in order to avoid double-counting.&lt;/p&gt;

&lt;p&gt;As the authors put it:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The study populations of the present and our previous study
overlapped substantially. Our previous analyses included all AMIs
detected either at a hospital or at an autopsy in Sweden from
1987 to 2006, a clear strength. In the present work, we investigated
only those AMI patients who were admitted to CCUs at participating
hospitals from 1995 to 2007. Although this limited our power
substantially, it allowed us to examine clinical factors that might
modify the risks related to DST transitions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;čulić-2013&quot;&gt;Čulić (2013)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;patients hospitalized because of AMI&lt;/li&gt;
  &lt;li&gt;from 1990 to 1996&lt;/li&gt;
  &lt;li&gt;40 patients on workdays following DST change&lt;/li&gt;
  &lt;li&gt;at University Hospital Centre Split in Split, Croatia&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is unclear whether the trend prediction is made from the 2 weeks before and after the posttransitional week, or from all 50 nontransitional weeks:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The incidence ratios of AMI for the first week after the
two DST shifts (posttransitional weeks) and each day of
that week were estimated by dividing the incidence
during those periods with the average incidences during
corresponding days and weeks throughout the year: 2
wks before and 2 wks after the posttransitional week,
and the 50 nontransitional weeks of the year altogether.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is unclear why exactly the data from 1990 to 1996 was analyzed, if the study was conducted in 2013. This is &lt;em&gt;suggestive of selection bias&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overanalysis&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;23 additional variables were analyzed (sex, employment status, use of β-blocker, etc.); some were bound to have low p-values:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The independent predictors for AMI during
this period in spring were male sex (p = 0.03) and nonengagement in physical activity (p = 0.02) and there was a trend
for the lower risk of incident among those taking calcium antagonists (p = 0.07). In autumn, the predictors were
female sex (p = 0.04), current employment (p = 0.006), not taking b-blocker (p = 0.03), and nonengagement in
physical activity (p = 0.02).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;jiddou-et-al-2013&quot;&gt;Jiddou et al. (2013)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a retrospective electronic chart review&lt;/li&gt;
  &lt;li&gt;all patients presenting to the emergency centers at Beaumont Hospitals in Royal Oak and Troy, Michigan, with the primary diagnosis of AMI&lt;/li&gt;
  &lt;li&gt;age: patients who were aged &amp;gt;18 years, resulting in 70±15 years&lt;/li&gt;
  &lt;li&gt;exclusion conditions: minor, pregnant&lt;/li&gt;
  &lt;li&gt;from October 2006 to April 2012 (7 years)&lt;/li&gt;
  &lt;li&gt;trend: patients admitted with comparable diagnoses on the corresponding weekdays 2 weeks before and 2 weeks after the shifts to and from DST&lt;/li&gt;
  &lt;li&gt;additional variables: demographic data, medical history, tobacco use, prescribed medications, whether the patient underwent cardiac catheterization; diagnosis of hypertension, hyperlipidemia, and coronary artery disease.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Quotes&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;2 AMIs occurred on Easter Sunday and were considered potential confounders and excluded.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is correct to note the incidences on Easter Sunday, but even more important would be the incidences on Easter &lt;em&gt;Monday&lt;/em&gt;. But even then, is only correct to exclude the patients entirely if the relevant control incidences are also reduced – it is unclear whether this trend correction happened.&lt;/p&gt;

&lt;h3 id=&quot;sandhu-et-al-2014&quot;&gt;Sandhu et al. (2014)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Time: 1 January 2010 – 15 September 2013 (3 fall and 4 spring DST changes; 1354 days)&lt;/li&gt;
  &lt;li&gt;Procedural data for hospital admissions where PCI was performed in the setting of AMI&lt;/li&gt;
  &lt;li&gt;Number of cases: 42,060 hospital admissions for AMI requiring PCI occurred during the study period.&lt;/li&gt;
  &lt;li&gt;The median daily AMI total was 31, ranging from a minimum of 14 to a maximum of 53 admissions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There was no difference in the total weekly number of PCIs performed for AMI for either the fall or spring time changes in the time period analysed. After adjustment for trend and seasonal effects, the Monday following spring time changes was associated with a 24% increase in daily AMI counts (p=0.011), and the Tuesday following fall changes was conversely associated with a 21% reduction (p=0.044). No other weekdays in the weeks following DST changes demonstrated significant associations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Analysis&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;I was unable to obtain the data at &lt;a href=&quot;https://bmc2.org&quot;&gt;Blue Cross Blue Shield of Michigan&lt;/a&gt; and the study did not include the number of AMI cases numerically, therefore I estimated it from the chart in Figure 3 (which was accurate to 0.4 AMI).&lt;/p&gt;

&lt;h3 id=&quot;kirchberger-et-al-2015&quot;&gt;Kirchberger et al. (2015)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;AMI count: 25,499 cases of AMI&lt;/li&gt;
  &lt;li&gt;data source: MONICA/KORA Myocardial Infarction Registry (&lt;a href=&quot;https://www.helmholtz-muenchen.de/herzschlag-info/&quot;&gt;link&lt;/a&gt;; public data should be published yearly according to &lt;a href=&quot;http://www.gbe-bund.de/gbe10/abrechnung.prc_abr_test_logon?p_uid=gast&amp;amp;p_aid=0&amp;amp;p_knoten=FID&amp;amp;p_sprache=E&amp;amp;p_suchstring=7014&quot;&gt;this website&lt;/a&gt;, but I did not find a link to download the dataset)&lt;/li&gt;
  &lt;li&gt;time period: 1 January 1985 and 31 October 2010 (26 spring and 25 fall DST changes – 2010 fall adjustment was on 31 October)&lt;/li&gt;
  &lt;li&gt;ages: 25–74&lt;/li&gt;
  &lt;li&gt;includes: coronary death and AMI&lt;/li&gt;
  &lt;li&gt;location: city of Augsburg (Germany) and the two adjacent counties (about 600,000 inhabitants)&lt;/li&gt;
  &lt;li&gt;additional variables: information on re-infarction, various medication prior to AMI, current occupation, history of hypertension, hyperlipidemia, diabetes, smoking, and obesity.&lt;/li&gt;
  &lt;li&gt;confounders accounted for: global time trend, temperature, relative humidity, barometric pressure, and indicators for month of the year, weekday and holiday&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Quotes&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The final model included the following covariates: time trend and previous two day mean relative humidity as regression splines with four and two degrees of freedom, respectively, previous two day mean temperature as a linear term and day of the week as categorical variable.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The optimized spring model [of the data from March and April, excluding the week in question] included time trend and same day mean relative humidity as regression splines with six and three degrees of freedom.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Six d.o.f. for 2 months is probably overfitting the data, even though it was the sum of 26 years. However, it shouldn’t make a predictible effect, and its overall effect is probably negligible.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The incidence rate ratio was assessed as observed over expected events per day and the mean per weekday and corresponding 95% confidence intervals were calculated.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, it is not stated how the confidence intervals were calculated: most importantly, which statistical test was used?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Analysis&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The paper stated only the calculated RRs for the spring and autumn prediction models (for all seven days), not the actual AMI counts.
Assuming the researchers analyzed the data in an honest manner (i.e. not picking model parameters for lower trend prediction and thus more significant observed increase), and that the model didn’t predict large deviations from the 2.7 AMI/day average, we can calculate a close approximation of the observations as &lt;script type=&quot;math/tex&quot;&gt;\mathrm{RR}_d \cdot \mathrm{trend}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;sipilä-et-al-2016&quot;&gt;Sipilä et al. (2016)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;years: 2001–2009, except 2002 and 2005 (due to Easter). 7 years.&lt;/li&gt;
  &lt;li&gt;Exclusion criterion: age &amp;lt; 18.&lt;/li&gt;
  &lt;li&gt;Age: mean age 71.2, SD 12.6 years&lt;/li&gt;
  &lt;li&gt;2 weeks prior and 3 weeks after DST transition&lt;/li&gt;
  &lt;li&gt;all 22 Finnish hospitals with coronary catheterization laboratory that treat emergency cardiac patients&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;database: Finnish Care Register for Health Care (CRHC), a nationwide, obligatory and automatically collected hospital discharge database.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Study group: posttransitional week&lt;/li&gt;
  &lt;li&gt;Control group: 2 weeks before/after posttransitional week&lt;/li&gt;
  &lt;li&gt;Easter in study group: 2002, 2005. “Years with DST spring transition on Easter Sunday were excluded from the analysis (2002 and 2005) to increase international comparability and avoid confounding”&lt;/li&gt;
  &lt;li&gt;Easter in control group: “When Easter Sunday was celebrated within 2 weeks after DST transition, post-DST control weeks after Easter were selected.”&lt;/li&gt;
  &lt;li&gt;Spring study+control group size: 1269+5029 = 6298&lt;/li&gt;
  &lt;li&gt;Standardized incidence of MI admissions in participating hospitals during spring study period was 259/100,000 person-years.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Quotes&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Incidence of MI admissions was similar to control
weeks for Sunday–Tuesday after DST transition
(Figure 1). However, on fourth day after transition
(Wednesday), there was a significant increase in MI
incidence compared to control weeks (IR 1.16; CI 1.01–
1.34).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is there anything special about the &lt;em&gt;Wednesday&lt;/em&gt; that follows a DST transition? One should not be surprised if a value falls outside of a 95% confidence/credible interval – after all, it happens &lt;em&gt;at least&lt;/em&gt; 5% of the time even in the absence of any “interesting” effect.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Patients admitted
during the week after DST transition were less likely to
have diagnosed diabetes or ventricular arrhythmias
compared to patients admitted during control weeks,
but had diagnosed renal failure more often.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is no simple and plausible explanation for this, therefore it is more probable that this is a result of finding patterns in noise.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Population-based incidence
of MI admissions to participating hospitals during
spring and autumn periods were calculated using
corresponding population data of mainland Finland
obtained from Statistics Finland and standardized to
European standard population 2013 by using the direct
method.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The meaning of the above statement is unclear.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;h4 id=&quot;footnote-1&quot;&gt;Footnote 1&lt;/h4&gt;

&lt;p&gt;“Sleep researchers show a 20% increase in risk of heart attacks in Michigan but a 10% decrease in Finland, so it is advised to travel to Europe for this week.”&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#fn-src-misinterpret&quot;&gt;[back to source]&lt;/a&gt; ↑&lt;/p&gt;

&lt;h4 id=&quot;footnote-2&quot;&gt;Footnote 2&lt;/h4&gt;

&lt;p&gt;Originally, I wrote the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Further research could analyze the publication bias (if you know how to do that in a Bayesian framework, please mention it in the comments below), or analyze more data, preferably from multiple countries. Maybe the DST transition has a smaller effect on the Finnish population than on the Swedish population, which could easily be analyzed using Bayesian statistics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But then I calculated the absolute global effect, which is quite small, therefore the updated recommendation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#fn-src-further&quot;&gt;[back to source]&lt;/a&gt; ↑&lt;/p&gt;</content><author><name></name></author><summary type="html">Laszlo Treszkai (firstname.lastname@gmail.com)</summary></entry><entry><title type="html">Trust in numbers (Sir David Spiegelhalter) – talk notes</title><link href="https://www.treszkai.com/blog/trust-in-numbers" rel="alternate" type="text/html" title="Trust in numbers (Sir David Spiegelhalter) – talk notes" /><published>2019-10-08T00:00:00+02:00</published><updated>2019-10-08T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/trust-in-numbers</id><content type="html" xml:base="https://www.treszkai.com/blog/trust-in-numbers">&lt;p&gt;The Institute of Medical Statistics of the Center for Medical Statistics, Informatics and Intelligent Systems at the Medical University of Vienna &lt;a href=&quot;https://cemsiis.meduniwien.ac.at/50years-of-ms/&quot;&gt;just turned 50 years old&lt;/a&gt;, and they organized a two-day event around it. I was fortunate to have attended the keynote talk of Sir David Spiegelhalter (&lt;a href=&quot;https://en.wikipedia.org/wiki/David_Spiegelhalter&quot;&gt;wiki&lt;/a&gt;), who is a British statistician and &lt;a href=&quot;https://en.wikipedia.org/wiki/Winton_Professorship_of_the_Public_Understanding_of_Risk&quot;&gt;Winton Professor of the Public Understanding of Risk&lt;/a&gt; at the Faculty of Mathematics, University of Cambridge, which was one of the most entertaining &lt;em&gt;and&lt;/em&gt; informative talk I have heard. There is no way I can do justice to the talk, and I wouldn’t even attempt to bring through the humor (his &lt;em&gt;humour&lt;/em&gt;) – the goal of this post is to increase your vigilance a little bit when it comes to any reports about science, and to shed light on the work of Spiegelhalter.&lt;/p&gt;

&lt;p&gt;The professor has authored several academic books on statistics, and was interviewed by the CNN with the title, &lt;a href=&quot;https://edition.cnn.com/videos/tv/2019/04/01/amanpour-david-spiegelhalter-statistics.cnn&quot;&gt;&lt;em&gt;Why statistics should make you suspicious&lt;/em&gt;&lt;/a&gt;. And keeps doing a huge service to science in a number of other ways.&lt;/p&gt;

&lt;iframe width=&quot;416&quot; height=&quot;234&quot; src=&quot;//fave.api.cnn.io/v1/fav/?video=tv/2019/04/01/amanpour-david-spiegelhalter-statistics.cnn&amp;amp;customer=cnn&amp;amp;edition=international&amp;amp;env=prod&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The problem explained in the talk was that &lt;strong&gt;numbers are used to persuade people, not to inform them&lt;/strong&gt;. (Actually, that was only the first half – the second half offered a handful of steps we could take when presenting our data.) Take for example politics, and the campaign around Brexit. Even if it were true that it costs £350 million a week for the UK to be a member of the EU, it would be much less misleading if it said that it costs 80 pence &lt;em&gt;per person per day&lt;/em&gt; to be a member of the EU. The cost of a bag of potato chips. (The other side committed similar errors too – I’m not trying to win a battle here.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;nhs.png&quot; alt=&quot;We send the EU £350 million a week; let’s fund our NHS instead. Vote Leave.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As Eliezer Yudkowsky says, &lt;a href=&quot;https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer&quot;&gt;politics is the mind-killer&lt;/a&gt;, but of course, using numbers to mislead instead of to show an honest representation of reality is done everywhere where there are numbers. My favorite topic these days: &lt;strong&gt;medical statistics&lt;/strong&gt;. I’m picking a topic from the talk as an example (which Spiegelhalter analyzed in more detail in a &lt;a href=&quot;https://medium.com/wintoncentre/are-we-individuals-or-members-of-populations-the-deeper-issues-behind-the-sausage-wars-a067aebf2063&quot;&gt;Medium post&lt;/a&gt;): dietary advice about processed meat consumption. CNN did a &lt;a href=&quot;https://edition.cnn.com/2019/04/17/health/colorectal-cancer-risk-red-processed-meat-study-intl/index.html&quot;&gt;great job&lt;/a&gt; with picking the title of their article to be as close to the original conclusions as possible: &lt;em&gt;Eating just one slice of bacon a day linked to higher risk of colorectal cancer, says study&lt;/em&gt;. But by the time this study reaches The Sun, it gets reported as the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;bacon.jpg&quot; alt=&quot;Rasher of bacon a day is deadly&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;Boy, that escalated quickly. And what does “higher risk of colorectal cancer” mean anyhow? In this case, the study showed a 19% increase. As Peter Attia explains in his detailed post series on science, &lt;a href=&quot;https://peterattiamd.com/ns001/&quot;&gt;Studying Studies&lt;/a&gt;, such big numbers generally mean an increase in &lt;em&gt;relative risk&lt;/em&gt;, not in &lt;em&gt;absolute risk&lt;/em&gt;. Relative risk is meaningless without knowing the base rate of the disease. In this case, 5% of US men and women born today are expected to be diagnosed with colorectal cancer sometime during their lives. Add 19% to that 5% figure (i.e., multiply it by 1.19), and you get 6%, for the people who eat 1 slice of bacon a day. (The 5% figure is surprisingly high, by the way! Fortunately, it has a five-year survival rate of 65%. I don’t know how much of the 5% is a false positive; I guess it doesn’t include the disconfirmed cases. These figures I just gathered from &lt;a href=&quot;https://en.wikipedia.org/wiki/Colorectal_cancer#Epidemiology&quot;&gt;Wikipedia&lt;/a&gt;, FWIW.)&lt;/p&gt;

&lt;p&gt;You can take the extra step and visualize these numbers using what &lt;a href=&quot;https://en.wikipedia.org/wiki/Gerd_Gigerenzer&quot;&gt;Gigerenzer&lt;/a&gt; calls natural frequencies. As one Wikipedia author puts it, “the problem is not simply in the human mind, but in the representation of the information”, so let’s deliver using things we evolved to understand: a small tribe of human-like icons.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All&lt;/em&gt; of these people below eat a clean diet without processed meat, and those with a distraught face will get colorectal cancer:&lt;/p&gt;

&lt;p&gt;😎😎😎😎😎😎😫😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😫😎😎😎😎😎😎&lt;br /&gt;
😎😎😫😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😫&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😫😎😎&lt;/p&gt;

&lt;p&gt;And &lt;em&gt;all&lt;/em&gt; of these people eat a slice of &lt;a href=&quot;https://en.wikipedia.org/wiki/Extrawurst&quot;&gt;Extrawurst&lt;/a&gt; daily:&lt;/p&gt;

&lt;p&gt;😎😎😎😎😎😎😫😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😫😎😎😎😎😎😎&lt;br /&gt;
😎😎😫😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😫&lt;br /&gt;
😎😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😫😎😎😎😎😎😎😎😎😎&lt;br /&gt;
😎😎😎😎😎😎😎😫😎😎&lt;/p&gt;

&lt;p&gt;See the difference? It’s that one troubled guy in row 9.&lt;/p&gt;

&lt;p&gt;Now, I’m not saying bacon is good for health, or that that additional risk factor would be negligible (admittedly, my mocking tone above suggests otherwise). But if the scientists, journalists, and clinicians report the risk honestly, &lt;em&gt;and&lt;/em&gt; no-one is trying to influence you into eating more burgers by playing at our primal instincts (including the marketing division of McDonald’s and our social group who calls you chicken if you don’t eat your &lt;a href=&quot;https://en.wikipedia.org/wiki/Black_pudding&quot;&gt;black pudding&lt;/a&gt;), then us puny humans could make more educated decisions about which sacrifices we are willing to make.&lt;/p&gt;

&lt;p&gt;This post was just a tiny part of what was said at the talk. In parting, I have two takeaway quotes. First,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;80% of statistics are false.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(From anonymous statistician, a comedian, and also Elon Musk.) Unfortunately, this factoid alone doesn’t enable one to navigate reality.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/aHGd6LqAVzw?start=43&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The second quote is of a little more value, but still doesn’t help one to sieve through statistics:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There’s no point in being trustworthy if you’re boring.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(From Spiegelhalter in today’s talk.)&lt;/p&gt;

&lt;p&gt;This talk was anything but boring. If you have a chance to see Spiegelhalter in person, do so: he gets my highest grade recommendation. (He also has a book, titled &lt;a href=&quot;https://smile.amazon.com/Art-Statistics-How-Learn-Data/dp/1541618513&quot;&gt;&lt;em&gt;The Art of Statistics&lt;/em&gt;&lt;/a&gt;, which I haven’t read.)&lt;/p&gt;

&lt;p&gt;(Somewhat related: just today on my way home I learned of Edward Tufte’s book, &lt;em&gt;The Visual Display of Quantitative Information,&lt;/em&gt; which also &lt;a href=&quot;https://www.edwardtufte.com/tufte/books_vdqi&quot;&gt;looks amazing&lt;/a&gt;.)&lt;/p&gt;</content><author><name></name></author><summary type="html">The Institute of Medical Statistics of the Center for Medical Statistics, Informatics and Intelligent Systems at the Medical University of Vienna just turned 50 years old, and they organized a two-day event around it. I was fortunate to have attended the keynote talk of Sir David Spiegelhalter (wiki), who is a British statistician and Winton Professor of the Public Understanding of Risk at the Faculty of Mathematics, University of Cambridge, which was one of the most entertaining and informative talk I have heard. There is no way I can do justice to the talk, and I wouldn’t even attempt to bring through the humor (his humour) – the goal of this post is to increase your vigilance a little bit when it comes to any reports about science, and to shed light on the work of Spiegelhalter.</summary></entry><entry><title type="html">On the overconfidence of modern neural networks</title><link href="https://www.treszkai.com/blog/overconfidence" rel="alternate" type="text/html" title="On the overconfidence of modern neural networks" /><published>2019-09-26T00:00:00+02:00</published><updated>2019-09-26T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/overconfidence</id><content type="html" xml:base="https://www.treszkai.com/blog/overconfidence">&lt;p&gt;&lt;em&gt;On the overconfidence of modern neural networks&lt;/em&gt;. This is the title of the coursework I did with a fellow student at the University of Edinburgh. (PDF: &lt;a href=&quot;mlp-cw3.pdf&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;mlp-cw4.pdf&quot;&gt;Part 2&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Our topic was influenced by a previous study, titled &lt;em&gt;On Calibration of Modern Neural Networks&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#Guo2017-calibration&quot;&gt;(Guo, Pleiss, Sun, &amp;amp; Weinberger, 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Applications of uncertainty estimation include threshold-based outlier detection, active learning, uncertainty-driven exploration of reinforcement learning, or certain safety-critical applications.&lt;/p&gt;

&lt;h2 id=&quot;what-is-uncertainty&quot;&gt;What is uncertainty?&lt;/h2&gt;

&lt;p&gt;No computer vision system is perfect, so an image classification algorithm sometimes identifies people as not-people, or not-people as people.
While we usually care about the class with the highest output (the “most likely” class), we can treat the softmax outputs of a classifier as uncertainty estimates.
(After all, that is how we trained a model when treating the softmax outputs of a classifier as a probability distribution, and minimizing the negative log likelihood of the model given the data.)
For example, out of 1000 classifications made with an output of 0.8, approximately 800 should be correct &lt;em&gt;if the system is well-calibrated&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;yolo.png&quot; alt=&quot;Example output of a YOLO object detection network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Example output of a YOLO object detection network, with the probability estimates. Image source: &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/&quot;&gt;Analytics Vidhya&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Ideally, we want our system to be 100% correct, but we rarely have access to an all-knowing Oracle. In cases where it is hard to distinguish between two categories (like on the cat-dog below) we want the uncertainties to be well-calibrated, so that predictions are neither overly confident nor insufficiently confident.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;catdog.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Image source: Google Brain)&lt;/p&gt;

&lt;h2 id=&quot;our-results&quot;&gt;Our results&lt;/h2&gt;

&lt;h3 id=&quot;interim-report&quot;&gt;Interim report&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;mlp-cw3.pdf&quot;&gt;Link to report (PDF)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our initial experiments showed that our baseline model is already well-calibrated when trained on the EMNIST By-Class dataset.
Calibration worsened when we used only a subset of the training set.
We found that increasing regularization increases calibration, but too much regularization leads to a decrease in both accuracy and calibration. (See figure below.)
This contradicts the findings of &lt;a class=&quot;citation&quot; href=&quot;#Guo2017-calibration&quot;&gt;(Guo, Pleiss, Sun, &amp;amp; Weinberger, 2017, sec. 3)&lt;/a&gt;, who found that model calibration can improve by increasing the weight decay constant, well after the model achieves minimum classification accuracy.
One of our main findings is that cross-entropy error is not a good indicator of model calibration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;mlp-cw3-fig5.png&quot; alt=&quot;Figure 5 of our interim report.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(ECE: expected calibration error. The lower the better.)&lt;/p&gt;

&lt;h3 id=&quot;final-report&quot;&gt;Final report&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;mlp-cw4.pdf&quot;&gt;Link to report (PDF)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We replicate the findings of &lt;a class=&quot;citation&quot; href=&quot;#Guo2017-calibration&quot;&gt;(Guo, Pleiss, Sun, &amp;amp; Weinberger, 2017)&lt;/a&gt; that deep neural networks achieve higher accuracy but worse calibration than shallow nets, and compare different approaches for improving the calibration of neural networks (see figure below). As the baseline approach, we consider the calibration of the softmax outputs from a single network; this is compared to &lt;em&gt;deep ensembles&lt;/em&gt;, &lt;em&gt;MC dropout&lt;/em&gt;, and &lt;em&gt;concrete dropout&lt;/em&gt;. Through experiments on the CIFAR-100 data set, we find that a large neural network can be significantly over-confident about its predictions. We show on a classification problem that an ensemble of deep networks has better classification accuracy and calibration compared to a single network, and that MC dropout and concrete dropout significantly improve the calibration of a large network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;mlp-cw4-fig2.png&quot; alt=&quot;Confidence and calibration plots for BigNet. (Figure 2 of our report)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;Top row:&lt;/em&gt; confidence plots for a deep neural net. The more skewed to the right, the better. &lt;em&gt;Bottom row:&lt;/em&gt; corresponding calibration plots. The more close to the diagonal, the better.)&lt;/p&gt;

&lt;h2 id=&quot;things-i-would-do-differently&quot;&gt;Things I would do differently&lt;/h2&gt;

&lt;p&gt;With a little more experience behind my back now, I would make the following changes in experiment design and writing the report:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Use a validation set.&lt;/em&gt; We only used a training set because we trained for minimum error, and we expected &lt;em&gt;calibration&lt;/em&gt; to be independent from &lt;em&gt;accuracy&lt;/em&gt;, but that is a strong assumption (and likely incorrect, seeing our results in the interim report).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Use better biblography sources.&lt;/em&gt; Instead of Google Scholar, I would search &lt;a href=&quot;https://dblp.uni-trier.de/&quot;&gt;DBLP&lt;/a&gt;, where the information is more correct and consistent.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Use pastel colors.&lt;/em&gt; I let my collaborator have it his way, but ever since this submission I’m having nightmares in purple and glowing green :D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In future work, I would like to test the calibration of a Bayesian neural network, where the weights of the network have a probability distribution instead of a point estimate.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Guo2017-calibration&quot;&gt;Guo, C., Pleiss, G., Sun, Y., &amp;amp; Weinberger, K. Q. (2017). On Calibration of Modern Neural Networks. In D. Precup &amp;amp; Y. W. Teh (Eds.), &lt;i&gt;Proceedings of the 34th International Conference on Machine Learning&lt;/i&gt; (Vol. 70, pp. 1321–1330). International Convention Centre, Sydney, Australia: PMLR. Retrieved from http://proceedings.mlr.press/v70/guo17a.html&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">On the overconfidence of modern neural networks. This is the title of the coursework I did with a fellow student at the University of Edinburgh. (PDF: Part 1, Part 2.)</summary></entry><entry><title type="html">Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004)</title><link href="https://www.treszkai.com/blog/irl-summary" rel="alternate" type="text/html" title="Paper summary: Abbeel, Ng: Inverse Reinforcement Learning (2004)" /><published>2019-08-19T00:00:00+02:00</published><updated>2019-08-19T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/irl-summary</id><content type="html" xml:base="https://www.treszkai.com/blog/irl-summary">&lt;p&gt;This post is a summary of the seminal paper on inverse reinforcement learning: Pieter Abbeel, Andrew Y. Ng: &lt;em&gt;Apprenticeship Learning via Inverse Reinforcement Learning&lt;/em&gt; (2004) [&lt;a href=&quot;http://ai.stanford.edu/~pabbeel/irl/&quot;&gt;link&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;Traditional &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot;&gt;reinforcement learning&lt;/a&gt; (RL) starts with specifying a reward function, and during training we search for policies that maximize this reward function&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. In contrast, inverse reinforcement learning (IRL) starts with expert demonstrations of the desired behavior, infers a reward function that the expert likely followed, and trains a policy to maximize that.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;IRL is useful for learning complex tasks where it is hard to manually specify a reward function that makes desirable trade-offs between desiderata; such tasks include driving a car or teaching a robot to do a backflip, where we want the car to reach to the destination promptly but also safely, or the robot to flip with its arms straight and &lt;a href=&quot;https://youtu.be/xet3KDUfS_U?t=50&quot;&gt;sticking the landing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In contrast with previous attempts at apprenticeship learning (i.e. learning from an expert), which tried to mimic the expert demonstrations directly, IRL assumes that the expert follows a reward function that is a linear combination of the feature vectors (&lt;script type=&quot;math/tex&quot;&gt;R = w^T φ(s)&lt;/script&gt;), and finds a reward function that maximizes the received reward under the set of demonstrations. The hand-specified function &lt;script type=&quot;math/tex&quot;&gt;φ: S→ℝ^k&lt;/script&gt; maps a state of the Markov decision process (MDP) to a feature vector, which vector includes parameters for the different desiderata of the task, such as the distances to objects surrounding the car, the speed of the car, or the current lane.&lt;/p&gt;

&lt;p&gt;IRL assumes knowledge of an expert policy &lt;script type=&quot;math/tex&quot;&gt;π_E&lt;/script&gt;, or at least samples from it. Using these, we only care about the estimated “accumulated feature values”, &lt;script type=&quot;math/tex&quot;&gt;μ(π_E) ∈ ℝ^k&lt;/script&gt;, which is the expected discounted sum of the feature vectors if sampled from the policy, because then the value of a policy (parametrised by &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;) can be calculated from it directly: &lt;script type=&quot;math/tex&quot;&gt;R = w^T μ(π_E)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The goal is then to find a policy whose performance is close to that of the expert’s on the unknown reward function &lt;script type=&quot;math/tex&quot;&gt;R_{\star} = w^T_{\star} φ&lt;/script&gt;. This is done by finding a policy whose feature vector is close to the expert’s feature vector, which assures that the value of these policies is close too.&lt;/p&gt;

&lt;p&gt;The algorithm for IRL is the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pick a random initial policy, and calculate its &lt;script type=&quot;math/tex&quot;&gt;μ&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Find the vector of weights w that lies within the unit ball and &lt;em&gt;maximizes&lt;/em&gt; the difference between the expert feature expectations and the feature expectations of our best policy thus far.&lt;/li&gt;
  &lt;li&gt;If this maximum is small, then go to step 7.&lt;/li&gt;
  &lt;li&gt;Otherwise &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is our new weights for &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Calculate optimal policy for this &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Repeat from step 2.&lt;/li&gt;
  &lt;li&gt;Let the agent designer pick a policy from any of those found in step 5 in the different iterations; or find the policy in the convex closure of these policies that is closest to the expert policy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The maximization in step 2 allows us to find a policy that is close to the expert’s, regardless of the choice of a reward function. After all, we are interested in the policy, not the reward function, and so the estimated &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; is not necessarily correct.&lt;/p&gt;

&lt;p&gt;This algorithm is proved to terminate within &lt;script type=&quot;math/tex&quot;&gt;O(k \log(k))&lt;/script&gt; steps, using at least &lt;script type=&quot;math/tex&quot;&gt;O(k \log(k))&lt;/script&gt; number of samples from the expert policy.&lt;/p&gt;

&lt;p&gt;Experiments are done in a gridworld environment, where IRL learns the expert policy in approximately 100 times less sample trajectories than simply mimicking the expert. Another experiment is a car driving simulator with 3 lanes viewed from the top, where IRL is capable of learning multiple driving styles, such as “prefer the right lane but avoid collisions”. Video demonstrations of the latter show that the sentiment of the expert policy is indeed followed, although sometimes with unnecessary lane switches (most modern RL algorithms also exhibit this undesired property).&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Or, more accurately, a policy that maximizes the expected utility derived from this reward function and some method of temporal discounting. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">This post is a summary of the seminal paper on inverse reinforcement learning: Pieter Abbeel, Andrew Y. Ng: Apprenticeship Learning via Inverse Reinforcement Learning (2004) [link].</summary></entry><entry><title type="html">Sampling from the posterior with Markov-chain Monte Carlo</title><link href="https://www.treszkai.com/blog/mcmc" rel="alternate" type="text/html" title="Sampling from the posterior with Markov-chain Monte Carlo" /><published>2019-08-06T00:00:00+02:00</published><updated>2019-08-06T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/mcmc</id><content type="html" xml:base="https://www.treszkai.com/blog/mcmc">&lt;p&gt;John K. Kruschke’s book, titled &lt;em&gt;Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan (2nd ed.)&lt;/em&gt; (&lt;a href=&quot;https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884&quot;&gt;Amazon&lt;/a&gt;, &lt;a href=&quot;https://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&quot;&gt;official site&lt;/a&gt;), gives a very quick and practical introduction to Bayesian analysis. Compared to BDA3, it contains less proofs, but also less jargon; more explanations that are informal, and more introductions to the basics; therefore, I would recommend it to someone who hasn’t had much of an exposure to statistics yet, or is not a mathematician nor a programmer.&lt;/p&gt;

&lt;p&gt;The book includes thorough and nicely visualized descriptions of multiple Markov-chain Monte Carlo methods for sampling from a posterior distribution, of which I’ll try to summarize the most basic one in this post.&lt;/p&gt;

&lt;h2 id=&quot;goal-of-sampling&quot;&gt;Goal of sampling&lt;/h2&gt;

&lt;p&gt;Given the prior &lt;script type=&quot;math/tex&quot;&gt;p(θ)&lt;/script&gt; and the likelihood &lt;script type=&quot;math/tex&quot;&gt;p(\D\given θ)&lt;/script&gt;, we want samples from the posterior &lt;script type=&quot;math/tex&quot;&gt;p(θ\given \D)&lt;/script&gt;. In the following sections I’ll use the fact that the unnormalized posterior is equal to the prior multiplied with the likelihood: &lt;script type=&quot;math/tex&quot;&gt;p(θ, \D) = p(θ)\,p(\D \given θ)&lt;/script&gt;. Here, I’ll talk only about continuous probability spaces; discrete spaces can be sampled similarly.&lt;/p&gt;

&lt;h2 id=&quot;metropolis-algorithm&quot;&gt;Metropolis algorithm&lt;/h2&gt;

&lt;p&gt;Just like the other MC methods, the Metropolis algorithm starts with a seed value for &lt;script type=&quot;math/tex&quot;&gt;θ&lt;/script&gt; – let’s call it &lt;script type=&quot;math/tex&quot;&gt;θ_0&lt;/script&gt;. (I assume in practice &lt;script type=&quot;math/tex&quot;&gt;θ_0&lt;/script&gt; is sampled from the prior.) Then, once you have a seed value &lt;script type=&quot;math/tex&quot;&gt;θ_i&lt;/script&gt;, repeat the following two steps for a prespecified number of iterations, or until an effective sample size is achieved.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample &lt;script type=&quot;math/tex&quot;&gt;θ'_{i+1}&lt;/script&gt; from a proposal distribution around &lt;script type=&quot;math/tex&quot;&gt;\theta_i&lt;/script&gt;, which could be a Gaussian: &lt;script type=&quot;math/tex&quot;&gt;\theta'_{i+1} \sim \N (θ_i, Σ)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;p(θ_{i},\D) \le p(θ'_{i+1},\D)&lt;/script&gt; – i.e. if &lt;script type=&quot;math/tex&quot;&gt;p(θ_{i} \given \D) \le p(θ'_{i+1} \given \D)&lt;/script&gt; – then &lt;em&gt;accept&lt;/em&gt; the proposed parameter value: &lt;script type=&quot;math/tex&quot;&gt;θ_{i+1} := θ'_{i+1}&lt;/script&gt;.&lt;/li&gt;
      &lt;li&gt;Otherwise, the probability of accepting the proposed parameter is the ratio of the posterior at the proposed value and at the current value; otherwise, reject it:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{gathered}
p = \frac{p(θ'_{i+1}, \D)}{p(θ_{i}, \D)} = \frac{p(θ'_{i+1} \given \D)}{p(θ_{i} \given \D)}, \\
b \sim Bernoulli(p), \\
θ_{i+1} =
\begin{cases}
    θ_{i+1}' &amp; \text{if } b=1,\\
    θ_i &amp; \text{if } b=0.
\end{cases}
\end{gathered} %]]&gt;&lt;/script&gt;

&lt;p&gt;It can be proven that after a so-called “burn-in” period, the probability of any &lt;script type=&quot;math/tex&quot;&gt;θ_{n}&lt;/script&gt; value will be the posterior probability: &lt;script type=&quot;math/tex&quot;&gt;θ_n \sim p(\theta_n\given \D)&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;n \gg 1&lt;/script&gt;, therefore if you do the procedure long enough, you’ll end up with many samples from the posterior. Note that the &lt;em&gt;effective sample size&lt;/em&gt; will be much lower than &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, because neighboring samples are strongly correlated, so we have to drop most of the &lt;script type=&quot;math/tex&quot;&gt;θ_i&lt;/script&gt; values so obtained.&lt;/p&gt;

&lt;p&gt;The beauty of this algorithm is that during this whole procedure, we only need to be able to compute the &lt;em&gt;unnormalized posterior&lt;/em&gt; – so the algorithm can be easily used for sampling using the prior and the likelihood, even when the model is specified up to a multiplicative constant (as in an undirected graphical model).&lt;/p&gt;

&lt;p&gt;This algorithm doesn’t easily escape a “probability island” – i.e. a region that is surrounded with a wide region of probability 0. (Although if the proposal distribution is wide enough, then the algorithm is theoretically able to make that jump &lt;em&gt;eventually&lt;/em&gt;, which maybe in practice “approximately never”.)&lt;/p&gt;

&lt;p&gt;One downside of this basic algorithm is that the proposal distribution needs to be fine-tuned for the individual application: differences in effective sample size can be orders of magnitudes, even for a simple &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(14,20)&lt;/script&gt; distribution (i.e. a 1-dimensional unimodal distribution with finite support).&lt;/p&gt;

&lt;p&gt;Another downside is that in multiple dimensions this random walk is quite inefficient, and &lt;em&gt;even more&lt;/em&gt; dependent on a correct choice of the covariance matrix &lt;script type=&quot;math/tex&quot;&gt;Σ&lt;/script&gt; – but apart from the obvious reason that “high-dimensional spaces are big”, I couldn’t tell why.&lt;/p&gt;

&lt;p&gt;The well-known Metropolis–Hastings algorithm, Gibbs sampling and Hamiltonian Monte Carlo are different twists on this core idea, and they are also described in the book.&lt;/p&gt;

&lt;p&gt;Allegedly, credit for this method is due more to Marshall and Arianna Rosenbluth – if there is agreement on that, we could rename it to Rosenbluthsian Monte Carlo.&lt;/p&gt;

&lt;h2 id=&quot;for-more-information&quot;&gt;For more information…&lt;/h2&gt;

&lt;p&gt;If you want to learn about sampling, or Bayesian data analysis, consider reading &lt;a href=&quot;https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884&quot;&gt;the book&lt;/a&gt;, it’s a great read from what I’ve read so far.&lt;/p&gt;

&lt;p&gt;Stay tuned for more of Bayes, or Curry, or Euler, or McCarthy.&lt;/p&gt;</content><author><name></name></author><summary type="html">John K. Kruschke’s book, titled Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan (2nd ed.) includes thorough and nicely visualized descriptions on three Markov-chain Monte Carlo methods for sampling from a posterior distribution, which I'll try to summarize in this post.</summary></entry><entry><title type="html">Bayesian inference: Approaching certainty through sampling</title><link href="https://www.treszkai.com/blog/approaching-certainty" rel="alternate" type="text/html" title="Bayesian inference: Approaching certainty through sampling" /><published>2019-07-24T00:00:00+02:00</published><updated>2019-07-24T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/approaching-certainty</id><content type="html" xml:base="https://www.treszkai.com/blog/approaching-certainty">&lt;p&gt;&lt;em&gt;Bayesian Data Analysis&lt;/em&gt; from Gelman et al. (2013), in section 3.7, presents the statistical analysis of a bioassay experiment. The parameters of the model are &lt;script type=&quot;math/tex&quot;&gt;(\alpha, \beta)&lt;/script&gt;, and we draw samples from the numerically calculated posterior. Then the authors write:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;All of the 1000 simulation draws had positive values of &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, so the posterior probability that &lt;script type=&quot;math/tex&quot;&gt;\beta &gt; 0&lt;/script&gt; is roughly estimated to exceed 0.999.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I thought this 0.999 figure is an overestimate; I analyze this question in this post.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;The event “&lt;script type=&quot;math/tex&quot;&gt;\beta &gt; 0&lt;/script&gt;” is a Bernoulli-distributed random variable; let’s denote it with &lt;script type=&quot;math/tex&quot;&gt;x \sim \text{Bernoulli}(\theta)&lt;/script&gt;. If we draw &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; samples from &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; (and denote the results with &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;), the conditional probability distribution of &lt;script type=&quot;math/tex&quot;&gt;p(\theta \given \{x_i\})&lt;/script&gt; is described by the following directed graphical model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;dgm-theta.svg&quot; alt=&quot;Bayes net for x_i and theta&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The node for &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is filled because it’s observed, and the plate represents &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; copies of this node (with &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; ranging from &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;n_1&lt;/script&gt; (resp. &lt;script type=&quot;math/tex&quot;&gt;n_0&lt;/script&gt;) denote the number of samples where &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is true (resp. false), the likelihood is described by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\{x_i\} \given \theta) = \text{Binomial}(n_1 \given n = S, p = \theta).&lt;/script&gt;

&lt;p&gt;We can assume a noninformative uniform prior on the probability &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; on the unit interval. A Beta prior is conjugate to the Bernoulli likelihood, and &lt;script type=&quot;math/tex&quot;&gt;p(\theta) = \text{Beta}(\theta \given \alpha_0 = 1, \beta_0 = 1) = \text{Uniform}(\theta \given a = 0, b = 1)&lt;/script&gt;, and this results in the following posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta \given \{x_i\}) = \text{Beta}(\theta \given \alpha_0 + n_0, \beta_0 + n_1).&lt;/script&gt;

&lt;p&gt;With &lt;script type=&quot;math/tex&quot;&gt;n_1 = 1000&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;n_0 = 0&lt;/script&gt;, this amounts to a &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(1001, 1)&lt;/script&gt; distribution, whose &lt;a href=&quot;https://en.wikipedia.org/wiki/Beta_distribution#Probability_density_function&quot;&gt;pdf&lt;/a&gt; is as such:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;beta-1000-pdf-big.svg&quot; alt=&quot;Pdf of Beta(1001,1)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As expected, most of the probability mass is close to 1.0. But that graph is not very legible, so let’s zoom in on the right end of the &lt;em&gt;x&lt;/em&gt; axis:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;beta-1000-pdf-zoomed.svg&quot; alt=&quot;Pdf of Beta(1001,1) in [.99,1.0] interval&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The red line marks the mean of the distribution, which is approximately &lt;script type=&quot;math/tex&quot;&gt;0.999&lt;/script&gt;, but not nearly all of the probability mass is on the right side of &lt;script type=&quot;math/tex&quot;&gt;0.999&lt;/script&gt;. Using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cumulative_distribution_function&quot;&gt;cdf&lt;/a&gt; of the posterior, we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta &gt; 0.999) = 0.63,&lt;/script&gt;

&lt;p&gt;meaning there’s still a 1 in 3 chance that the posterior probability that &lt;script type=&quot;math/tex&quot;&gt;\beta &gt; 0&lt;/script&gt; does &lt;em&gt;not&lt;/em&gt; exceed &lt;script type=&quot;math/tex&quot;&gt;0.999&lt;/script&gt;. To be fair, &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;0.999&lt;/script&gt; is still good for a “rough estimate”&lt;/strong&gt;, unless one has a strong prior for &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\beta &lt; 0 %]]&gt;&lt;/script&gt;. (Given the nature of the experiment and the meaning of the parameter &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; – the toxicity of a compound –, a flat prior on “&lt;script type=&quot;math/tex&quot;&gt;\beta &gt; 0&lt;/script&gt;” is reasonable.)&lt;/p&gt;

&lt;h3 id=&quot;presidential-elections&quot;&gt;Presidential elections&lt;/h3&gt;

&lt;p&gt;A similar statement was made for 1988 pre-election polls, on page 70:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;All of the 1000 simulations &lt;script type=&quot;math/tex&quot;&gt;\theta_1 &gt; \theta_2&lt;/script&gt;; thus, the estimated posterior probability that Bush had more support than Dukakis in the survey population is over 99.9%.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When a presidential election is won “by a landslide”, that rarely means more than a 60-40% results; so in this case, I would rather use a prior that puts more mass on results close to 50-50%, for example &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(10,10)&lt;/script&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;beta-10-pdf.svg&quot; alt=&quot;Pdf of Beta(10,10)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This results in the following posterior:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;beta-1010-pdf.svg&quot; alt=&quot;Pdf of Beta(1010,10)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So in this case, the crude estimate does does not suffice, and we should rather be only 98% certain. (This is a 20-fold difference, &lt;script type=&quot;math/tex&quot;&gt;(1-.98)/(1-0.999)&lt;/script&gt;, and a well-calibrated &lt;a href=&quot;https://goodjudgment.com/philip-tetlocks-10-commandments-of-superforecasting/&quot;&gt;superforecaster&lt;/a&gt; could tell them apart.) If the stakes are high, then refine your model, and draw more samples.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The meaning of 1000 true + 0 false simulations depends on your prior beliefs: the posterior mean could be 0.999 (with a uniform prior), or anything less than 0.99 (with a prior weighted more towards the center or zero).&lt;/p&gt;

&lt;p&gt;I love BDA3; I’m nowhere near finished, but even the first chapters have taught me new ideas and proofs (e.g. the Bayesian cookbook in section 3.8, or modeling normal data with unknown mean &lt;em&gt;and&lt;/em&gt; variance). The examples and exercises are a great combination of applications and theory. As you can see from this post, all I can do is nitpick some tiny details. A quick intro to practical Bayesian modeling is a &lt;a href=&quot;https://www.youtube.com/watch?v=T1gYvX5c2sM&quot;&gt;presentation from Andrew Gelman&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Did you like this post, did I make a mistake, or do you know a BDA3 discussion group? Let me know in the comments below!&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set_matplotlib_formats&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'svg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pdf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;θ&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Probability density function&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Pdf of Beta(θ | α = {alpha}, β = {beta})&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;beta-1000-pdf-big.svg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;beta-1000-pdf-big.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ramanujan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Series that converges to 1/π at an exponential rate,
    by Srinivasa Ramanujan&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9801&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                               &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
                               &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;396&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                               &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1103&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;26390&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                              &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1/ramanujan({i}) - π ≈ {1/ramanujan(i) - math.pi:.2e}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Easter egg. Thanks for reading!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;1/ramanujan(1) - π ≈ 7.64e-08&lt;/p&gt;

  &lt;p&gt;1/ramanujan(2) - π ≈ 4.44e-16&lt;/p&gt;

  &lt;p&gt;1/ramanujan(3) - π ≈ 0.00e+00&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.990&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;beta-1000-pdf-zoomed.svg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;beta-1000-pdf-zoomed.svg&quot; alt=&quot;svg&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;0.999001996007984&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(θ &amp;gt; 0.999) = {:d}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;P(θ &amp;gt; 0.999) = 63%&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(θ &amp;gt; 0.998) = {:d}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.998&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;P(θ &amp;gt; 0.998) = 86%&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. 2013. &lt;em&gt;Bayesian Data Analysis: Third Edition&lt;/em&gt;. &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;Official webpage&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="statistics," /><category term="Bayes," /><category term="sampling" /><summary type="html">Bayesian Data Analysis from Gelman et al. (2013), in section 3.7, presents the statistical analysis of a bioassay experiment. The parameters of the model are , and we draw samples from the numerically calculated posterior. Then the authors write:</summary></entry><entry><title type="html">Evaluation of function calls in Haskell</title><link href="https://www.treszkai.com/blog/haskell-eval" rel="alternate" type="text/html" title="Evaluation of function calls in Haskell" /><published>2019-07-13T00:00:00+02:00</published><updated>2019-07-13T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/haskell-eval</id><content type="html" xml:base="https://www.treszkai.com/blog/haskell-eval">&lt;p&gt;Chapter 27 of &lt;a href=&quot;http://haskellbook.com/&quot;&gt;&lt;em&gt;Haskell Programming from first principles&lt;/em&gt;&lt;/a&gt; (by Christopher Allen and Julie Moronuki) is about the evaluation system of Haskell, with a focus on non-strictness. In the section &lt;em&gt;Preventing sharing on purpose&lt;/em&gt;, they write you want to prevent sharing the result of a function call when it would mean storing some big data just to calculate a small result. Two examples are provided to demonstrate the alternatives. In the first, the result of &lt;code class=&quot;highlighter-rouge&quot;&gt;g _&lt;/code&gt; is not shared but calculated twice:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Prelude&amp;gt; f x = (x 3) + (x 10)
Prelude&amp;gt; g' = \_ -&amp;gt; trace &quot;hi g'&quot; 2
Prelude&amp;gt; f g'
hi g'
hi g'
4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the second, the result of &lt;code class=&quot;highlighter-rouge&quot;&gt;g _&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; shared, i.e. calculated only once and the result is stored:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Prelude&amp;gt; g = const (trace &quot;hi g&quot; 2)
Prelude&amp;gt; f g
hi g
4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(Edited to add:) In practice, sharing is usually achieved with a &lt;code class=&quot;highlighter-rouge&quot;&gt;let&lt;/code&gt; expression or a &lt;code class=&quot;highlighter-rouge&quot;&gt;where&lt;/code&gt; construct.&lt;/p&gt;

&lt;p&gt;(Note that this latter is called a &lt;a href=&quot;https://wiki.haskell.org/Pointfree&quot;&gt;“point-free” definition&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The authors conclude that&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;functions aren’t shared when there are named arguments but are when the arguments are elided, as in pointfree. So, one way to prevent sharing is adding named arguments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Quoted from version 1.0RC4 of the book.)&lt;/p&gt;

&lt;p&gt;In this post I analyze the runtime differences between point-free and pointful definitions.&lt;/p&gt;

&lt;h2 id=&quot;behind-the-scenes&quot;&gt;Behind the scenes&lt;/h2&gt;

&lt;p&gt;As &lt;a href=&quot;#Further-resources&quot;&gt;Tom Ellis describes&lt;/a&gt;, the definitions of &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; translate to the following (in a close approximation to the “Core” language used during compilation):&lt;/p&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g&quot;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tg&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g'&quot;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(Calling &lt;code class=&quot;highlighter-rouge&quot;&gt;f g&lt;/code&gt; with these definitions does &lt;em&gt;not&lt;/em&gt; result in the same trace in GHCi 8.6.5 as with the original definitions. However, the code has the expected behavior if loaded into GHCi from a source file like &lt;a href=&quot;#Sharing&quot;&gt;that below&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Two things to point out here. First, every function definition is a lambda. Second, &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; was turned into a &lt;em&gt;let&lt;/em&gt; expression because we can only apply functions to variables or literals (in Core), not to function calls. &lt;em&gt;Edited to add:&lt;/em&gt; It would be reasonable to ask why &lt;code class=&quot;highlighter-rouge&quot;&gt;g = const (trace &quot;hi g&quot; 2)&lt;/code&gt;  doesn’t translate to &lt;code class=&quot;highlighter-rouge&quot;&gt;\y -&amp;gt; let {tg = trace &quot;hi g&quot; 2} in const tg y&lt;/code&gt; (similar to &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt;), to which the pragmatic answer is that &lt;em&gt;apparently&lt;/em&gt; the order is the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;not-fully-applied functions are turned into lambdas,&lt;/li&gt;
  &lt;li&gt;parameters that are function calls are turned into named variables, and&lt;/li&gt;
  &lt;li&gt;named function arguments from the left-hand side of &lt;code class=&quot;highlighter-rouge&quot;&gt;=&lt;/code&gt; are moved to the right as a lambda.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;evaluation-with-sharing&quot;&gt;Evaluation with sharing&lt;/h2&gt;

&lt;p&gt;This is what happens during the evaluation of &lt;code class=&quot;highlighter-rouge&quot;&gt;f g&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ans&lt;/code&gt; is a function call, so its evaluation proceeds with substituting &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; for the argument of &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ans&lt;/code&gt; is a &lt;em&gt;let&lt;/em&gt; expression, so we put the following &lt;em&gt;thunks&lt;/em&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;x3&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;x10&lt;/code&gt; on the heap under some unique name:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…and then proceed with evaluating the &lt;em&gt;in&lt;/em&gt; part:&lt;/p&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_x3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_x10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;During the evaluation of this function call, &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x3&lt;/code&gt; will be evaluated (or potentially &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x10&lt;/code&gt; first, or both in parallel). &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x3&lt;/code&gt; is a function call, so first we evaluate &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; to a lambda. As &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is a &lt;em&gt;let&lt;/em&gt; expression, we create a closure for &lt;code class=&quot;highlighter-rouge&quot;&gt;trace &quot;hi g&quot; 2&lt;/code&gt; on the heap, and then continue with the &lt;em&gt;in&lt;/em&gt; part of &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;\y -&amp;gt; const tg y&lt;/code&gt;). This is a lambda now, meaning it’s in weak head normal form, so the heap contents for &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is overwritten with that:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g&quot;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Back to &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x3&lt;/code&gt;, now the argument &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; is substituted in the definition of &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans_x3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a function call, with &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt; already a lambda &lt;code class=&quot;highlighter-rouge&quot;&gt;\x _ -&amp;gt; x&lt;/code&gt;, so the arguments can now be substituted in the body, leaving us with&lt;/p&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;-- (Pointer to the same address as g_tg.)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;During the evaluation of &lt;code class=&quot;highlighter-rouge&quot;&gt;g_tg&lt;/code&gt;, the magic printout happens (&lt;code class=&quot;highlighter-rouge&quot;&gt;hi g&lt;/code&gt; on stdout), and its value is resolved to be &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;, so the heap is updated as such:&lt;/p&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x3&lt;/code&gt; is a pointer to the same memory content &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Analogously, the evaluation of &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x10&lt;/code&gt; proceeds as such:&lt;/p&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans_x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_tg&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;-- let ans_x10 points to the memory location of g_tg:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, &lt;code class=&quot;highlighter-rouge&quot;&gt;ans = (+) ans_x3 ans_x10&lt;/code&gt;, which evaluates to &lt;code class=&quot;highlighter-rouge&quot;&gt;ans = 4&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-without-sharing&quot;&gt;Evaluation without sharing&lt;/h2&gt;

&lt;p&gt;In contrast, the evaluation of &lt;code class=&quot;highlighter-rouge&quot;&gt;f g'&lt;/code&gt; proceeds as follows:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x3'&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x10'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ans'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_x3'&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_x10'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x3'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g'&quot;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now &lt;code class=&quot;highlighter-rouge&quot;&gt;hi g'&lt;/code&gt; is printed, and the heap is updated:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x3'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When evaluating &lt;code class=&quot;highlighter-rouge&quot;&gt;ans_x10'&lt;/code&gt;, we &lt;strong&gt;again print&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;hi g'&lt;/code&gt;, and store the result of the trace under a different thunk:&lt;/p&gt;

&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Heap:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ans_x10'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now &lt;code class=&quot;highlighter-rouge&quot;&gt;ans'&lt;/code&gt; evaluates to &lt;code class=&quot;highlighter-rouge&quot;&gt;(+) 2 2&lt;/code&gt;, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;4&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;attempt-at-verifying-my-translated-definitions&quot;&gt;Attempt at verifying my translated definitions&lt;/h2&gt;

&lt;p&gt;I attempted to verify what I was saying above about the definitions of &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;g'&lt;/code&gt; in Core, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;-ddump-simpl&lt;/code&gt; compiler flag of GHCi, but it didn’t fulfil my expectations.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Sharing&quot;&gt;&lt;/a&gt;Sharing.hs:&lt;/p&gt;
&lt;div class=&quot;language-haskell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kr&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Sharing&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;where&lt;/span&gt;

&lt;span class=&quot;kr&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Debug.Trace&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;::&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Int&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;-- share&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g'&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g'&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;-- don't share&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g''&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hi g&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tg&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;-- share (equivalent to g)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In GHCi:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Prelude&amp;gt; :set -ddump-simpl -dsuppress-all -Wno-missing-signatures
Prelude&amp;gt; :l Sharing
[1 of 1] Compiling Sharing          ( Sharing.hs, interpreted )

==================== Tidy Core ====================
Result size of Tidy Core
  = {terms: 52, types: 39, coercions: 0, joins: 0/0}

f = \ x_a1Fl -&amp;gt; + $fNumInt (x_a1Fl (I# 3#)) (x_a1Fl (I# 10#))
g = \ @ b_a1Gi -&amp;gt; const (trace (unpackCString# &quot;hi g&quot;#) (I# 2#))
g' = \ @ p_a1G6 -&amp;gt; \ _ -&amp;gt; trace (unpackCString# &quot;hi g'&quot;#) (I# 2#)
tg_r1F4 = trace (unpackCString# &quot;hi g&quot;#) (I# 2#)
g'' = \ @ b_a1FJ -&amp;gt; \ y_a1Fn -&amp;gt; const tg_r1F4 y_a1Fn

... and some more stuff
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Nonetheless, as &lt;a href=&quot;https://stackoverflow.com/a/6121495/8424390&quot;&gt;a SO answer describes&lt;/a&gt;, we can see that a function application in Core is defined as &lt;code class=&quot;highlighter-rouge&quot;&gt;Expr Atom&lt;/code&gt;, where &lt;em&gt;Atom&lt;/em&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;var | Literal&lt;/code&gt;. I attempted to install &lt;a href=&quot;http://hackage.haskell.org/package/ghc-core&quot;&gt;ghc-core&lt;/a&gt; but the build failed, so further analysis is put on the shelf.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;So, what’s the essential difference between &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;g'&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;g = const (trace &quot;hi g&quot; 2)&lt;/code&gt; is a function application where the argument is a function application, which is treated as a &lt;em&gt;let&lt;/em&gt; expression. When you evaluate &lt;code class=&quot;highlighter-rouge&quot;&gt;g ()&lt;/code&gt;, the auxiliary variable introduced by the &lt;em&gt;let&lt;/em&gt; – i.e.,&lt;code class=&quot;highlighter-rouge&quot;&gt;tg = trace &quot;hi g&quot; 2&lt;/code&gt; – is evaluated to a literal and its value is stored on the heap. On subsequent calls, some other argument can be applied to the &lt;code class=&quot;highlighter-rouge&quot;&gt;const tg&lt;/code&gt; function, but its first argument &lt;code class=&quot;highlighter-rouge&quot;&gt;tg&lt;/code&gt; is already evaluated.&lt;/p&gt;

&lt;p&gt;In contrast, &lt;code class=&quot;highlighter-rouge&quot;&gt;g' = \_ -&amp;gt; trace &quot;hi g'&quot; 2&lt;/code&gt; is a lambda, so it is already fully evaluated, and nothing in it can be simplified further. If we apply &lt;code class=&quot;highlighter-rouge&quot;&gt;g'&lt;/code&gt; first to the argument &lt;code class=&quot;highlighter-rouge&quot;&gt;()&lt;/code&gt;, the expression &lt;code class=&quot;highlighter-rouge&quot;&gt;g' ()&lt;/code&gt; will evaluate to the body of &lt;code class=&quot;highlighter-rouge&quot;&gt;g'&lt;/code&gt; with the unused parameter discarded, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;trace &quot;hi g'&quot; 2&lt;/code&gt;. If we later evaluate &lt;code class=&quot;highlighter-rouge&quot;&gt;g' []&lt;/code&gt;, then it again results in the (same) body after the (dummy) function application. Nowhere during this process did we store the value of &lt;code class=&quot;highlighter-rouge&quot;&gt;trace &quot;hi g'&quot; 2&lt;/code&gt;: in particular, we didn’t update the definition of &lt;code class=&quot;highlighter-rouge&quot;&gt;g'&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;\_ -&amp;gt; 2&lt;/code&gt;, simply because that is not the definition of &lt;code class=&quot;highlighter-rouge&quot;&gt;g'&lt;/code&gt;. (But could we have updated it? Even though functions are always pure, I think the answer is generally &lt;em&gt;no&lt;/em&gt;: sometimes the result of a function is bigger than the definition, and the result is not needed often enough to warrant this speed–memory tradeoff.)&lt;/p&gt;

&lt;p&gt;Recall the original wording:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;functions aren’t shared when there are named arguments but are when the arguments are elided, as in pointfree.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As we saw, &lt;em&gt;functions&lt;/em&gt; themselves are never shared. Rather, if &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is a partially applied function whose argument is a function application &lt;code class=&quot;highlighter-rouge&quot;&gt;fun arg&lt;/code&gt;, then &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is equivalent to a &lt;em&gt;let&lt;/em&gt; expression, and after its first evaluation &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; will &lt;em&gt;change&lt;/em&gt; to a lambda with &lt;code class=&quot;highlighter-rouge&quot;&gt;fun arg&lt;/code&gt; already evaluated.&lt;/p&gt;

&lt;p&gt;As a generally-okay heuristic, point-free definitions allow sharing inner function calls, whereas nothing in a lambda (or a function with all arguments on the left-hand side) is shared.&lt;/p&gt;

&lt;h2 id=&quot;further-resources&quot;&gt;Further resources&lt;/h2&gt;

&lt;p&gt;More details on similar behavior are given by Tom Ellis in his talk &lt;a href=&quot;https://skillsmatter.com/skillscasts/8726-haskell-programs-how-do-they-run&quot;&gt;&lt;em&gt;Haskell programs: how do they run?&lt;/em&gt;&lt;/a&gt; (free registration required to watch the talk).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://skillsmatter.com/skillscasts/8800-functional-and-low-level-watching-the-stg-execute&quot;&gt;talk of David Luposchainsky (a.k.a. &lt;code class=&quot;highlighter-rouge&quot;&gt;quchen&lt;/code&gt;)&lt;/a&gt; goes into more depth – down to the Core –, in which he uses his own implementation of the spineless tagless graph reduction machine (STG), to visualize the evaluation of any given Haskell code (&lt;a href=&quot;https://github.com/quchen/stgi&quot;&gt;link to repo&lt;/a&gt;).&lt;/p&gt;</content><author><name></name></author><category term="Haskell" /><summary type="html">Chapter 27 of Haskell Programming from first principles (by Christopher Allen and Julie Moronuki) is about the evaluation system of Haskell, with a focus on non-strictness. In the section Preventing sharing on purpose, they write you want to prevent sharing the result of a function call when it would mean storing some big data just to calculate a small result. Two examples are provided to demonstrate the alternatives. In the first, the result of g _ is not shared but calculated twice:</summary></entry><entry><title type="html">The wise men puzzle</title><link href="https://www.treszkai.com/blog/wise-men" rel="alternate" type="text/html" title="The wise men puzzle" /><published>2018-08-18T00:00:00+02:00</published><updated>2018-08-18T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/wise-men</id><content type="html" xml:base="https://www.treszkai.com/blog/wise-men">&lt;p&gt;Today I understood the wise men puzzle at a conceptual level, well enough that I could explain it and possibly generalize to similar domains. This post is my attempt at explaining it.&lt;/p&gt;

&lt;p&gt;The puzzle is described in &lt;a class=&quot;citation&quot; href=&quot;#Huth2000-Logic-book&quot;&gt;(Huth &amp;amp; Ryan, 2000)&lt;/a&gt; as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are three wise men. It’s common knowledge—known by everyone and known to be known by everyone, etc.—that there are three red hats and two white hats. The king puts a hat on each of the wise men in such a way that they are not able to see their own hat, and asks each one in turn whether they are not able to see their own hat, and asks each one in turn whether they know the color of the hat on their head. Suppose the first man says he does not know; then the second says he does not know either.
It follows that the third man must be able to say that he knows the colour of his hat. Why is this? What colour has the third man’s hat?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s call the people Alpha, Beta, Gamma, in the order they speak.&lt;/p&gt;

&lt;p&gt;One solution is to think about the puzzle in terms of possible worlds. A world in this problem is described by an assignment of hat colors to people, which is equally an ordered triple of colours &lt;script type=&quot;math/tex&quot;&gt;⟨c_1, c_2, c_3⟩&lt;/script&gt;, with &lt;script type=&quot;math/tex&quot;&gt;c_i ∈ \{R,W\}&lt;/script&gt;. There are only 2 white hats, so in the beginning, the seven possible worlds are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{cc}
⟨R,R,R⟩ &amp; ⟨R,R,W⟩ &amp; ⟨R,W,R⟩ &amp; ⟨R,W,W⟩ \\
⟨W,R,R⟩ &amp; ⟨W,R,W⟩ &amp; ⟨W,W,R⟩ &amp; \\
\end{array}. %]]&gt;&lt;/script&gt;

&lt;p&gt;If Beta and Gamma were both wearing white hats, then Alpha would know that that his hat is red. Therefore, when Alpha says “no”, Beta and Gamma both learn that both of them cannot be white, i.e. at least one of them is red. The remaining possible worlds are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{cc}
{⟨R,R,R⟩} &amp; ⟨R,R,W⟩ &amp; ⟨R,W,R⟩ &amp; \crossed{⟨R,W,W⟩} \\
{⟨W,R,R⟩} &amp; ⟨W,R,W⟩ &amp; ⟨W,W,R⟩ &amp; \\
\end{array}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, &lt;em&gt;we&lt;/em&gt; know that the world is one of the 6 worlds above, but Beta also sees the hats of Alpha and Gamma. What we think as outsiders only matters for whether &lt;em&gt;we&lt;/em&gt; can tell who’s wearing what.
But back to the observations of A,B,C. When Beta says “no”, that rules out the worlds where Gamma is white (because then Beta would be red).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{cc}
{⟨R,R,R⟩} &amp; \crossed{⟨R,R,W⟩} &amp; ⟨R,W,R⟩ &amp; \crossed{⟨R,W,W⟩} \\
{⟨W,R,R⟩} &amp; \crossed{⟨W,R,W⟩} &amp; ⟨W,W,R⟩ &amp; \\
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;This means that Gamma is red, and he also knows this.&lt;/p&gt;

&lt;h1 id=&quot;another-way&quot;&gt;Another way&lt;/h1&gt;

&lt;p&gt;Our solution is more procedural than is necessary, and it does not show the essence of omniscient agents acting with one another. As this problem is small enough, we could list for every world every statement any agent could make, which is simply their knowledge base of true statements (i.e. whatever they can deduce from their view and from the common knowledge, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;). (Say, with atoms &lt;script type=&quot;math/tex&quot;&gt;R_1, R_2, R_3, W_1, W_2, W_3&lt;/script&gt;, meaning “I think person &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has color X”, with a &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt; abbreviating &lt;script type=&quot;math/tex&quot;&gt;R_1\wedge W_2 \wedge R_3&lt;/script&gt;.) We can only do this because we are not interested in making statements like “X knows that Y knows that Z knows that φ”.
Besides, in every world, we implicitly include what is common knowledge, and what any agent can see, i.e. the whole problem statement in the opening paragraph.
The common knowledge at the beginning in any of these worlds is &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩&lt;/script&gt;. That’s not very much, but at least symmetric, which allows us to write down only three worlds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When Alpha says “no” in the beginning, that means he is not in a world where from his knowledge base he can conclude his own colour. His statement becomes common knowledge (&lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;), i.e. &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt; is extended with &lt;script type=&quot;math/tex&quot;&gt;\lnot(W_2\wedge W_3)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,W,W⟩}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,W,W⟩}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,W,W⟩}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,W,W⟩}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,W,W⟩}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We were able to cross out some worlds! And in the world &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,W⟩&lt;/script&gt; we were left with zero possible worlds for Alpha, i.e. Alpha’s statement would lead to a contradiction: he would have answered “yes”. In fact, this was how we eliminated possible-worlds in the previous solution. Next turn: the king asks Beta, who says “no”. The common knowledge is extended with &lt;script type=&quot;math/tex&quot;&gt;\lnot(W_1 \wedge W_3)&lt;/script&gt;. (Right? At this point I can imagine myself making an incorrect deduction.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div id=&quot;mistaken1&quot; style=&quot;display: block;&quot;&gt;
  &lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨W,R,W⟩}&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&quot;fixed1&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\star \wedge \lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,R,W}⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨W,R,W⟩}&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,R,W}⟩&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,R,R}⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,R,W}⟩&lt;/script&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨W,R,W⟩}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,W⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨W,R,W⟩}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨W,R,W⟩}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨W,R,W⟩}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another world disappeared. But what about &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;, why is it still there, when last time we argued that it’s not possible for Gamma to be white? In fact, it is not: in that world Beta would have said yes, as he knew what colour he had.
Although never explicitly stated, we assumed that if someone’s not then he’s white, and vice versa. Use &lt;script type=&quot;math/tex&quot;&gt;\star&lt;/script&gt; to denote this fact:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\star ≡ \bigwedge_{i=1}^3 (\lnot R_i → W_i) \wedge (\lnot W_i → R_i).&lt;/script&gt;

&lt;p&gt;We also know that common knowledge is true: for every formula &lt;script type=&quot;math/tex&quot;&gt;φ&lt;/script&gt;, it’s an axiom that &lt;script type=&quot;math/tex&quot;&gt;\mathcal C φ → φ&lt;/script&gt;.
Then, it’s simple to show that Alpha is red and Gamma is white, Beta is red.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal C \Big((R_1 \vee R_2 \vee R_3) \wedge (R_2 \vee R_3) \wedge (R_1 \vee R_3) \Big) \wedge \star \vdash
    (R_1 \wedge W_3) → R_2.&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
function showById(id, btn, displayStyle) {
    document.getElementById(id).style.display = 'block';
    btn.style.display = 'none';
}
function showInlineById(id, btn, displayStyle) {
    document.getElementById(id).style.display = 'inline';
    btn.style.display = 'none';
}
function hideById(id, btn) {
    document.getElementById(id).style.display = 'none';
    btn.style.display = 'none';
}
&lt;/script&gt;

&lt;p&gt;Click this to fix that above: &lt;a href=&quot;#&quot; onclick=&quot;showById('fixed1', this); hideById('mistaken1', this); return false;&quot;&gt;click me!&lt;/a&gt; (Needs JavaScript.)&lt;/p&gt;

&lt;p&gt;Now we are left with the following worlds:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\star \wedge \lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;span id=&quot;mistaken2&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt;&lt;/span&gt; &lt;span id=&quot;fixed2&quot; style=&quot;display: none;&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\crossed{⟨R,R,W⟩}&lt;/script&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\star \wedge \lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\star \wedge \lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;World&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;, &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;: &lt;script type=&quot;math/tex&quot;&gt;\star \wedge \lnot⟨W,W,W⟩\ \wedge \lnot(W_2\wedge W_3) \wedge \lnot(W_1\wedge W_3)&lt;/script&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alpha: &lt;script type=&quot;math/tex&quot;&gt;⟨R,W,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Beta: &lt;script type=&quot;math/tex&quot;&gt;⟨W,R,R⟩&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Gamma: &lt;script type=&quot;math/tex&quot;&gt;⟨W,W,R⟩&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At first sight, Gamma’s knowledge base in some worlds (&lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt;) contains a world with &lt;script type=&quot;math/tex&quot;&gt;\lnot R_3&lt;/script&gt;. But every four of the above worlds has &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt;, meaning &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt; is deducible from &lt;script type=&quot;math/tex&quot;&gt;\star&lt;/script&gt; and the &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;, making &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,W⟩&lt;/script&gt; in world &lt;script type=&quot;math/tex&quot;&gt;⟨R,R,R⟩&lt;/script&gt; impossible. &lt;a href=&quot;#&quot; onclick=&quot;showInlineById('fixed2', this); hideById('mistaken2', this); return false;&quot;&gt;Click me to fix that.&lt;/a&gt; This means &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt; is &lt;abbr title=&quot;Common knowledge&quot;&gt;CK&lt;/abbr&gt;. Yay!&lt;/p&gt;

&lt;p&gt;Note: there might be some other true statements that could be deduced, so maybe Alpha knows his colour too in some worlds—I haven’t solved the problem in full. For example, when Gamma answers “yes” in the end, it doesn’t say anything we didn’t already know, and nothing that Alpha and Beta didn’t know already, as &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt; can be deduced from the common knowledge. Maybe someone else knows theirs too?&lt;/p&gt;

&lt;h1 id=&quot;another-problem&quot;&gt;Another problem&lt;/h1&gt;

&lt;p&gt;A slight modification is to map a natural number &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; to worlds where X is able to decide their colour after &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; utterances, if it wasn’t X who spoke last.&lt;/p&gt;

&lt;p&gt;Related: it feels like there is a situation with &lt;script type=&quot;math/tex&quot;&gt;n&gt;2&lt;/script&gt; people, where two agents can keep on discarding possible worlds just by them speaking in turns. If you know of one such problem, please let me know.&lt;/p&gt;

&lt;h1 id=&quot;notes&quot;&gt;Notes&lt;/h1&gt;

&lt;p&gt;I hope I didn’t make a mistake in the calculations, I admit I enumerated the possible worlds by hand instead of with Prolog.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Listen to people when they say “no”.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Huth2000-Logic-book&quot;&gt;Huth, M., &amp;amp; Ryan, M. D. (2000). &lt;i&gt;Logic in Computer Science - modelling and reasoning about systems&lt;/i&gt;. Cambridge University Press.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><category term="logic" /><summary type="html">Today I understood the wise men puzzle at a conceptual level, well enough that I could explain it and possibly generalize to similar domains. This post is my attempt at explaining it.</summary></entry><entry><title type="html">Blog post summary: Medical AI safety: where are we and where are we heading</title><link href="https://www.treszkai.com/blog/medical-safety" rel="alternate" type="text/html" title="Blog post summary: Medical AI safety: where are we and where are we heading" /><published>2018-07-11T00:00:00+02:00</published><updated>2018-07-11T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/medical-safety</id><content type="html" xml:base="https://www.treszkai.com/blog/medical-safety">&lt;p&gt;In this post I summarize a &lt;a href=&quot;https://lukeoakdenrayner.wordpress.com/2018/07/11/medical-ai-safety-we-have-a-problem/&quot;&gt;blog post about “medical AI safety”&lt;/a&gt;: the potential consequences of using advanced medical systems without sufficient evidence to back up their usefulness.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Epistemic status: the author (Luke Oakden-Rayner) is a PhD candidate radiologist, and I’m not an expert in medicine.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the first time ever, AI systems could actually be responsible for medical disasters.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The risk of a medical AI system increases with its complexity: from the lowest complexity &lt;em&gt;processing systems&lt;/em&gt;, through &lt;em&gt;triage systems&lt;/em&gt; that order the priority queue of patients, we are now moving towards autonomous &lt;em&gt;diagnostic systems&lt;/em&gt;, and eventually to autonomous &lt;em&gt;prediction systems&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Some systems in the wild are worse than humans in both recall and sensitivity:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Not only did CAD [computer-aided diagnosis] increase the recalls without improving cancer detection, but, in some cases, even decreased sensitivity by missing some cancers.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nonetheless, we are already proceeding to the next level:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A few months ago the FDA approved a new AI system by IDx, and it makes independent medical decisions without the need for a clinician. [In this case, screening for eye disease through a retina scan.]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But on the upside, these tools improve the ratio of people screened:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;But while there is a big potential upside here (about 50% of people with diabetes are not screened regularly enough), and the decision to “refer or not” is rarely immediately vision-threatening, approving a system like this without &lt;em&gt;clinical testing&lt;/em&gt; raises some concerns.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And systems operate now on a larger scale too:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;NHS is already using an automated smart-phone triage system “powered by” babylonhealth AI. This one is definitely capable of leading to serious harm, since it recommends when to go (or not to go) to hospital.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;… which system gave 90% confidence to non-lethal diagnosis X, not even offering lethal diagnosis Y which was suggested by 90% of MDs on Twitter. (And I assume it’s not even an adversarial attack.) It’s fair to say that there is room for improvement. (Compare this with the amount of news coverage received by the monthly crash of an autonomous vehicle.)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The real point is that none of the FDA, NHS, nor the various regulatory agencies in other nations appear to be concerned [to the extent required] about the specific risks of autonomous decision making AI.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Are we potentially racing towards an AI event on the scale of elixir sulfanilamide [which prompted the foundation of FDA] or thalidomide [which the FDA banned before other countries, preventing 10,000 birth malformations]?&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">In this post I summarize a blog post about “medical AI safety”: the potential consequences of using advanced medical systems without sufficient evidence to back up their usefulness.</summary></entry><entry><title type="html">International Winter School on Gravity and Light, Tutorial 3: Multilinear Algebra – Solutions for Exercise 1</title><link href="https://www.treszkai.com/blog/multilinear-tutorial" rel="alternate" type="text/html" title="International Winter School on Gravity and Light, Tutorial 3: Multilinear Algebra – Solutions for Exercise 1" /><published>2018-06-09T00:00:00+02:00</published><updated>2018-06-09T00:00:00+02:00</updated><id>https://www.treszkai.com/blog/multilinear-tutorial</id><content type="html" xml:base="https://www.treszkai.com/blog/multilinear-tutorial">&lt;p&gt;Solutions for exercise 1 of &lt;a href=&quot;https://www.youtube.com/watch?v=5oeWX3NUhMA&quot;&gt;tutorial 3&lt;/a&gt; of the &lt;a href=&quot;https://gravity-and-light.herokuapp.com&quot;&gt;International Winter School on Gravity and Light&lt;/a&gt;. (&lt;a href=&quot;https://www.youtube.com/watch?v=mbv3T15nWq0&quot;&gt;Link to video of lecture 3&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;

&lt;p&gt;On this solution sheet, I’ll speak of a vector space &lt;script type=&quot;math/tex&quot;&gt;(V,+,\cdot)&lt;/script&gt; over a field &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;+: V\times V \rightarrow V&lt;/script&gt; is the addition and &lt;script type=&quot;math/tex&quot;&gt;\cdot: K \times V \rightarrow V&lt;/script&gt; is called (scalar) multiplication or S-multiplication. The field &lt;script type=&quot;math/tex&quot;&gt;(K, \textcolor{red}{+}, \textcolor{red}{\cdot})&lt;/script&gt; has &lt;script type=&quot;math/tex&quot;&gt;\textcolor{red}{+}:K\times K \rightarrow K&lt;/script&gt; as addition and &lt;script type=&quot;math/tex&quot;&gt;\textcolor{red}{\cdot}:K\times K \rightarrow K&lt;/script&gt; as multiplication operations. The dot is often omitted, i.e. &lt;script type=&quot;math/tex&quot;&gt;a \mathbf v&lt;/script&gt; is short for &lt;script type=&quot;math/tex&quot;&gt;a \cdot \mathbf v&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a b&lt;/script&gt; is short for &lt;script type=&quot;math/tex&quot;&gt;a \textcolor{red}{\cdot} b&lt;/script&gt;. (Note that the lecture dealt with real vector spaces, i.e. the field &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; was always the set of reals &lt;script type=&quot;math/tex&quot;&gt;\mathbb R&lt;/script&gt;.)
The scalars, i.e. the elements of &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, are denoted with normal letters &lt;script type=&quot;math/tex&quot;&gt;a,b&lt;/script&gt;, and the vectors, i.e. the elements of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, are denoted with boldface letters &lt;script type=&quot;math/tex&quot;&gt;\mathbf u, \mathbf v, \mathbf w&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
function showById(id, btn) {
    document.getElementById(id).style.display = 'block';
    btn.style.display = 'none';
}
function showByClass(cls, btn) {
    for (var x of document.getElementsByClassName(cls))
        x.style.display = 'block';
    btn.style.display = 'none';
}
function hideByClass(cls) {
    for (var x of document.getElementsByClassName(cls))
        x.style.display = 'none';
}
&lt;/script&gt;

&lt;h1 id=&quot;exercise-1-true-or-false&quot;&gt;Exercise 1: True or false?&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Tick the correct statements, but not the incorrect ones.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showByClass('answer', this); hideByClass('show-answer'); return false;&quot;&gt;Show all answers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;a) Which statements on vector spaces are correct?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;Commutativity of multiplication is a vector space axiom.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer1', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer1&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;The scalar multiplication &lt;script type=&quot;math/tex&quot;&gt;\cdot: K \times V \rightarrow V&lt;/script&gt; doesn’t even have the same sets in its two arguments, i.e. &lt;script type=&quot;math/tex&quot;&gt;\mathbf v \cdot a&lt;/script&gt; is not even defined.&lt;/li&gt;
    &lt;li&gt;The vector space has the commutativity of &lt;em&gt;addition&lt;/em&gt; as an axiom: for any &lt;script type=&quot;math/tex&quot;&gt;\mathbf u,\mathbf v \in V&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;{\mathbf u+\mathbf v} = {\mathbf v + \mathbf u}&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;The underlying field &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; &lt;em&gt;does&lt;/em&gt; have the commutativity of multiplication as a field axiom: for any &lt;script type=&quot;math/tex&quot;&gt;a,b \in K&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a \textcolor{red}{\cdot} b = b \textcolor{red}{\cdot} a&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;As a consequence, for any &lt;script type=&quot;math/tex&quot;&gt;\mathbf v \in V&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a, b \in K&lt;/script&gt;,&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;a (b \mathbf v) = (a\textcolor{red}{\cdot} b)\mathbf v = (b \textcolor{red}{\cdot} a)\mathbf v = b(a \mathbf v).&lt;/script&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;Every vector is a matrix with only one column.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer2', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer2&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;By definition, a vector is an element of a vector space. If we fix a basis for the vector space, then any vector can be represented by an ordered set of numbers, which could be treated as a column vector, i.e. a matrix with one column. However, this representation depends on the choice of basis.&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://youtu.be/5oeWX3NUhMA?t=1m09s&quot;&gt;official answer&lt;/a&gt; brings up as a counterexample the vector space of polynomials up to some finite degree. However, here again we could represent the vectors as a column vector with any choice of a basis. E.g. using the standard basis, &lt;script type=&quot;math/tex&quot;&gt;p(x) = 0x^2 + 4x + 5&lt;/script&gt; could be represented as &lt;script type=&quot;math/tex&quot;&gt;\mathbf p = [0, 4, 5]^T&lt;/script&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;Every linear map between vector spaces can be represented by a unique quadratic matrix.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer3', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer3&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;As above, a linear map &lt;script type=&quot;math/tex&quot;&gt;f: V \rightarrow W&lt;/script&gt; can be represented as a unique matrix only once bases are chosen for its domain &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; and codomain &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;This matrix is quadratic only if the dimensions of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; are equal.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;Every vector space has a corresponding dual vector space.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer4', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer4&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; true.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;The dual space of a vector space &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; is defined as the set of linear maps from &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;V^* \coloneqq Hom(V,K) \coloneqq \{φ\ \vert \ φ: V \linmap K\}&lt;/script&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;The set of everywhere positive functions on &lt;script type=&quot;math/tex&quot;&gt;\mathbb R&lt;/script&gt; with pointwise addition and S-multiplication is a vector space.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer5', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer5&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;This set doesn’t have a commutative identity element: by the field axioms of &lt;script type=&quot;math/tex&quot;&gt;\mathbb R&lt;/script&gt;, it could only be the constant zero function, but that’s not an element of the set.&lt;/li&gt;
    &lt;li&gt;This set doesn’t have a commutative inverse for any element.&lt;/li&gt;
    &lt;li&gt;For the scalar multiplication we’d need to know the underlying field. Usually it would be &lt;script type=&quot;math/tex&quot;&gt;\mathbb R&lt;/script&gt;, but then S-multiplication with a negative number wouldn’t result in an everywhere positive function. (Although one can construct a field from &lt;script type=&quot;math/tex&quot;&gt;\mathbb R^+&lt;/script&gt;, I wonder how well that would combine with the above attempt at a vector space.)&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;b) What is true about tensors and their components?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;The tensor product of two tensors is a tensor.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer6', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer6&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; true.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;The lecture didn’t mention tensor products, so a definition is in order. The product of an &lt;script type=&quot;math/tex&quot;&gt;(l,k)&lt;/script&gt;-tensor &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; and an &lt;script type=&quot;math/tex&quot;&gt;(n,m)&lt;/script&gt;-tensor &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;(l+n,k+m)&lt;/script&gt;-tensor &lt;script type=&quot;math/tex&quot;&gt;S \otimes T&lt;/script&gt;, whose &lt;script type=&quot;math/tex&quot;&gt;(i_1, \ldots, i_{l+n}, j_1, \ldots, j_{k+m})&lt;/script&gt;-th component is the product of the relevant components of &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;:&lt;/li&gt;
  &lt;/ul&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;(S \otimes T)^{i_1, \ldots, i_l, i_{l+1}, \ldots, i_{l+n}}_ {j_1, \ldots, j_k, j_{k+1}, \ldots, j_{k+m} } =
   S^{i_1, \ldots, i_l}_ {j_1, \ldots, j_k}
   T^{i_{1}, \ldots, i_{n}}_ {j_{1}, \ldots, j_{m}}.&lt;/script&gt;

  &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tensor#Tensor_product&quot;&gt;Source: Wikipedia&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;This means that if the arguments of &lt;script type=&quot;math/tex&quot;&gt;S \otimes T&lt;/script&gt; are&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;the &lt;script type=&quot;math/tex&quot;&gt;l+n&lt;/script&gt; linear maps &lt;script type=&quot;math/tex&quot;&gt;φ^{(p)} = \sum^{dim V}_{i=1} \varphi^{(p)}_i \epsilon^i&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;1 \le p \le l+n&lt;/script&gt;, and&lt;/li&gt;
    &lt;li&gt;the &lt;script type=&quot;math/tex&quot;&gt;k+m&lt;/script&gt; vectors &lt;script type=&quot;math/tex&quot;&gt;\v_{(q)} = \sum^{dim V}_{j=1} v_{(q)}^j \e_j&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;1 \le q \le k+m&lt;/script&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;(with some particular choice of basis vectors &lt;script type=&quot;math/tex&quot;&gt;\{\e_i\}_i&lt;/script&gt; and basis covectors &lt;script type=&quot;math/tex&quot;&gt;\{\epsilon^i\}_i&lt;/script&gt; ), then&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
(S\otimes T) &amp;(φ^{(1)}, \ldots, φ^{(l+n)}, \v_{(1)}, \ldots, \v_{(k+m)}) = \\
  &amp;= S (φ^{(1)}, \ldots, φ^{(l)}, \v_{(1)}, \ldots, \v_{(k)})\,\cdot\,
  T (φ^{(l+1)}, \ldots, φ^{(l+n)}, \v_{(k+1)}, \ldots, \v_{(k+m)})\\
  &amp;= \Bigg(
      \sum_{i_1}^{\dim V} \cdots \sum_{i_l}^{\dim V}
      \sum_{j_1}^{\dim V} \cdots \sum_{j_k}^{\dim V}
      \varphi^{(1)}_{i_1} \ldots \varphi^{(l)}_{i_l}
      v_{(1)}^{j_1} \ldots v_{(k)}^{j_k}
      S^{i_1, \ldots, i_l}_{j_1, \ldots, j_k}
  \Bigg) \cdot \phantom.\\
  &amp;\phantom{=} \Bigg(
      \sum_{i_{l+1}}^{\dim V} \cdots \sum_{i_{l+n}}^{\dim V}
      \sum_{j_{k+1}}^{\dim V} \cdots \sum_{j_{k+m}}^{\dim V}
      \varphi^{(l+1)}_{i_{l+1}} \ldots \varphi^{(l+n)}_{i_{l+n}}
      v_{(k+1)}^{j_{k+1}} \ldots v_{(k+m)}^{j_{k+m}}
      T^{i_{l+1}, \ldots, i_{l+n}}_{j_{k+1}, \ldots, j_{k+n}}
  \Bigg) \\
  &amp;=  \sum_{i_1}^{\dim V} \cdots \sum_{i_{l+n}}^{\dim V}
      \sum_{j_1}^{\dim V} \cdots \sum_{j_{k+m}}^{\dim V}
      \varphi^{(1)}_{i_1} \ldots \varphi^{(l+n)}_{i_{l+n}}
      v_{(1)}^{j_1} \ldots v_{(k+m)}^{j_{k+m}}
      S^{i_1, \ldots, i_l}_{j_1, \ldots, j_k}
      T^{i_{l+1}, \ldots, i_{l+n}}_{j_{k+1}, \ldots, j_{k+n}}.
\end{aligned} %]]&gt;&lt;/script&gt;

  &lt;p&gt;These &lt;script type=&quot;math/tex&quot;&gt;(l+n+k+m)&lt;/script&gt; summations are quite a mess, but the above derivation shows that the &lt;a href=&quot;http://mathworld.wolfram.com/EinsteinSummation.html&quot;&gt;Einstein summation convention&lt;/a&gt; works for tensor products as well:&lt;/p&gt;

  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
(S\otimes T) &amp;(φ^{(1)}, \ldots, φ^{(l+n)}, v_{(1)}, \ldots, v_{(k+m)}) =\\
  &amp;= S (φ^{(1)}, \ldots, φ^{(l)}, v_{(1)}, \ldots, v_{(k)})\,\cdot\,
  T (φ^{(l+1)}, \ldots, φ^{(l+n)}, v_{(k+1)}, \ldots, v_{(k+m)})\\
  &amp;= \Big(
      \varphi^{(1)}_{i_1} \ldots \varphi^{(l)}_{i_l}
      v_{(1)}^{j_1} \ldots v_{(k)}^{j_k}
      S^{i_1, \ldots, i_l}_{j_1, \ldots, j_k}
  \Big)
  \Big(
      \varphi^{(l+1)}_{i_{l+1}} \ldots \varphi^{(l+n)}_{i_{l+n}}
      v_{(k+1)}^{j_{k+1}} \ldots v_{(k+m)}^{j_{k+m}}
      T^{i_{l+1}, \ldots, i_{l+n}}_{j_{k+1}, \ldots, j_{k+n}}
  \Big) \\
 &amp;=  \varphi^{(1)}_{i_1} \ldots \varphi^{(l+n)}_{i_{l+n}}
      v_{(1)}^{j_1} \ldots v_{(k+m)}^{j_{k+m}}
      S^{i_1, \ldots, i_l}_{j_1, \ldots, j_k}
      T^{i_{l+1}, \ldots, i_{l+n}}_{j_{k+1}, \ldots, j_{k+n}}.
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;You can always reconstruct a tensor from its components and the corresponding basis.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer7', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer7&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; true.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;If we know the basis vectors for the vector space and the dual vector space, then the components of the vector and covector arguments are uniquely determined, and we can apply the tensor to the arguments using the components of the tensor (or some relevant finite subset in case &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; is not finite dimensional).&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;The number of indices of the tensor components depends on dimension.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer8', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer8&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;A tensor component usually has one index for each argument, e.g. for a &lt;script type=&quot;math/tex&quot;&gt;(2,1)&lt;/script&gt;-tensor &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;, the components are &lt;script type=&quot;math/tex&quot;&gt;T^{i_1,i_2}_{j_1}&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;The &lt;em&gt;range&lt;/em&gt; of these indices does depend on the dimension: each index ranges from &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\dim V&lt;/script&gt;. Therefore an &lt;script type=&quot;math/tex&quot;&gt;(n,m)&lt;/script&gt;-tensor &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; has &lt;script type=&quot;math/tex&quot;&gt;(\dim V)^{n+m}&lt;/script&gt; many components.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;The Einstein summation convention does not apply to tensor components.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer9', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer9&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification: see above.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; &lt;em&gt;A change of basis does not change the tensor components.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer10', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer10&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;the tensor components are defined with respect to a given basis.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;c) Given a basis for a &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-dimensional vector space &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, …&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; …&lt;em&gt;one can find exactly &lt;script type=&quot;math/tex&quot;&gt;d^2&lt;/script&gt;-different dual bases for the corresponding dual vector space &lt;script type=&quot;math/tex&quot;&gt;V^*&lt;/script&gt;.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer11', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer11&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Given a basis of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;E = \{\mathbf{e}_i\}_{i=1}^d \subset V&lt;/script&gt;, there is a &lt;em&gt;unique&lt;/em&gt; dual basis of &lt;script type=&quot;math/tex&quot;&gt;V^*&lt;/script&gt;, namely &lt;script type=&quot;math/tex&quot;&gt;E^* = \{\epsilon_i\}_{i=1}^d&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\epsilon_i(\e_i) = 1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\epsilon_i(\e_j) = 0&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i ≠ j&lt;/script&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; …&lt;em&gt;by removing one basis vector of the basis of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, a basis for a &lt;script type=&quot;math/tex&quot;&gt;(d - 1)&lt;/script&gt;-dimensional vector space &lt;script type=&quot;math/tex&quot;&gt;V_1&lt;/script&gt; is obtained.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer12', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer12&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; true.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;The resulting set of &lt;script type=&quot;math/tex&quot;&gt;(d-1)&lt;/script&gt; vectors are still linearly independent, and their span is a &lt;script type=&quot;math/tex&quot;&gt;(d-1)&lt;/script&gt;-dimensional subspace of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; …&lt;em&gt;the continuity of a map &lt;script type=&quot;math/tex&quot;&gt;f : V → W&lt;/script&gt; depends on the choice of basis for the vector space &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer13', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer13&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; false.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;The continuity of a map is defined for &lt;em&gt;topological spaces&lt;/em&gt;, not for vector spaces.&lt;/li&gt;
    &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is continuous &lt;em&gt;iff&lt;/em&gt; the preimage of every open set in &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; is open in &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;. Note that no term in this definition depends on the choice of basis for either &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;Assuming that &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; are real vector spaces, it is customary to equip them with the standard topology. A set &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is open in &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; &lt;em&gt;iff&lt;/em&gt; either it is the union of open &lt;script type=&quot;math/tex&quot;&gt;ε&lt;/script&gt;-balls, or of Cartesian products of open intervals. While these definitions assume a basis for &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, they all result in the exact same topologies. (Meaning a set can be covered with open balls &lt;em&gt;iff&lt;/em&gt; it can be covered with open cuboids &lt;em&gt;iff&lt;/em&gt; it can be covered with open cubes – an interesting but easy-to-prove result.)&lt;/li&gt;
    &lt;li&gt;It’s easy to see that every &lt;em&gt;linear&lt;/em&gt; map between real vector spaces (equipped with the standard topology) is continuous.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; …&lt;em&gt;one can extract the components of the elements of the dual vector space &lt;script type=&quot;math/tex&quot;&gt;V^*&lt;/script&gt;.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer14', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer14&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; true.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;a basis for &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; uniquely determines a dual basis for &lt;script type=&quot;math/tex&quot;&gt;V^*&lt;/script&gt;, which uniquely determines the components of any covector.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;?.&lt;/strong&gt; …&lt;em&gt;each vector of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; can be reconstructed from its components.&lt;/em&gt; &lt;a href=&quot;#&quot; onclick=&quot;showById('answer15', this); return false;&quot; class=&quot;show-answer&quot;&gt;Show answer&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;answer15&quot; class=&quot;answer&quot; style=&quot;display: none;&quot;&gt;
  &lt;p&gt;&lt;em&gt;Answer:&lt;/em&gt; true.&lt;/p&gt;

  &lt;p&gt;Clarification:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Given the basis vectors &lt;script type=&quot;math/tex&quot;&gt;\mathbf{e}_i&lt;/script&gt; and components &lt;script type=&quot;math/tex&quot;&gt;v^i&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;1 \leq i \leq d&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{v} = \sum_{i=1}^d v^i \mathbf{e}_i&lt;/script&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Solutions for exercise 1 of tutorial 3 of the International Winter School on Gravity and Light. (Link to video of lecture 3.)</summary></entry></feed>