<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Laszlo Treszkai - AI</title><link href="https://www.treszkai.com/" rel="alternate"></link><link href="https://www.treszkai.com/feeds/ai.atom.xml" rel="self"></link><id>https://www.treszkai.com/</id><updated>2019-10-08T00:00:00+02:00</updated><entry><title>Trust in numbers â€” notes of a talk given by Sir DavidÂ Spiegelhalter</title><link href="https://www.treszkai.com/2019/10/08/trust-in-numbers/" rel="alternate"></link><published>2019-10-08T00:00:00+02:00</published><updated>2019-10-08T00:00:00+02:00</updated><author><name>Laszlo Treszkai</name></author><id>tag:www.treszkai.com,2019-10-08:/2019/10/08/trust-in-numbers/</id><summary type="html">&lt;p&gt;Summary of a keynote talk given by Sir David Spiegelhalter about the reporting of medical&amp;nbsp;results.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Institute of Medical Statistics of the Center for Medical Statistics, Informatics and Intelligent Systems at the Medical University of Vienna &lt;a href="https://cemsiis.meduniwien.ac.at/50years-of-ms/"&gt;just turned 50 years old&lt;/a&gt;, and they organized a two-day event around it. I was fortunate to have attended the keynote talk of Sir David Spiegelhalter (&lt;a href="https://en.wikipedia.org/wiki/David_Spiegelhalter"&gt;wiki&lt;/a&gt;), who is a British statistician and &lt;a href="https://en.wikipedia.org/wiki/Winton_Professorship_of_the_Public_Understanding_of_Risk"&gt;Winton Professor of the Public Understanding of Risk&lt;/a&gt; at the Faculty of Mathematics, University of Cambridge, which was one of the most entertaining &lt;em&gt;and&lt;/em&gt; informative talk I have heard. There is no way I can do justice to the talk, and I wouldnâ€™t even attempt to bring through the humor (his &lt;em&gt;humour&lt;/em&gt;) â€“ the goal of this post is to increase your vigilance a little bit when it comes to any reports about science, and to shed light on the work of&amp;nbsp;Spiegelhalter.&lt;/p&gt;
&lt;p&gt;The professor has authored several academic books on statistics, and was interviewed by the &lt;span class="caps"&gt;CNN&lt;/span&gt; with the title, &lt;a href="https://edition.cnn.com/videos/tv/2019/04/01/amanpour-david-spiegelhalter-statistics.cnn"&gt;&lt;em&gt;Why statistics should make you suspicious&lt;/em&gt;&lt;/a&gt;. And keeps doing a huge service to science in a number of other&amp;nbsp;ways.&lt;/p&gt;
&lt;p&gt;The problem explained in the talk was that &lt;strong&gt;numbers are used to persuade people, not to inform them&lt;/strong&gt;. (Actually, that was only the first half â€“ the second half offered a handful of steps we could take when presenting our data.) Take for example politics, and the campaign around Brexit. Even if it were true that it costs Â£350 million a week for the &lt;span class="caps"&gt;UK&lt;/span&gt; to be a member of the &lt;span class="caps"&gt;EU&lt;/span&gt;, it would be much less misleading if it said that it costs 80 pence &lt;em&gt;per person per day&lt;/em&gt; to be a member of the &lt;span class="caps"&gt;EU&lt;/span&gt;. The cost of a bag of potato chips. (The other side committed similar errors too â€“ Iâ€™m not trying to win a battle&amp;nbsp;here.)&lt;/p&gt;
&lt;p&gt;&lt;img alt="We send the EU Â£350 million a week; letâ€™s fund our NHS instead. Vote Leave." src="https://www.treszkai.com/2019/10/08/trust-in-numbers/nhs.png"&gt;&lt;/p&gt;
&lt;p&gt;As Eliezer Yudkowsky says, &lt;a href="https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer"&gt;politics is the mind-killer&lt;/a&gt;, but of course, using numbers to mislead instead of to show an honest representation of reality is done everywhere where there are numbers. My favorite topic these days: &lt;strong&gt;medical statistics&lt;/strong&gt;. Iâ€™m picking a topic from the talk as an example (which Spiegelhalter analyzed in more detail in a &lt;a href="https://medium.com/wintoncentre/are-we-individuals-or-members-of-populations-the-deeper-issues-behind-the-sausage-wars-a067aebf2063"&gt;Medium post&lt;/a&gt;): dietary advice about processed meat consumption. &lt;span class="caps"&gt;CNN&lt;/span&gt; did a &lt;a href="https://edition.cnn.com/2019/04/17/health/colorectal-cancer-risk-red-processed-meat-study-intl/index.html"&gt;great job&lt;/a&gt; with picking the title of their article to be as close to the original conclusions as possible: &lt;em&gt;Eating just one slice of bacon a day linked to higher risk of colorectal cancer, says study&lt;/em&gt;. But by the time this study reaches The Sun, it gets reported as the&amp;nbsp;following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Rasher of bacon a day is deadly" src="https://www.treszkai.com/2019/10/08/trust-in-numbers/bacon.jpg"&gt;.&lt;/p&gt;
&lt;p&gt;Boy, that escalated quickly. And what does â€œhigher risk of colorectal cancerâ€ mean anyhow? In this case, the study showed a 19% increase. As Peter Attia explains in his detailed post series on science, &lt;a href="https://peterattiamd.com/ns001/"&gt;Studying Studies&lt;/a&gt;, such big numbers generally mean an increase in &lt;em&gt;relative risk&lt;/em&gt;, not in &lt;em&gt;absolute risk&lt;/em&gt;. Relative risk is meaningless without knowing the base rate of the disease. In this case, 5% of &lt;span class="caps"&gt;US&lt;/span&gt; men and women born today are expected to be diagnosed with colorectal cancer sometime during their lives. Add 19% to that 5% figure (i.e., multiply it by 1.19), and you get 6%, for the people who eat 1 slice of bacon a day. (The 5% figure is surprisingly high, by the way! Fortunately, it has a five-year survival rate of 65%. I donâ€™t know how much of the 5% is a false positive; I guess it doesnâ€™t include the disconfirmed cases. These figures I just gathered from &lt;a href="https://en.wikipedia.org/wiki/Colorectal_cancer#Epidemiology"&gt;Wikipedia&lt;/a&gt;, &lt;span class="caps"&gt;FWIW&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;You can take the extra step and visualize these numbers using what &lt;a href="https://en.wikipedia.org/wiki/Gerd_Gigerenzer"&gt;Gigerenzer&lt;/a&gt; calls natural frequencies. As one Wikipedia author puts it, â€œthe problem is not simply in the human mind, but in the representation of the informationâ€, so letâ€™s deliver using things we evolved to understand: a small tribe of human-like&amp;nbsp;icons.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All&lt;/em&gt; of these people below eat a clean diet without processed meat, and those with a distraught face will get colorectal&amp;nbsp;cancer:&lt;/p&gt;
&lt;p&gt;ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜«&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜&lt;/p&gt;
&lt;p&gt;And &lt;em&gt;all&lt;/em&gt; of these people eat a slice of &lt;a href="https://en.wikipedia.org/wiki/Extrawurst"&gt;Extrawurst&lt;/a&gt;&amp;nbsp;daily:&lt;/p&gt;
&lt;p&gt;ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜«&lt;br /&gt;
ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;
ğŸ˜«ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜&lt;br /&gt;ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜«ğŸ˜ğŸ˜&lt;/p&gt;
&lt;p&gt;See the difference? Itâ€™s that one troubled guy in row&amp;nbsp;9.&lt;/p&gt;
&lt;p&gt;Now, Iâ€™m not saying bacon is good for health, or that that additional risk factor would be negligible (admittedly, my mocking tone above suggests otherwise). But if the scientists, journalists, and clinicians report the risk honestly, &lt;em&gt;and&lt;/em&gt; no-one is trying to influence you into eating more burgers by playing at our primal instincts (including the marketing division of McDonaldâ€™s and our social group who calls you chicken if you donâ€™t eat your &lt;a href="https://en.wikipedia.org/wiki/Black_pudding"&gt;black pudding&lt;/a&gt;), then us puny humans could make more educated decisions about which sacrifices we are willing to&amp;nbsp;make.&lt;/p&gt;
&lt;p&gt;This post was just a tiny part of what was said at the talk. In parting, I have two takeaway quotes.&amp;nbsp;First,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;80% of statistics are&amp;nbsp;false.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(From anonymous statistician, a comedian, and also &lt;a href="https://www.youtube.com/embed/aHGd6LqAVzw?start=43"&gt;Elon Musk&lt;/a&gt;.) Unfortunately, this factoid alone doesnâ€™t enable one to navigate&amp;nbsp;reality.&lt;/p&gt;
&lt;p&gt;The second quote is of a little more value, but still doesnâ€™t help one to sieve through&amp;nbsp;statistics:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thereâ€™s no point in being trustworthy if youâ€™re&amp;nbsp;boring.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(From Spiegelhalter in todayâ€™s&amp;nbsp;talk.)&lt;/p&gt;
&lt;p&gt;This talk was anything but boring. If you have a chance to see Spiegelhalter in person, do so: he gets my highest grade recommendation. (He also has a book, titled &lt;a href="https://smile.amazon.com/Art-Statistics-How-Learn-Data/dp/1541618513"&gt;&lt;em&gt;The Art of Statistics&lt;/em&gt;&lt;/a&gt;, which I havenâ€™t&amp;nbsp;read.)&lt;/p&gt;
&lt;p&gt;(Somewhat related: just today on my way home I learned of Edward Tufteâ€™s book, &lt;em&gt;The Visual Display of Quantitative Information,&lt;/em&gt; which also &lt;a href="https://www.edwardtufte.com/tufte/books_vdqi"&gt;looks amazing&lt;/a&gt;.)&lt;/p&gt;</content><category term="AI"></category></entry><entry><title>On the overconfidence of modern neuralÂ networks</title><link href="https://www.treszkai.com/2019/09/26/overconfidence/" rel="alternate"></link><published>2019-09-26T00:00:00+02:00</published><updated>2019-09-26T00:00:00+02:00</updated><author><name>Laszlo Treszkai</name></author><id>tag:www.treszkai.com,2019-09-26:/2019/09/26/overconfidence/</id><summary type="html">&lt;p&gt;Evaluating various methods to improve the calibration of deep neural&amp;nbsp;networks.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;On the overconfidence of modern neural networks&lt;/em&gt;. This is the title of the coursework I did with a fellow student at the University of Edinburgh. (&lt;span class="caps"&gt;PDF&lt;/span&gt;: &lt;a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw3.pdf"&gt;Part 1&lt;/a&gt;, &lt;a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw4.pdf"&gt;Part 2&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Our topic was influenced by a previous study, titled &lt;em&gt;On Calibration of Modern Neural Networks&lt;/em&gt; &lt;!-- {% cite Guo2017-calibration %} --&gt; &lt;a class="citation" href="#Guo2017-calibration"&gt;(Guo, Pleiss, Sun, &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Weinberger, 2017)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Applications of uncertainty estimation include threshold-based outlier detection, active learning, uncertainty-driven exploration of reinforcement learning, or certain safety-critical&amp;nbsp;applications.&lt;/p&gt;
&lt;h2&gt;What is&amp;nbsp;uncertainty?&lt;/h2&gt;
&lt;p&gt;No computer vision system is perfect, so an image classification algorithm sometimes identifies people as not-people, or not-people as people.
While we usually care about the class with the highest output (the â€œmost likelyâ€ class), we can treat the softmax outputs of a classifier as uncertainty estimates.
(After all, that is how we trained a model when treating the softmax outputs of a classifier as a probability distribution, and minimizing the negative log likelihood of the model given the data.)
For example, out of 1000 classifications made with an output of 0.8, approximately 800 should be correct &lt;em&gt;if the system is well-calibrated&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Example output of a YOLO object detection network" src="https://www.treszkai.com/2019/09/26/overconfidence/yolo.png"&gt;&lt;/p&gt;
&lt;p&gt;(Example output of a &lt;span class="caps"&gt;YOLO&lt;/span&gt; object detection network, with the probability estimates. Image source: &lt;a href="https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/"&gt;Analytics Vidhya&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Ideally, we want our system to be 100% correct, but we rarely have access to an all-knowing Oracle. In cases where it is hard to distinguish between two categories (like on the cat-dog below) we want the uncertainties to be well-calibrated, so that predictions are neither overly confident nor insufficiently&amp;nbsp;confident.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Image of a cat that could be mistaken for a dog" src="https://www.treszkai.com/2019/09/26/overconfidence/catdog.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;(Image source: Google&amp;nbsp;Brain)&lt;/p&gt;
&lt;h2&gt;Our&amp;nbsp;results&lt;/h2&gt;
&lt;h3&gt;Interim&amp;nbsp;report&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw3.pdf"&gt;Link to report (&lt;span class="caps"&gt;PDF&lt;/span&gt;)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our initial experiments showed that our baseline model is already well-calibrated when trained on the &lt;span class="caps"&gt;EMNIST&lt;/span&gt; By-Class dataset.
Calibration worsened when we used only a subset of the training set.
We found that increasing regularization increases calibration, but too much regularization leads to a decrease in both accuracy and calibration. (See figure below.)
This contradicts the findings of &lt;!-- {% cite Guo2017-calibration -L section -l 3 %} --&gt; &lt;a class="citation" href="#Guo2017-calibration"&gt;(Guo, Pleiss, Sun, &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Weinberger, 2017, sec. 3)&lt;/a&gt;, who found that model calibration can improve by increasing the weight decay constant, well after the model achieves minimum classification accuracy.
One of our main findings is that cross-entropy error is not a good indicator of model&amp;nbsp;calibration.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5 of our interim report." src="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw3-fig5.png"&gt;&lt;/p&gt;
&lt;p&gt;(&lt;span class="caps"&gt;ECE&lt;/span&gt;: expected calibration error. The lower the&amp;nbsp;better.)&lt;/p&gt;
&lt;h3&gt;Final&amp;nbsp;report&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw4.pdf"&gt;Link to report (&lt;span class="caps"&gt;PDF&lt;/span&gt;)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We replicate the findings of &lt;!-- {% cite Guo2017-calibration %} --&gt; &lt;a class="citation" href="#Guo2017-calibration"&gt;(Guo, Pleiss, Sun, &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Weinberger, 2017)&lt;/a&gt;Â£ that deep neural networks achieve higher accuracy but worse calibration than shallow nets, and compare different approaches for improving the calibration of neural networks (see figure below). As the baseline approach, we consider the calibration of the softmax outputs from a single network; this is compared to &lt;em&gt;deep ensembles&lt;/em&gt;, &lt;em&gt;&lt;span class="caps"&gt;MC&lt;/span&gt; dropout&lt;/em&gt;, and &lt;em&gt;concrete dropout&lt;/em&gt;. Through experiments on the &lt;span class="caps"&gt;CIFAR&lt;/span&gt;-100 data set, we find that a large neural network can be significantly over-confident about its predictions. We show on a classification problem that an ensemble of deep networks has better classification accuracy and calibration compared to a single network, and that &lt;span class="caps"&gt;MC&lt;/span&gt; dropout and concrete dropout significantly improve the calibration of a large&amp;nbsp;network.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Confidence and calibration plots for BigNet. (Figure 2 of our report)" src="https://www.treszkai.com/2019/09/26/overconfidence/mlp-cw4-fig2.png"&gt;&lt;/p&gt;
&lt;p&gt;(&lt;em&gt;Top row:&lt;/em&gt; confidence plots for a deep neural net. The more skewed to the right, the better. &lt;em&gt;Bottom row:&lt;/em&gt; corresponding calibration plots. The more close to the diagonal, the&amp;nbsp;better.)&lt;/p&gt;
&lt;h2&gt;Things I would do&amp;nbsp;differently&lt;/h2&gt;
&lt;p&gt;With a little more experience behind my back now, I would make the following changes in experiment design and writing the report:
 - &lt;em&gt;Use a validation set.&lt;/em&gt; We only used a training set because we trained for minimum error, and we expected &lt;em&gt;calibration&lt;/em&gt; to be independent from &lt;em&gt;accuracy&lt;/em&gt;, but that is a strong assumption (and likely incorrect, seeing our results in the interim report).
 - &lt;em&gt;Use better biblography sources.&lt;/em&gt; Instead of Google Scholar, I would search &lt;a href="https://dblp.uni-trier.de/"&gt;&lt;span class="caps"&gt;DBLP&lt;/span&gt;&lt;/a&gt;, where the information is more correct and consistent.
 - &lt;em&gt;Use pastel colors.&lt;/em&gt; I let my collaborator have it his way, but ever since this submission Iâ€™m having nightmares in purple and glowing green&amp;nbsp;:D&lt;/p&gt;
&lt;p&gt;In future work, I would like to test the calibration of a Bayesian neural network, where the weights of the network have a probability distribution instead of a point&amp;nbsp;estimate.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!&amp;#8212; {% bibliography &amp;#8212;cited %} &amp;#8212;&gt;

&lt;ol class="bibliography"&gt;&lt;li&gt;&lt;span id="Guo2017-calibration"&gt;Guo, C., Pleiss, G., Sun, Y., &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Weinberger, &lt;span class="caps"&gt;K. Q.&lt;/span&gt;(2017). On Calibration of Modern Neural Networks. In D. Precup &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; &lt;span class="caps"&gt;Y. W.&lt;/span&gt; Teh (Eds.), &lt;i&gt;Proceedings of the 34th International Conference on Machine Learning&lt;/i&gt; (Vol. 70, pp. 1321â€“1330). International Convention Centre, Sydney, Australia: &lt;span class="caps"&gt;PMLR&lt;/span&gt;. Retrieved from&amp;nbsp;http://proceedings.mlr.press/v70/guo17a.html&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="AI"></category></entry><entry><title>Paper summary: Abbeel, Ng: Inverse Reinforcement LearningÂ (2004)</title><link href="https://www.treszkai.com/2019/08/19/irl-summary/" rel="alternate"></link><published>2019-08-19T00:00:00+02:00</published><updated>2019-08-19T00:00:00+02:00</updated><author><name>Laszlo Treszkai</name></author><id>tag:www.treszkai.com,2019-08-19:/2019/08/19/irl-summary/</id><summary type="html">&lt;p&gt;Summary of the seminal paper on inverse reinforcement&amp;nbsp;learning.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is a summary of the seminal paper on inverse reinforcement learning: Pieter Abbeel, Andrew Y. Ng: &lt;em&gt;Apprenticeship Learning via Inverse Reinforcement Learning&lt;/em&gt; (2004) [&lt;a href="http://ai.stanford.edu/~pabbeel/irl/"&gt;link&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Traditional &lt;a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html"&gt;reinforcement learning&lt;/a&gt; (&lt;span class="caps"&gt;RL&lt;/span&gt;) starts with specifying a reward function, and during training we search for policies that maximize this reward function&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. In contrast, inverse reinforcement learning (&lt;span class="caps"&gt;IRL&lt;/span&gt;) starts with expert demonstrations of the desired behavior, infers a reward function that the expert likely followed, and trains a policy to maximize&amp;nbsp;that.&lt;/p&gt;
&lt;!-- more --&gt;

&lt;p&gt;&lt;span class="caps"&gt;IRL&lt;/span&gt; is useful for learning complex tasks where it is hard to manually specify a reward function that makes desirable trade-offs between desiderata; such tasks include driving a car or teaching a robot to do a backflip, where we want the car to reach to the destination promptly but also safely, or the robot to flip with its arms straight and &lt;a href="https://youtu.be/xet3KDUfS_U?t=50"&gt;sticking the landing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In contrast with previous attempts at apprenticeship learning (i.e. learning from an expert), which tried to mimic the expert demonstrations directly, &lt;span class="caps"&gt;IRL&lt;/span&gt; assumes that the expert follows a reward function that is a linear combination of the feature vectors (&lt;script type="math/tex"&gt;R = w^T Ï†(s)&lt;/script&gt;), and finds a reward function that maximizes the received reward under the set of demonstrations. The hand-specified function &lt;script type="math/tex"&gt;Ï†: Sâ†’â„^k&lt;/script&gt; maps a state of the Markov decision process (&lt;span class="caps"&gt;MDP&lt;/span&gt;) to a feature vector, which vector includes parameters for the different desiderata of the task, such as the distances to objects surrounding the car, the speed of the car, or the current&amp;nbsp;lane.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;IRL&lt;/span&gt; assumes knowledge of an expert policy &lt;script type="math/tex"&gt;Ï€_E&lt;/script&gt;, or at least samples from it. Using these, we only care about the estimated &amp;#8220;accumulated feature values&amp;#8221;, &lt;script type="math/tex"&gt;Î¼(Ï€_E) âˆˆ â„^k&lt;/script&gt;, which is the expected discounted sum of the feature vectors if sampled from the policy, because then the value of a policy (parametrised by &lt;script type="math/tex"&gt;w&lt;/script&gt;) can be calculated from it directly: &lt;script type="math/tex"&gt;R = w^T Î¼(Ï€_E)&lt;/script&gt;.&lt;/p&gt;
&lt;p&gt;The goal is then to find a policy whose performance is close to that of the expert&amp;#8217;s on the unknown reward function &lt;script type="math/tex"&gt;R_{\star} = w^T_{\star} Ï†&lt;/script&gt;. This is done by finding a policy whose feature vector is close to the expert&amp;#8217;s feature vector, which assures that the value of these policies is close&amp;nbsp;too.&lt;/p&gt;
&lt;p&gt;The algorithm for &lt;span class="caps"&gt;IRL&lt;/span&gt; is the following:
 1. Pick a random initial policy, and calculate its &lt;script type="math/tex"&gt;Î¼&lt;/script&gt;.
 2. Find the vector of weights w that lies within the unit ball and &lt;em&gt;maximizes&lt;/em&gt; the difference between the expert feature expectations and the feature expectations of our best policy thus far.
 3. If this maximum is small, then go to step 7.
 4. Otherwise &lt;script type="math/tex"&gt;w&lt;/script&gt; is our new weights for &lt;script type="math/tex"&gt;R&lt;/script&gt;.
 5. Calculate optimal policy for this &lt;script type="math/tex"&gt;R&lt;/script&gt;.
 6. Repeat from step 2.
 7. Let the agent designer pick a policy from any of those found in step 5 in the different iterations; or find the policy in the convex closure of these policies that is closest to the expert&amp;nbsp;policy.&lt;/p&gt;
&lt;p&gt;The maximization in step 2 allows us to find a policy that is close to the expert&amp;#8217;s, regardless of the choice of a reward function. After all, we are interested in the policy, not the reward function, and so the estimated &lt;script type="math/tex"&gt;R&lt;/script&gt; is not necessarily&amp;nbsp;correct.&lt;/p&gt;
&lt;p&gt;This algorithm is proved to terminate within &lt;script type="math/tex"&gt;O(k \log(k))&lt;/script&gt; steps, using at least &lt;script type="math/tex"&gt;O(k \log(k))&lt;/script&gt; number of samples from the expert&amp;nbsp;policy.&lt;/p&gt;
&lt;p&gt;Experiments are done in a gridworld environment, where &lt;span class="caps"&gt;IRL&lt;/span&gt; learns the expert policy in approximately 100 times less sample trajectories than simply mimicking the expert. Another experiment is a car driving simulator with 3 lanes viewed from the top, where &lt;span class="caps"&gt;IRL&lt;/span&gt; is capable of learning multiple driving styles, such as &amp;#8220;prefer the right lane but avoid collisions&amp;#8221;. Video demonstrations of the latter show that the sentiment of the expert policy is indeed followed, although sometimes with unnecessary lane switches (most modern &lt;span class="caps"&gt;RL&lt;/span&gt; algorithms also exhibit this undesired&amp;nbsp;property).&lt;/p&gt;
&lt;h4&gt;Footnotes&lt;/h4&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Or, more accurately, a policy that maximizes the expected utility derived from this reward function and some method of temporal discounting.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="AI"></category></entry><entry><title>Blog post summary: Medical AI safety: where are we and where are weÂ heading</title><link href="https://www.treszkai.com/2018/07/11/medical-safety/" rel="alternate"></link><published>2018-07-11T00:00:00+02:00</published><updated>2018-07-11T00:00:00+02:00</updated><author><name>Laszlo Treszkai</name></author><id>tag:www.treszkai.com,2018-07-11:/2018/07/11/medical-safety/</id><summary type="html">&lt;p&gt;I summarize a blog post about medical &lt;span class="caps"&gt;AI&lt;/span&gt; safety, which describes the potential consequences of using advanced medical systems without sufficient evidence to back up their&amp;nbsp;usefulness.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post I summarize a &lt;a href="https://lukeoakdenrayner.wordpress.com/2018/07/11/medical-ai-safety-we-have-a-problem/"&gt;blog post about â€œmedical &lt;span class="caps"&gt;AI&lt;/span&gt; safetyâ€&lt;/a&gt;: the potential consequences of using advanced medical systems without sufficient evidence to back up their&amp;nbsp;usefulness.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Epistemic status: the author (Luke Oakden-Rayner) is a PhD candidate radiologist, and I&amp;#8217;m not an expert in&amp;nbsp;medicine.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the first time ever, &lt;span class="caps"&gt;AI&lt;/span&gt; systems could actually be responsible for medical&amp;nbsp;disasters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The risk of a medical &lt;span class="caps"&gt;AI&lt;/span&gt; system increases with its complexity: from the lowest complexity &lt;em&gt;processing systems&lt;/em&gt;, through &lt;em&gt;triage systems&lt;/em&gt; that order the priority queue of patients, we are now moving towards autonomous &lt;em&gt;diagnostic systems&lt;/em&gt;, and eventually to autonomous &lt;em&gt;prediction systems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Some systems in the wild are worse than humans in both recall and&amp;nbsp;sensitivity:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œNot only did &lt;span class="caps"&gt;CAD&lt;/span&gt; [computer-aided diagnosis] increase the recalls without improving cancer detection, but, in some cases, even decreased sensitivity by missing some&amp;nbsp;cancers.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nonetheless, we are already proceeding to the next&amp;nbsp;level:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A few months ago the &lt;span class="caps"&gt;FDA&lt;/span&gt; approved a new &lt;span class="caps"&gt;AI&lt;/span&gt; system by IDx, and it makes independent medical decisions without the need for a clinician. [In this case, screening for eye disease through a retina&amp;nbsp;scan.]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But on the upside, these tools improve the ratio of people&amp;nbsp;screened:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But while there is a big potential upside here (about 50% of people with diabetes are not screened regularly enough), and the decision to â€œrefer or notâ€ is rarely immediately vision-threatening, approving a system like this without &lt;em&gt;clinical testing&lt;/em&gt; raises some&amp;nbsp;concerns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And systems operate now on a larger scale&amp;nbsp;too:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="caps"&gt;NHS&lt;/span&gt; is already using an automated smart-phone triage system â€œpowered byâ€ babylonhealth &lt;span class="caps"&gt;AI&lt;/span&gt;. This one is definitely capable of leading to serious harm, since it recommends when to go (or not to go) to&amp;nbsp;hospital.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;#8230; which system gave 90% confidence to non-lethal diagnosis X, not even offering lethal diagnosis Y which was suggested by 90% of MDs on Twitter. (And I assume it&amp;#8217;s not even an adversarial attack.) It&amp;#8217;s fair to say that there is room for improvement. (Compare this with the amount of news coverage received by the monthly crash of an autonomous&amp;nbsp;vehicle.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The real point is that none of the &lt;span class="caps"&gt;FDA&lt;/span&gt;, &lt;span class="caps"&gt;NHS&lt;/span&gt;, nor the various regulatory agencies in other nations appear to be concerned [to the extent required] about the specific risks of autonomous decision making &lt;span class="caps"&gt;AI&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Are we potentially racing towards an &lt;span class="caps"&gt;AI&lt;/span&gt; event on the scale of elixir sulfanilamide [which prompted the foundation of &lt;span class="caps"&gt;FDA&lt;/span&gt;] or thalidomide [which the &lt;span class="caps"&gt;FDA&lt;/span&gt; banned before other countries, preventing 10,000 birth&amp;nbsp;malformations]?&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="AI"></category><category term="AI"></category><category term="medicine"></category></entry><entry><title>Probabilistically interesting planningÂ problems</title><link href="https://www.treszkai.com/2018/05/28/probabilistically-interesting/" rel="alternate"></link><published>2018-05-28T00:00:00+02:00</published><updated>2018-05-28T00:00:00+02:00</updated><author><name>Laszlo Treszkai</name></author><id>tag:www.treszkai.com,2018-05-28:/2018/05/28/probabilistically-interesting/</id><summary type="html">&lt;p&gt;This post briefly describes the problem of probabilistic planning, and explains what makes a planning problem â€œprobabilistically&amp;nbsp;interestingâ€.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post briefly describes the problem of &lt;em&gt;probabilistic planning&lt;/em&gt;, and explains in informal terms what makes a planning problem &lt;em&gt;probabilistically interesting&lt;/em&gt;, along with some&amp;nbsp;examples.&lt;/p&gt;
&lt;h1&gt;Primer on probabilistic&amp;nbsp;planning&lt;/h1&gt;
&lt;p&gt;In a nutshell, planning is about &lt;em&gt;finding a way to win&lt;/em&gt;, and as such, the field of research on planners is vast. However, there is no single textbook definition of â€œplanningâ€, so in this post I&amp;#8217;ll try to be as general as possible. One description of a planning problem could be: given a description of an environment, find a sequence of actions that brings the environment from the initial state of the environment to a goal state. There are multiple ways to describe the environment: for example in formal logic with the &lt;a href="https://en.wikipedia.org/wiki/Situation_calculus"&gt;situation calculus&lt;/a&gt;, or more commonly as a &lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process"&gt;Markov decision process (&lt;span class="caps"&gt;MDP&lt;/span&gt;)&lt;/a&gt;. In probabilistic planning problems, the functions describing the &lt;span class="caps"&gt;MDP&lt;/span&gt; are not necessarily deterministic: executing action &lt;script type="math/tex"&gt;a&lt;/script&gt; in state &lt;script type="math/tex"&gt;s&lt;/script&gt; will bring the environment to state &lt;script type="math/tex"&gt;s'&lt;/script&gt; with a probability of &lt;script type="math/tex"&gt;T(s,a,s')&lt;/script&gt;. In contrast with the &lt;em&gt;control problem&lt;/em&gt; of reinforcement learning, where the goal is to find an optimal &lt;em&gt;policy&lt;/em&gt; (i.e. a mapping from states to actions), in planning one is interested only in a partial policy that brings the agent closer to a goal state, or frequently only a single action that brings the agent closer to a goal state from the current state. An example planning problem is thus: â€œSiri, show me a way to the library.â€ Then Siri responds either with a plan that I can follow from the first step to the last (i.e. a route from start to finish), or only an action that I can take right now (â€œgo forward 100&amp;nbsp;metersâ€).&lt;/p&gt;
&lt;p&gt;Graphical representation of an example &lt;span class="caps"&gt;MDP&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graphical representation of an example MDP" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/MDP-env.jpg"&gt;&lt;/p&gt;
&lt;p&gt;An example policy for the same &lt;span class="caps"&gt;MDP&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="An example policy for the same MDP" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/MDP-policy.jpg"&gt;&lt;/p&gt;
&lt;p&gt;An example plan for the same &lt;span class="caps"&gt;MDP&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="An example plan for the same MDP" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/MDP-plan.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The approach taken by a planner differs based on the discounting factor \&lt;script type="math/tex"&gt; \gamma \&lt;/script&gt; and the distribution of rewards. In a &lt;em&gt;shortest path problem&lt;/em&gt; the future rewards are discounted (\&lt;script type="math/tex"&gt; 0 &lt; \gamma &lt; 1 \&lt;/script&gt;), and there might be a constant negative reward for every step taken. Together with a positive reward in goal states, an agent with the goal of maximizing return â€“ i.e. the sum of discounted expected future rewards â€“ has incentives to minimize the length of the path to the goal. However, if there is no discounting (\&lt;script type="math/tex"&gt;\gamma = 1 \&lt;/script&gt;) and there's a positive reward only in the goal states, it is sufficient for the agent to find &lt;em&gt;any&lt;/em&gt; way to the goal. (Some call these &lt;em&gt;goal-based problems&lt;/em&gt; &lt;a href="#Yoon2008-probabilistic-planning"&gt;(Yoon, Fern, Givan, &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Kambhampati, 2008)&lt;/a&gt;.) In the next section we'll see that not all plans are created equal, so even in the non-discounted case we want one that ends up in a goal state with the highest&amp;nbsp;probability.&lt;/p&gt;
&lt;p&gt;In an &lt;em&gt;offline&lt;/em&gt; approach to deterministic planning problems, a planner is given an environment, initial state and goal state, and it needs to return a sequence of actions that brings the environment to the goal state. However, this offline approach does not work for probabilistic problems, where the outcome of an action is not always in our control. Hence a probabilistic planner is usually executed &lt;em&gt;online&lt;/em&gt;: it makes an observation (e.g. the current state of the environment, in the fully observable case), does some magic, and outputs a single action that brings the agent closer to a goal state. Nature brings the agent to a new state, not necessarily the one you desired, and these steps are repeated, until you run out of time or end up at a&amp;nbsp;goal.&lt;/p&gt;
&lt;p&gt;Since the fourth &lt;a href="http://icaps-conference.org/index.php/Main/Competitions"&gt;International Planning Competition&lt;/a&gt; in 2004 hosted by the &lt;span class="caps"&gt;ICAPS&lt;/span&gt; (International Conference on Automated Planning and Scheduling), this event featured a probabilistic track. The winner of &lt;span class="caps"&gt;IPPC&lt;/span&gt; 2004 was &lt;span class="caps"&gt;FF&lt;/span&gt;-Replan, a planner that simplifies the probabilistic planning problem into a deterministic one by not considering the multiple potential effects of an action &lt;a href="#Yoon2007-FF-replan"&gt;(Yoon, Fern, &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Givan, 2007)&lt;/a&gt; â€“ hence the title of the paper, â€œ&lt;span class="caps"&gt;FF&lt;/span&gt;-Replan: A Baseline for Probabilistic&amp;nbsp;Planning.â€&lt;/p&gt;
&lt;h1&gt;Probabilistically interesting planning&amp;nbsp;problems&lt;/h1&gt;
&lt;p&gt;Iain Little and Sylvie ThiÃ©baux analyzed the common characteristics of planning problems that can and cannot be optimally solved by a planner like &lt;span class="caps"&gt;FF&lt;/span&gt;-Replan &lt;a href="#Little2007-probabilistic-planning"&gt;(Little &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; ThiÃ©baux, 2007)&lt;/a&gt;. They gave necessary and sufficient conditions for a probabilistic planning problem to be &lt;em&gt;probabilistically interesting&lt;/em&gt;: on a problem fulfilling these conditions, a planner that determinizes the problem will lose crucial information, and will do worse than a probabilistic planner. In this section I'll summarize these conditions using natural language, slightly diverging from the vocabulary of the paper. For formal definitions and more examples, see the &lt;a href="http://users.cecs.anu.edu.au/~iain/icaps07.pdf"&gt;original paper&lt;/a&gt;; it is an interesting&amp;nbsp;read.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Criterion 1:&lt;/em&gt; there are multiple paths from the start to the goal. If there is only a single path, then any planner that finds &lt;em&gt;a&lt;/em&gt; path will do equally good, as this will be the only&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;Counterexample:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graphical description of an MDP with a single goal trajectory" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/counter-1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Criterion 2:&lt;/em&gt; where the above two paths diverge, there is a choice about which way to go, i.e. a state \&lt;script type="math/tex"&gt;s_{crossroads}\&lt;/script&gt; from which action \&lt;script type="math/tex"&gt;a_1\&lt;/script&gt; leads to one road with a different probability than action \&lt;script type="math/tex"&gt;a_2\&lt;/script&gt; does. (Yes, this is a sufficient condition for the first criterion.) If it's only luck that separates the two paths, then the agent doesn't have much of a choice to do&amp;nbsp;better.&lt;/p&gt;
&lt;p&gt;Counterexample:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MDP with skill doesn't help" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/counter-2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Criterion 3:&lt;/em&gt; there must be a non-trivially avoidable dead end in the environment. A &lt;em&gt;dead end&lt;/em&gt; is an absorbing state that is not a goal state, i.e. a state from which there is no path to any goal state. For a dead end to be &lt;em&gt;avoidable&lt;/em&gt;, there must be a state \&lt;script type="math/tex"&gt;s_{crossroads}\&lt;/script&gt; with at least two possible actions \&lt;script type="math/tex"&gt;a_{deadly}\&lt;/script&gt; and \&lt;script type="math/tex"&gt;a_{winning}\&lt;/script&gt;, such that executing \&lt;script type="math/tex"&gt;a_{deadly}\&lt;/script&gt; brings the agent to the dead end with a higher probability than executing \&lt;script type="math/tex"&gt;a_{winning}\&lt;/script&gt;. A dead end is &lt;em&gt;non-trivially avoidable&lt;/em&gt; if \&lt;script type="math/tex"&gt;s_{crossroads}\&lt;/script&gt; is on a path from the initial state to a goal state, and there is a non-zero chance of reaching a goal state after executing either \&lt;script type="math/tex"&gt;a_{winning}\&lt;/script&gt; or \&lt;script type="math/tex"&gt;a_{deadly}\&lt;/script&gt;.&lt;/p&gt;
&lt;p&gt;Counterexample: the probabilistic version of Blocksworld, where the worst case scenario is that a block is dropped accidentally, does not contain dead ends; the environment is irreducible. (This was an actual problem of &lt;span class="caps"&gt;IPPC&lt;/span&gt;&amp;nbsp;2004.)&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probabilistic Blocks world" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/blocksworld.png"&gt;&lt;/p&gt;
&lt;p&gt;Counterexample: all dead ends are&amp;nbsp;unavoidable.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MDP with no avoidable dead end" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/counter-3b.png"&gt;&lt;/p&gt;
&lt;p&gt;Counterexample: all dead ends are trivially&amp;nbsp;avoidable.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MDP with only trivially avoidable dead ends" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/counter-3c.png"&gt;&lt;/p&gt;
&lt;h1&gt;A simple yet â€œinterestingâ€ planning&amp;nbsp;problem&lt;/h1&gt;
&lt;p&gt;A very simple problem that is probabilistically interesting is what the authors call &lt;code&gt;climber&lt;/code&gt;, described by the following&amp;nbsp;story:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are stuck on a roof because the ladder you climbed up on fell down. There are plenty of people around; if you call out for help someone will certainly lift the ladder up again. Or you can try to climb down without it. You arenâ€™t a very good climber though, so there is a 40% chance that you will fall and break your neck if you do it alone. What do you&amp;nbsp;do?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Graphical representation of the &lt;code&gt;climber&lt;/code&gt; problem:
&lt;img alt="Graphical representation of the climber problem" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/climber-orig.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Despite the simplicity of this problem, most methods to turn it into a deterministic problem fail. Little and ThiÃ©baux described 3 ways to determinize a problem, and they called a resulting deteministic problem a&amp;nbsp;â€œcompilationâ€.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;&lt;span class="caps"&gt;REPLAN1&lt;/span&gt;&lt;/em&gt; approach simply drops all but the most likely outcome of every action, and finds the shortest goal trajectory. (This was the approach used by &lt;span class="caps"&gt;FF&lt;/span&gt;-Replan.) Compilation of the climber problem according to &lt;span class="caps"&gt;REPLAN1&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Compilation of the climber problem according to REPLAN1" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/climber-det1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="caps"&gt;REPLAN2&lt;/span&gt;(shortest)&lt;/em&gt; turns every possible probabilistic outcome of an action into the outcome of a deterministic action, each with a cost of 1. Optimizing for smallest cost thus finds the &lt;em&gt;shortest&lt;/em&gt; goal trajectory, but this might not be the one with the highest success probability. Compilation of the climber problem according to &lt;span class="caps"&gt;REPLAN2&lt;/span&gt;(shortest):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Compilation of the climber problem according to REPLAN2(shortest)" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/climber-det2.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="caps"&gt;REPLAN2&lt;/span&gt;(most-likely)&lt;/em&gt; also turns every outcome into a separate deterministic action, but the new action costs are the negative log probability of the relevant outcome. This is the only compilation of the problem that finds the optimal path for &lt;code&gt;climber&lt;/code&gt;, but for many other problems even this one will be suboptimal. The resulting compilation is as&amp;nbsp;follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Compilation of the climber problem according to REPLAN2(most-likely)" src="https://www.treszkai.com/2018/05/28/probabilistically-interesting/climber-det3.jpg"&gt;&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Finding the optimal goal trajectory in a probabilistic planning problem is computationally expensive, so most planners use some heuristics. One way to plan in a stochastic environment is to change the probabilistic planning problem into a deterministic shortest path problem and replan after (almost) every step, which is computationally efficient, but in many cases suboptimal. This article outlined the attributes of probabilistically interesting problems, where the deterministic replanning approach often fails. As such, recent probabilistic planners use more complicated methods (or often a portfolio of probabilistic planners), but replanners remain a good baseline to compare&amp;nbsp;against.&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol class="bibliography"&gt;&lt;li&gt;&lt;span id="Little2007-probabilistic-planning"&gt;Little, I., &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; ThiÃ©baux, S. (2007). Probabilistic planning vs. replanning. &lt;i&gt;Workshop, &lt;span class="caps"&gt;ICAPS&lt;/span&gt; 2007&lt;/i&gt;. Retrieved from http://users.cecs.anu.edu.au/&amp;nbsp;iain/icaps07.pdf&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id="Yoon2007-FF-replan"&gt;Yoon, S. W., Fern, A., &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Givan, R. (2007). &lt;span class="caps"&gt;FF&lt;/span&gt;-Replan: A Baseline for Probabilistic Planning. In &lt;span class="caps"&gt;M. S.&lt;/span&gt; Boddy, M. Fox, &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; S. ThiÃ©baux (Eds.), &lt;i&gt;Proceedings of the Seventeenth International Conference on Automated
               Planning and Scheduling, &lt;span class="caps"&gt;ICAPS&lt;/span&gt; 2007, Providence, Rhode Island, &lt;span class="caps"&gt;USA&lt;/span&gt;,
               September 22-26, 2007&lt;/i&gt; (p. 352). &lt;span class="caps"&gt;AAAI&lt;/span&gt;. Retrieved from http://www.aaai.org/Library/&lt;span class="caps"&gt;ICAPS&lt;/span&gt;/2007/icaps07-045.php&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id="Yoon2008-probabilistic-planning"&gt;Yoon, S. W., Fern, A., Givan, R., &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Kambhampati, S. (2008). Probabilistic Planning via Determinization in Hindsight. In D. Fox &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; &lt;span class="caps"&gt;C. P.&lt;/span&gt; Gomes (Eds.), &lt;i&gt;Proceedings of the Twenty-Third &lt;span class="caps"&gt;AAAI&lt;/span&gt; Conference on Artificial Intelligence,
               &lt;span class="caps"&gt;AAAI&lt;/span&gt; 2008, Chicago, Illinois, &lt;span class="caps"&gt;USA&lt;/span&gt;, July 13-17, 2008&lt;/i&gt; (pp. 1010â€“1016). &lt;span class="caps"&gt;AAAI&lt;/span&gt; Press. Retrieved from http://www.aaai.org/Library/&lt;span class="caps"&gt;AAAI&lt;/span&gt;/2008/aaai08-160.php&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="AI"></category></entry></feed>